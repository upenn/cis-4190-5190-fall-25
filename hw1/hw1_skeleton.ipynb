{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 4190/5190 Fall 2025 - Homework 1**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tips for Coding Homeworks**\n",
        "Dear class, here are some tips to follow in coding HWs in this class, to minimize the chances of hitting hard-to-resolve bugs:\n",
        "\n",
        "1.   **Do not set/alter random seeds.**\n",
        "2.   Most bugs come down to minor errors in your code. The hardest and most important part of coding is debugging; debugging ML code is a key skillset you should acquire in this class.\n",
        "3.   Good python coding avoids loops if at all possible: in particular, **for any vector and matrix operations, use `numpy` functions instead of loops**. This will often speed up your code by orders of magnitude. You may also want to take a look at the [python primer](https://www.seas.upenn.edu/~cis5190/fall2025/docs/basics_python_numpy.ipynb).\n",
        "4.   Pay **extra attention to \"copying\" vs \"moving\" objects** i.e. take care during assignment operations. Otherwise, it may lead to incorrect results or Out Of Memory issues and Colab notebook restarts.\n",
        "5.   If you are running into a `'__builtins__'` error, it's likely because you're using a function call of the form `numpy.ndarray.mean()`, like `a.mean()`. This is technically correct, but our autograding software PennGrader can't handle it. Please use the function **call `numpy.mean(a)` instead.**\n",
        "6.   For this homework especially, pay **special attention to** the `compute_gradient` function and the `fit` function of the **Linear regression** section as these involve multiple key concepts.\n",
        "\n",
        "Have fun coding up and training your first ML algorithms!\n"
      ],
      "metadata": {
        "id": "2KzjVMJIWiHd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Please remember to **import libraries**, **set up PennGrader**, and **type in your PennID** to get points, especially when you do this HW module by module."
      ],
      "metadata": {
        "id": "VV4asDUKqhHV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line!!"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dill\n",
        "from dill.source import getsource\n",
        "\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ],
      "metadata": {
        "id": "rKxwaZkaulBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjXBdEb-p8K"
      },
      "source": [
        "# **PennGrader Setup**\n",
        "\n",
        "First, you'll need to set up PennGrader, an autograder we are going to use throughout the semester. PennGrader will automatically grade your answer and provide you with instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attempts per day). **We will only record your latest score in our backend database**. To successfully get your grade, **please ensure your 8-Digit Penn-ID is entered correctly**. If not, the autograder won't know what result to check.\n",
        "\n",
        "After finishing each homework assignment, you must submit your .ipynb notebook to Gradescope before the homework deadline. Gradescope will then retrieve and display your scores from our backend database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GCTLN4G-nK2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip3 install penngrader-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLvQV5yVfQvv"
      },
      "outputs": [],
      "source": [
        "%%writefile student_config.yaml\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLnoPRci-sTC"
      },
      "outputs": [],
      "source": [
        "from penngrader.grader import *"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Serialization code needed by the autograder\n",
        "import inspect, sys\n",
        "from IPython.core.magics.code import extract_symbols\n",
        "\n",
        "def new_getfile(object, _old_getfile=inspect.getfile):\n",
        "    if not inspect.isclass(object):\n",
        "        return _old_getfile(object)\n",
        "\n",
        "    # Lookup by parent module (as in current inspect)\n",
        "    if hasattr(object, '__module__'):\n",
        "        object_ = sys.modules.get(object.__module__)\n",
        "        if hasattr(object_, '__file__'):\n",
        "            return object_.__file__\n",
        "\n",
        "    # If parent module is __main__, lookup by methods (NEW)\n",
        "    for name, member in inspect.getmembers(object):\n",
        "        if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
        "            return inspect.getfile(member)\n",
        "    else:\n",
        "        raise TypeError('Source for {!r} not found'.format(object))\n",
        "inspect.getfile = new_getfile\n",
        "\n",
        "def grader_serialize(obj):\n",
        "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
        "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
        "    return class_code"
      ],
      "metadata": {
        "id": "7h3WBtpyzOqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu0XYZHO-t8J"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO IN OUR BACKEND\n",
        "STUDENT_ID = 12345678       # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDTGGbo-xkf"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immediately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile notebook-config.yaml\n",
        "\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "85m5diQ7-rd8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_QDnZk-vvI"
      },
      "outputs": [],
      "source": [
        "grader = PennGrader('notebook-config.yaml', 'cis5190_f25_HW1', STUDENT_ID, STUDENT_ID)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfs6aWUhmWJT"
      },
      "source": [
        "# **1. Linear Regression**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.1. Linear Regression Implementation [40 pts, Autograded]**\n",
        "\n",
        "In this section you will implement linear regression with both L1 and L2 regularization. Your class LinearRegression must implement the following API:\n",
        "\n",
        "* `__init__(alpha, tol, max_iter, beta_init, penalty, lambd)`\n",
        "* `compute_cost(beta, X, y)`\n",
        "* `compute_gradient(beta, X, y)`\n",
        "* `fit(X, y)`\n",
        "* `has_converged(beta_old, beta_new)`\n",
        "* `predict(X)`\n",
        "\n",
        "Note that these methods have already been defined correctly for you in the LinearRegression class. **DO NOT** change the API.\n",
        "\n",
        "### **1.1.1. Cost Function [9 pts]**\n",
        "\n",
        "The `compute_cost` function should compute the cost for a given $\\beta$ vector. The cost is a scalar value given by:\n",
        "\n",
        "$\n",
        "L({\\beta}) = \\frac{1}{N}\\sum_{i =1}^N (f_{{\\beta}}({x}_i) - y_i)^2\n",
        "$\n",
        "\n",
        "where\n",
        "\n",
        "> $f_{{\\beta}}({x}_i) = \\beta^Tx_i$\n",
        "\n",
        "Based on the regularization penalty, you may need to add below regularization penalty loss to MSE Loss computed previously.\n",
        "\n",
        "L1 Regularization Loss:\n",
        ">$\n",
        "L_1({\\beta}) = L({\\beta}) + \\lambda\\sum_{j = 1}^D  |{\\beta}_j|\n",
        "$\n",
        "\n",
        "L2 Regularization Loss:\n",
        ">$\n",
        "L_2({\\beta}) = L({\\beta}) + \\lambda\\sum_{j = 1}^D  {\\beta}_j^2\n",
        "$\n",
        "\n",
        "$N$ is the number of training samples and $D$ is the number of features (excluding the intercept term). $\\beta$ is a $D + 1$ dimensional vector, with the first element being the intercept term. **Note that we do not include the intercept in the regularization terms.**\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.2. Gradient of the Cost Function [9 pts]**\n",
        "\n",
        "The `compute_gradient` function should compute the gradient of the cost function at a given $\\beta$.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.3. Convergence Check [2 pts]**\n",
        "\n",
        "The `has_converged` function should return whether gradient descent algorithm has converged or not. Refer to the below section 1.1.4 for the convergence condition.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.4. Training with Gradient Descent [8 pts]**\n",
        "\n",
        "The `fit` method should train the model via gradient descent, relying on the cost and gradient functions. The trained weights/coefficients must be stored as `beta_`. The weights and the corresponding cost after every gradient descent iteration must be stored in `hist_beta_` and `hist_cost_` respectively.\n",
        "\n",
        "* The gradient descent stops or converges when $\\beta$ stops changing or changes negligibly between consecutive iterations; i.e., when\n",
        "$\\| {\\beta}_\\mathit{new} -  {\\beta}_\\mathit{old} \\|_2 \\leq \\epsilon$,\n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance).\n",
        "\n",
        "* To ensure that the function terminates, you should set a maximum limit for the number of epochs, irrespective of whether $\\beta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the gradient descent algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.5. Training with Stochastic Gradient Descent (SGD) [8 pts]**\n",
        "\n",
        "The `fit_sgd` method should train the model via stochastic gradient descent (SGD), relying on the cost and gradient functions.\n",
        "\n",
        "The trained weights/coefficients must be stored as `beta_`. The weights and the corresponding cost after every SGD iteration must be stored in `hist_beta_` and `hist_cost_` respectively.\n",
        "\n",
        "* Unlike regular (or batch) gradient descent, SGD takes a gradient step on a single training example on each iteration. In other words, rather than compute the gradient for all training examples, summing them, and taking a single gradient step, it iterates through training examples one by one, computing the gradient and performing a step for each. One pass over the entire training dataset is called an epoch; at the end of an epoch, the next epoch restarts from the first example in the training dataset.\n",
        "\n",
        "* As with gradient descent, SGD stops or converges when $\\beta$ stops changing or changes negligibly between consecutive iterations, i.e., when\n",
        "$\\| {\\beta}_\\mathit{new} -  {\\beta}_\\mathit{old} \\|_2 \\leq \\epsilon$,\n",
        "for some small $\\epsilon$ (e.g., $\\epsilon$ = 1E-4). $\\epsilon$ is stored as `tol` (short for tolerance). Since each step is much shorter, SGD typically only checks for convergence at the **end of each epoch**.\n",
        "\n",
        "* To ensure that the function terminates, you should set a maximum limit for the number of gradient descent iterations irrespective of whether $\\beta$ converges or not. The limit is stored as `max_iter`.\n",
        "\n",
        "* `alpha` is the learning rate of the SGD algorithm.\n",
        "\n",
        "---\n",
        "\n",
        "### **1.1.6. Predict [4 pts]**\n",
        "\n",
        "The `predict` function should predict outputs given an input data matrix by applying the coefficient matrix.\n",
        "\n",
        "---\n",
        "\n",
        "Your implementation of the Linear Regression algorithm should closely follow the workflow described in the diagram below:\n",
        "\n",
        "```plaintext\n",
        "            ┌─────────────────────────────────────────────┐\n",
        "            │                   init                      │\n",
        "            │ (alpha, beta, max_iter, tol, penalty, lambd,│\n",
        "            │            hist_cost_, hist_beta_)          │    \n",
        "            └────────────────────┬────────────────────────┘\n",
        "                                 │\n",
        "                                 ▼\n",
        "                    ┌────────────────────────┐\n",
        "                    │ Choose training entry  │\n",
        "                    │ point                  │\n",
        "                    └────────────┬───────────┘\n",
        "                                 │\n",
        "                ┌────────────────┘───────────────┐\n",
        "                ▼                                ▼\n",
        " ┌─────────────────────────────┐     ┌──────────────────────────────┐\n",
        " │ fit()                       │     │ fit_sgd()                    │\n",
        " │ (Batch Gradient Descent)    │     │ (Stochastic Gradient Descent)│\n",
        " └───────────────┬─────────────┘     └───────────────┬──────────────┘\n",
        "                 │                                   │\n",
        "                 ▼                                   ▼\n",
        " ┌───────────────────────────┐     ┌────────────────────────────────┐\n",
        " │ Loop per-dataset averaged:│     │ Loop per-sample/epoch:         │\n",
        " │ - compute_gradient()      │     │ - compute_gradient()           │\n",
        " │ - compute_cost()          │     │ - compute_cost()               │\n",
        " │ - has_converged()?        │     │ - has_converged()?             │\n",
        " └───────────────┬───────────┘     └───────────────┬────────────────┘\n",
        "                 │                                 │\n",
        "                 └───────────────┬─────────────────┘\n",
        "                                 ▼\n",
        "                      ┌─────────────────────┐\n",
        "                      │ predict (Inference) │\n",
        "                      └─────────────────────┘\n",
        "\n",
        "```\n",
        "---\n"
      ],
      "metadata": {
        "id": "aHeeCVIAcls7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_iD4A-TmjKe"
      },
      "outputs": [],
      "source": [
        "class LinearRegression:\n",
        "\n",
        "    \"\"\"\n",
        "    Linear Regression\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    alpha: float, default=0.01\n",
        "        Learning rate\n",
        "    tol: float, default=0.0001\n",
        "        Tolerance for stopping criteria\n",
        "    max_iter: int, default=10000\n",
        "        Maximum number of iterations of gradient descent\n",
        "    beta_init: None (or) numpy.ndarray of shape (D + 1,)\n",
        "        The initial weights; if None, all weights will be zero by default\n",
        "    penalty: string, default = None\n",
        "        The type of regularization if any. The other options are l1 and l2\n",
        "    lambd: float, default = 1.0\n",
        "        The parameter regularization constant (i.e. lambda)\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    beta_: numpy.ndarray of shape (D + 1,)\n",
        "        The value of the coefficients after gradient descent has converged\n",
        "        or the number of iterations hit the maximum limit\n",
        "    hist_beta_: numpy.ndarray of shape (num_iter, D + 1) where num_iter is the number of gradient descent iterations\n",
        "        Stores beta_ after every gradient descent iteration\n",
        "    hist_cost_: numpy.ndarray of shape (num_iter,) where num_iter is the number of gradient descent iterations\n",
        "        Stores cost after every gradient descent iteration\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, alpha = 0.01, tol=1e-4, max_iter = 100, beta_init = None, penalty = None, lambd = 0):\n",
        "\n",
        "        # store metadata\n",
        "        self.alpha = alpha\n",
        "        self.beta_init = beta_init\n",
        "        self.max_iter = max_iter\n",
        "        self.tol = tol\n",
        "        self.penalty = penalty\n",
        "        self.lambd = lambd\n",
        "\n",
        "        self.beta_ = None\n",
        "        self.hist_cost_ = None\n",
        "        self.hist_beta_ = None\n",
        "\n",
        "    def compute_cost(self, beta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the cost/objective function with penalty if any.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        beta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The input features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        cost: float\n",
        "            The cost as a scalar value\n",
        "        \"\"\"\n",
        "\n",
        "        # STUDENT TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        pass\n",
        "        # STUDENT TODO ENDS\n",
        "\n",
        "    def compute_gradient(self, beta, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the gradient of the cost function with penalty if any.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        beta: numpy.ndarray of shape (D + 1,)\n",
        "            The coefficients\n",
        "        X: numpy.ndarray of shape (N, D + 1)\n",
        "            The input features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        gradient: numpy.ndarray of shape (D + 1,)\n",
        "            The gradient values\n",
        "        \"\"\"\n",
        "\n",
        "        # STUDENT TODO STARTS: Complete the function (should account for three cases - no penalty, l1 penalty, and l2 penalty)\n",
        "        pass\n",
        "        # STUDENT TODO ENDS\n",
        "\n",
        "    def has_converged(self, beta_old, beta_new):\n",
        "\n",
        "        \"\"\"\n",
        "        Return whether gradient descent has converged.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        beta_old: numpy.ndarray of shape (D + 1,)\n",
        "            The weights prior to the update by gradient descent\n",
        "        beta_new: numpy.ndarray of shape (D + 1,)\n",
        "            The weights after the update by gradient descent\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        converged: bool\n",
        "            Whether gradient descent converged or not\n",
        "        \"\"\"\n",
        "\n",
        "        # STUDENT TODO START: Complete the function\n",
        "        pass\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def fit(self, X, y):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using gradient descent and store them as beta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The input features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "\n",
        "        # Initializing the weights\n",
        "        if self.beta_init is None:\n",
        "            beta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            beta_old = self.beta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_beta_ = np.array([beta_old])\n",
        "        # print(self.hi) - Removed this line as it causes AttributeError\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(beta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "\n",
        "        # STUDENT TODO START: Complete the function\n",
        "        pass\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def fit_sgd(self, X, y, random_state=None):\n",
        "\n",
        "        \"\"\"\n",
        "        Compute the coefficients using Stochastic Gradient Descent\n",
        "        and store them as beta_.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The input features matrix\n",
        "        y: numpy.ndarray of shape (N,)\n",
        "            The target variable array\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Nothing\n",
        "        \"\"\"\n",
        "\n",
        "        N, D = X.shape\n",
        "\n",
        "        # Adding a column of ones at the beginning for the bias term\n",
        "        ones_col = np.ones((N, 1))\n",
        "        X = np.hstack((ones_col, X))\n",
        "\n",
        "        # Initializing the weights\n",
        "        if self.beta_init is None:\n",
        "            beta_old = np.zeros((D + 1,))\n",
        "        else:\n",
        "            beta_old = self.beta_init\n",
        "\n",
        "        # Initializing the historical weights matrix\n",
        "        # Remember to append this matrix with the weights after every gradient descent iteration\n",
        "        self.hist_beta_ = np.array([beta_old])\n",
        "\n",
        "        # Computing the cost for the initial weights\n",
        "        cost = self.compute_cost(beta_old, X, y)\n",
        "\n",
        "        # Initializing the historical cost array\n",
        "        # Remember to append this array with the cost after every gradient descent iteration\n",
        "        self.hist_cost_ = np.array([cost])\n",
        "\n",
        "        # Init RNG, use it for shuffle\n",
        "        if random_state is None:\n",
        "            rng = np.random.default_rng()\n",
        "        else:\n",
        "            rng = np.random.default_rng(int(random_state))\n",
        "\n",
        "        # Please use rng.permutation(N) for shuffle\n",
        "        # STUDENT TODO START: Complete the function\n",
        "        pass\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def predict(self, X):\n",
        "\n",
        "        \"\"\"\n",
        "        Predict the target variable values for the data points in X.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        X: numpy.ndarray of shape (N, D)\n",
        "            The input features matrix\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        y_hat: numpy.ndarray of shape (N,)\n",
        "            The predicted target variables values for the data points in X\n",
        "        \"\"\"\n",
        "\n",
        "        N = X.shape[0]\n",
        "        X = np.hstack((np.ones((N, 1)), X))\n",
        "\n",
        "        # STUDENT TODO START: Complete the function\n",
        "        pass\n",
        "        # STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0gOdODTWQQX0"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_compute_cost(StudentLinearRegression):\n",
        "\n",
        "    test_case_beta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_beta, test_case_X, test_case_y)\n",
        "    required_ans = 4.881828654157736\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_beta, test_case_X, test_case_y)\n",
        "    required_ans = 4.94300429515773\n",
        "\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_cost(test_case_beta, test_case_X, test_case_y)\n",
        "    required_ans = 4.919253244675344\n",
        "    assert np.abs(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_lin_reg_compute_cost(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ct-hUcbC9Zp1"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_lin_reg_compute_cost', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pysdW3awRLl1"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_compute_gradient(StudentLinearRegression):\n",
        "\n",
        "    test_case_beta = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_beta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.53908485]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l1\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_beta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.63908485]\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(penalty=\"l2\", lambd=0.1)\n",
        "    student_ans = student_lr_reg.compute_gradient(test_case_beta, test_case_X, test_case_y)\n",
        "    required_ans = [ 4.79663712, -3.66143613]\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_lin_reg_compute_gradient(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VptfbtsMAEVB"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_lin_reg_compute_gradient', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "naEgZPDXQ5_U"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_has_converged(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression()\n",
        "    test_case_beta_old = np.array([ 1.62434536, -0.61175641])\n",
        "    test_case_beta_new = np.array([1.624345, -0.611756])\n",
        "    student_ans = student_lr_reg.has_converged(test_case_beta_old, test_case_beta_new)\n",
        "    required_ans = True\n",
        "\n",
        "    assert student_ans == required_ans\n",
        "\n",
        "test_lin_reg_has_converged(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7rx213_3_gmj"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_lin_reg_has_converged', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NF9jGYOVSrC2",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_fit(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.hist_beta_\n",
        "    required_ans = np.array([[ 0.        ,  0.        ,  0.        ],\n",
        "       [ 0.012     ,  0.00566085, -0.00773638],\n",
        "       [ 0.02351422,  0.01085581, -0.01491529],\n",
        "       [ 0.03457102,  0.01561393, -0.0215702 ],\n",
        "       [ 0.04519706,  0.01996249, -0.02773259],\n",
        "       [ 0.05541739,  0.02392713, -0.03343205]])\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "\n",
        "test_lin_reg_fit(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgCoBChfNobE"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_lin_reg_fit', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P0ra9f4IKvy6",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_fit_sgd(StudentLinearRegression):\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    test_case_X = np.array([[ 1.62434536, -0.61175641],\n",
        "                            [-0.52817175, -1.07296862],\n",
        "                            [ 0.86540763, -2.3015387 ],\n",
        "                            [ 1.74481176, -0.7612069 ],\n",
        "                            [ 0.3190391,  -0.24937038]])\n",
        "    test_case_y = np.array([1, 1, 0, 0, 1])\n",
        "    np.random.seed(42)\n",
        "    student_lr_reg.fit_sgd(test_case_X, test_case_y,random_state=42)\n",
        "    student_ans = student_lr_reg.hist_beta_\n",
        "    required_ans = np.array([[ 0.,          0.,          0.        ],\n",
        "                             [ 0.02,        0.00638078, -0.00498741],\n",
        "                             [ 0.01925999,  0.00574037, -0.00328424],\n",
        "                             [ 0.01862447,  0.00463151, -0.00280048],\n",
        "                             [ 0.03824081, -0.00572928, -0.02384819],\n",
        "                             [ 0.05737033,  0.02534367, -0.0355508 ],\n",
        "                             [ 0.0547973,   0.02085421, -0.03359219],\n",
        "                             [ 0.07261286,  0.04979283, -0.04449097],\n",
        "                             [ 0.09073184,  0.0402229,  -0.06393207],\n",
        "                             [ 0.08527817,  0.03550326, -0.05138025],\n",
        "                             [ 0.10308982,  0.04118587, -0.05582195],\n",
        "                             [ 0.09774564,  0.03656098, -0.04352212],\n",
        "                             [ 0.11534038,  0.04217439, -0.04790973],\n",
        "                             [ 0.13107727,  0.06773654, -0.05753687],\n",
        "                             [ 0.14793655,  0.05883194, -0.07562635],\n",
        "                             [ 0.14177346,  0.04807851, -0.07093497],\n",
        "                             [ 0.13484065,  0.0420788,  -0.05497883],\n",
        "                             [ 0.15160114,  0.04742606, -0.0591584 ],\n",
        "                             [ 0.16630458,  0.07130952, -0.06815333],\n",
        "                             [ 0.15945248,  0.05935389, -0.06293746],\n",
        "                             [ 0.17553981,  0.05085702, -0.08019866],\n",
        "                             [ 0.18939559,  0.07336358, -0.08867502],\n",
        "                             [ 0.20447974,  0.06539656, -0.10485984],\n",
        "                             [ 0.21944988,  0.07017262, -0.10859295],\n",
        "                             [ 0.20884771,  0.06099742, -0.08419164],\n",
        "                             [ 0.20126043,  0.04775905, -0.07841615]])\n",
        "\n",
        "    assert np.linalg.norm(student_ans - required_ans) <= 1e-2\n",
        "test_lin_reg_fit_sgd(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sewbA8dC5Fdm"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_lin_reg_fit_sgd', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQQN2ky4UYxx"
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_predict(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit(test_case_X, test_case_y)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    required_ans = np.array([0.04739416, 0.02735934, 0.02140787, 0.04634383, 0.04320043,\n",
        "       0.02836861, 0.03726417, 0.03808224, 0.03214353, 0.05166998,\n",
        "       0.05102933, 0.05639199, 0.0416892 , 0.03175554, 0.04895695,\n",
        "       0.03465034, 0.02912364, 0.03954521, 0.0396391 , 0.06440433,\n",
        "       0.03189335, 0.06016748, 0.03661307, 0.07146111, 0.05261461,\n",
        "       0.04180017, 0.03223834, 0.0500466 , 0.06128615, 0.05703506,\n",
        "       0.05467262, 0.04388664, 0.04648138, 0.07052753, 0.04140456,\n",
        "       0.02830984, 0.05608863, 0.0212115 , 0.05238969, 0.05514024,\n",
        "       0.04020117, 0.05048966, 0.04696158, 0.04438422, 0.05897309,\n",
        "       0.05443805, 0.03375689, 0.04794345, 0.04242038, 0.04869202])\n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "test_lin_reg_predict(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LvZwTzBBN00l"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_lin_reg_predict', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIACk5LZLqz7",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def test_lin_reg_predict_sgd(StudentLinearRegression):\n",
        "\n",
        "    student_lr_reg = StudentLinearRegression(max_iter=5)\n",
        "    np.random.seed(1)\n",
        "    test_case_X = np.random.randn(50, 2)\n",
        "    test_case_y = np.random.randint(0, 2, 50)\n",
        "    student_lr_reg.fit_sgd(test_case_X, test_case_y, random_state=1)\n",
        "    student_ans = student_lr_reg.predict(test_case_X)\n",
        "    required_ans = np.array([0.40692846, 0.21449028, 0.09548026, 0.38974227, 0.39264848, 0.1669377,\n",
        "                             0.33574137, 0.3019747,  0.26525779, 0.51010281, 0.53662101, 0.54474898,\n",
        "                             0.35549442, 0.2587561,  0.48460994, 0.31332467, 0.24204004, 0.37571796,\n",
        "                             0.39049628, 0.62483088, 0.26262755, 0.64350889, 0.31600571, 0.76036112,\n",
        "                             0.51989245, 0.3751962,  0.29598906, 0.49684306, 0.60965866, 0.57176677,\n",
        "                             0.57299078, 0.39558417, 0.42973035, 0.71972543, 0.31274811, 0.25459013,\n",
        "                             0.56340224, 0.10967673, 0.52996714, 0.54906182, 0.37050622, 0.49051677,\n",
        "                             0.44474082, 0.43799543, 0.60169802, 0.51055005, 0.2921985,  0.4505304,\n",
        "                             0.40277209, 0.49191095])\n",
        "    assert np.mean(np.abs(student_ans - required_ans)) <= 1e-2\n",
        "\n",
        "test_lin_reg_predict_sgd(LinearRegression)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WKhfPR6uUJm9"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_lin_reg_predict_sgd', answer = grader_serialize(LinearRegression))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qUcFLi6tjL_"
      },
      "source": [
        "## **1.2. Effect of polynomial degree on training and validation error [5190 Extra, 5pts, Manually Graded]**"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we consider a dataset that was generated using some higher degree polynomial function of the input variable. We do not know the degree of the underlying polynomial $p$, so let's try to estimate it.\n",
        "\n",
        "Polynomial regression hypothesis for one input variable or feature $x$ can be written as:\n",
        "> $y = w_0 + w_1x + w_2x^2 + ... + w_px^p $\n",
        "\n",
        "If you observe carefully, this can still be solved as a linear regression, where, instead of just 2 weights, we have $p+1$ weights, and the new features are higher order terms of the original feature. Using this idea, we will investigate how changing the assumed polynomial degree $p$ in our model affects the training and validation error."
      ],
      "metadata": {
        "id": "FDWAfN9bsHWG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UcpXD4pTarFC"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"cis5190_hw1_poly_reg.csv\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw1/cis5190_hw1_poly_reg.csv\n",
        "\n",
        "poly_reg_df = pd.read_csv('cis5190_hw1_poly_reg.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to fit a Linear Regression model on this dataset after scaling base features into polynomial features of different degrees. Then, plot the variation of the train and test loss with respect to the assumed polynomial degree for the model. Perform the following steps for each degree $d$ in degrees:\n",
        "1. Transform the base features `X_base` into polynomial features of degree `d` using `PolynomialFeatures` from `sklearn.preprocessing`. Set `include_bias` to be `False`\n",
        "2. Split both `X_base` and `y` into train/test (70-30 train/test ratio and `random_state` as 42)\n",
        "3. Scale `X_train` and `X_test` appropriately using `StandardScaler`\n",
        "4. Use scikit-learn's `LinearRegression` (imported as `LinearRegressionSklearn` for you) to fit a linear model between the scaled version of `X_train` and `y_train`\n",
        "5. Obtain predictions of the model on train and test data\n",
        "6. Compute the mean squared error (MSE) and store it in `loss_train` and `loss_test`\n",
        "7. Append `loss_train` to `loss_train_list` and `loss_test` to `loss_test_list`"
      ],
      "metadata": {
        "id": "YvqMk0uaW1tr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJMhCZ6SuepA"
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression as LinearRegressionSklearn\n",
        "\n",
        "def polynomial_regression(poly_reg_df, degrees):\n",
        "    \"\"\"\n",
        "    Runs polynomial regression on dataset 'poly_reg_df' for all p in 'degrees'\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    poly_reg_df: numpy.ndarray of shape (N, D + 1)\n",
        "        Input dataset with all input features and output label (last column)\n",
        "    degrees: numpy.ndarray of shape (P,)\n",
        "        List of polynomial degrees to run polynomial regression on\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    loss_train_list: list of length P (number of degrees to try)\n",
        "        List of training losses for polynomial regression for all degrees\n",
        "    loss_test_list: list of length P (number of degrees to try)\n",
        "        List of testing losses for polynomial regression for all degrees\n",
        "    \"\"\"\n",
        "\n",
        "    loss_train_list = []\n",
        "    loss_test_list = []\n",
        "\n",
        "    X_base = poly_reg_df.iloc[:, :-1].values\n",
        "    y = poly_reg_df.iloc[:, -1].values\n",
        "\n",
        "    for d in degrees:\n",
        "\n",
        "        # STUDENT TODO START: Complete the function:\n",
        "        pass\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    return loss_train_list, loss_test_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDiIc_XjuBqk"
      },
      "outputs": [],
      "source": [
        "degrees = np.arange(1, 9)\n",
        "\n",
        "loss_train_list, loss_test_list = polynomial_regression(poly_reg_df, degrees)\n",
        "\n",
        "def plot_polynomial_regression(degrees, loss_train_list, loss_test_list):\n",
        "  \"\"\"\n",
        "  Plot the train and test losses of polynomial regression for all d in 'degrees'\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  degrees: numpy.ndarray of shape (P,)\n",
        "      List of polynomial degrees that polynomial regression was run on\n",
        "  loss_train_list: list of length P (number of degrees to try)\n",
        "      List of training losses for polynomial regression for all degrees\n",
        "  loss_test_list: list of length P (number of degrees to try)\n",
        "      List of testing losses for polynomial regression for all degrees\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Nothing\n",
        "  \"\"\"\n",
        "\n",
        "  # STUDENT TODO START:\n",
        "  # Plot the polynomial degrees (x-axis) against loss_train_list (y-axis) and loss_test_list (y-axis) in a single plot, with different colors.\n",
        "  # Make sure to include x and y axis labels, legend, as well as the title\n",
        "  pass\n",
        "  # STUDENT TODO END\n",
        "\n",
        "plot_polynomial_regression(degrees, loss_train_list, loss_test_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **1.3. Effect of learning rate on gradient descent [5190 Extra, 5pts, Manually Graded]**\n"
      ],
      "metadata": {
        "id": "dTr7IRVoq-vJ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxNxtdHnRLws"
      },
      "source": [
        "Run the below cell to download the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6W4LVZgMl3g"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"cis5190_hw1_admit.csv\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw1/cis5190_hw1_admit.csv\n",
        "\n",
        "train_df = pd.read_csv(\"cis5190_hw1_admit.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mqzaeWj1PIa-"
      },
      "source": [
        "The dataset contains two features - the input is the scores in two exams and the target is whether the student was admitted into a college or not. Your task is to use this dataset and plot the **variation of the cost function with respect to the number of gradient descent iterations for different learning rates**. Perform the following steps:\n",
        "\n",
        "1. Scale the features using StandardScaler\n",
        "2. For each of the learning rates in `learning_rates`, fit a linear regression model to the scaled data by running a maximum of 100 iterations of gradient descent with L2 penalty and $\\lambda$ as 0.001. Briefly comment on the effect of increasing the learning rate and what the best learning rate among the values would be based on the plot.\n",
        "3. Show the variation of the cost (stored in `hist_cost_`) with respect to the number of iterations for all the learning rates in the same plot.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "learning_rates = [0.001, 0.01, 0.03, 0.1, 1.0]"
      ],
      "metadata": {
        "id": "gM43m8jk9Ad-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5xbTy8vxAaYf",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "def plot_lr_cost_var(train_df, learning_rates):\n",
        "  \"\"\"\n",
        "  For each learning rate in learning_rates, plot the variations in cost for a\n",
        "  Linear Regression model with that learning rate plus other settings specified above.\n",
        "\n",
        "  Parameters\n",
        "  ----------\n",
        "  train_df: numpy.ndarray of shape (N, D + 1)\n",
        "      Dataset with features and output labels\n",
        "  learning_rates: list of length L (number of learning rates to try)\n",
        "      List of learning rates to run Linear Regression with\n",
        "\n",
        "  Returns\n",
        "  -------\n",
        "  Nothing\n",
        "  \"\"\"\n",
        "  # STUDENT TODO STARTS:\n",
        "  pass\n",
        "  # STUDENT TODO ENDS\n",
        "\n",
        "plot_lr_cost_var(train_df, learning_rates)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGlB3lojik_m"
      },
      "source": [
        "## **1.4. Synthetic dataset [ungraded]**\n",
        "\n",
        "In this section we will first create some synthetic data on which we will run your linear regression implementation. We are creating 100 data points around the function $y = mx + b$, introducing Gaussian noise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rqp2jLGTiJQo"
      },
      "outputs": [],
      "source": [
        "# Don't modify this cell\n",
        "num_samples = 100\n",
        "\n",
        "np.random.seed(1)\n",
        "noise = np.random.randn(num_samples, 1)\n",
        "X = np.random.randn(num_samples, 1)\n",
        "\n",
        "y_ideal = 11*X + 5\n",
        "y_real = (11*X + 5) + noise\n",
        "\n",
        "plt.plot(X, y_real, 'ro')\n",
        "plt.plot(X, y_ideal, 'b')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2G9qQh_uiXQt"
      },
      "source": [
        "We see that this data is clearly regressable with a line, which, ideally, would be `11x + 5`\n",
        "\n",
        "After training a linear regression model using gradient descent, you should see that training loss goes down with the number of iterations and you should obtain a $\\beta$ that converges to a value very close to [b, m], which in this case, for `11x + 5`, would be $\\beta$ = [5, 11]\n",
        "\n",
        "Also, pay attention to the effect of the type of regularization on the $\\beta$ obtained (after convergence) as well as the test MSE loss. Do they make sense, given what was discussed in class?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MYzOlitsiNCo",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def test_synthetic_data(X, y, n_iter = 2000, penalty=None, lambd=0):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=37)\n",
        "  # Given that we want to get beta as the weights of the linear equation, we won't\n",
        "  # standardize in this section\n",
        "\n",
        "  alpha = 0.03  # Learning Rate\n",
        "  print(y_train.shape)\n",
        "  print(y_train[:, 0].shape)\n",
        "\n",
        "  # Train the model\n",
        "  lr_model = LinearRegression(alpha = alpha, tol=1e-4, max_iter = n_iter, penalty=penalty, lambd=lambd)\n",
        "  lr_model.fit(X_train,y_train[:, 0])\n",
        "  y_predict = lr_model.predict(X_test)\n",
        "  loss = sklearn.metrics.mean_squared_error(y_predict, y_test)\n",
        "  print()\n",
        "  print(\" beta: {} \\n Norm of beta: {} \\n Testing MSELoss: {}\".format(lr_model.beta_, np.linalg.norm(lr_model.beta_, ord=2), loss))\n",
        "\n",
        "  print(lr_model.beta_.shape)\n",
        "  loss_history = lr_model.hist_cost_\n",
        "  plt.plot(range(len(loss_history)), loss_history)\n",
        "  plt.title(\"OLS Training Loss\")\n",
        "  plt.xlabel(\"iteration\")\n",
        "  plt.ylabel(\"Loss\")\n",
        "  if penalty == \"l1\":\n",
        "    plt.title(\"L1 Regularised Training Loss\")\n",
        "  elif penalty == \"l2\":\n",
        "    plt.title(\"L2 Regularised Training Loss\")\n",
        "  plt.show()\n",
        "\n",
        "test_synthetic_data(X, y_ideal, 500)\n",
        "test_synthetic_data(X, y_ideal, 500, \"l1\", 0.02)\n",
        "test_synthetic_data(X, y_ideal, 500, \"l2\", 0.02)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.13 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "377c3e77380f886ab555d62b93e59a1648fc55affccd8d0220be3281f77f4c6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}