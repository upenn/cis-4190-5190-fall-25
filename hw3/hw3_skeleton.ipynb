{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 4190/5190 Homework 3 - Fall 2025**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn import preprocessing\n",
        "from scipy.spatial import distance\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "\n",
        "# Random seeds for reproducibility\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "import base64"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2VtEzsZ-loR"
      },
      "outputs": [],
      "source": [
        "# For autograder only, do not modify this cell.\n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjXBdEb-p8K"
      },
      "source": [
        "# **PennGrader Setup**\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**.\n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GCTLN4G-nK2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install penngrader-client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile student_config.yaml\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ],
      "metadata": {
        "id": "m-GjiPaNBXzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLnoPRci-sTC"
      },
      "outputs": [],
      "source": [
        "from penngrader.grader import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu0XYZHO-t8J"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 12345678          # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDTGGbo-xkf"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immediately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_QDnZk-vvI"
      },
      "outputs": [],
      "source": [
        "#GRADER TODO\n",
        "grader = PennGrader('student_config.yaml', 'cis5190_f25_HW3', STUDENT_ID, STUDENT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0_ydbgD0Kvf"
      },
      "outputs": [],
      "source": [
        "# Serialization code needed by the autograder\n",
        "import inspect, sys\n",
        "from IPython.core.magics.code import extract_symbols\n",
        "\n",
        "def new_getfile(object, _old_getfile=inspect.getfile):\n",
        "    if not inspect.isclass(object):\n",
        "        return _old_getfile(object)\n",
        "\n",
        "    # Lookup by parent module (as in current inspect)\n",
        "    if hasattr(object, '__module__'):\n",
        "        object_ = sys.modules.get(object.__module__)\n",
        "        if hasattr(object_, '__file__'):\n",
        "            return object_.__file__\n",
        "\n",
        "    # If parent module is __main__, lookup by methods (NEW)\n",
        "    for name, member in inspect.getmembers(object):\n",
        "        if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
        "            return inspect.getfile(member)\n",
        "    else:\n",
        "        raise TypeError('Source for {!r} not found'.format(object))\n",
        "inspect.getfile = new_getfile\n",
        "\n",
        "def grader_serialize(obj):\n",
        "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
        "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
        "    return class_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXK6ET2npr2"
      },
      "source": [
        "## **Datasets**\n",
        "Next, we will download the dataset from Github to your local runtime. After successful download, you may verify that all datasets are present in your Colab instance.\n",
        "\n",
        "- [cis5190_hw3_observations.csv](https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw3/cis5190_hw3_observations.csv)\n",
        "- [cis5190_hw3_test_student.csv](https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw3/cis5190_hw3_test_student.csv)\n",
        "\n",
        "\n",
        "#### Acknowledgement\n",
        "Dataset obtained from kaggle.com [Hourly Weather Surface - Brazil (Southeast region)](https://www.kaggle.com/PROPPG-PPG/hourly-weather-surface-brazil-southeast-region/metadata )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dexqdaw4n3Yo"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "  if not os.path.exists(\"cis5190_hw3_observations.csv\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw3/cis5190_hw3_observations.csv\n",
        "  if not os.path.exists(\"cis5190_hw3_test_student.csv\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw3/cis5190_hw3_test_student.csv\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HDoI9Xz1Dkam"
      },
      "source": [
        "#**1. [4190: 12 autograded, 4 manual; 5190: 12 autograded, 9 manually graded] K-means Clustering**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKFPy_KWdWqK"
      },
      "source": [
        "We will implement the K-means clustering algorithm using the Breast Cancer dataset. As with all unsupervised learning problems, our goal is to discover and describe some hidden structure in unlabeled data. The K-means algorithm, in particular, attempts to determine how to separate the data into <em>k</em> distinct groups over a set of features ***given that we know (are provided) the value of k***.\n",
        "\n",
        "Knowing there are <em>k</em> distinct 'classes' however, doesn't tell anything about the content/properties within each class. If we could find samples that are representative of each of these *k* groups, then we could label the rest of the data based on how similar they are to each of the prototypical samples. We will refer to these representatives as the centroids (cluster centers) that correspond to each cluster."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gswBAEyLdYoX"
      },
      "source": [
        "## **1.1. Import the dataset**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaQTk5t1daXa"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "cancer_dataset = load_breast_cancer()\n",
        "\n",
        "# STUDENT TODO START:\n",
        "\"\"\"\n",
        "First load the dataset X from cancer_dataset.\n",
        "X -  (m, n) -> m x n matrix where m is the number of training points = 569 and n is the number of features = 30\n",
        "\"\"\"\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNk3WkQ2dhRp"
      },
      "source": [
        "## **1.2. [10 pts] K-means clustering implementation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chCjiFFMdjmo"
      },
      "source": [
        "We will first implement a class for K-means clustering.<br>\n",
        "These are the main functions: <br>\n",
        "- `__init__`: The constructor (This is implemented for you)\n",
        "- `fit`: Entrypoint function that takes in the dataset (X) as well as centroid initializations and returns:\n",
        "    - the cluster labels for each row (data point) in the dataset\n",
        "    - list of centroids corresponding to each cluster\n",
        "    - no of iterations taken to converge.\n",
        "\n",
        "Inside the `fit()` function, you will need to implement the actual K-means functionality. <br>\n",
        "The K-means process you should follow is listed below:\n",
        "1. Initialize each of the `n_clusters` centroids to a **random datapoint** if initialization is not provided.\n",
        "2. Update each data point's cluster to the closest *centroid*\n",
        "3. Calculate the new *centroid* of each cluster\n",
        "4. Repeat the previous two steps until no centroid value changes. Make sure you break out of the loop reagrdless of whether you converged or not once max iterations `max_iter` are reached.\n",
        "\n",
        "To help streamline this process, three helper functions have been given to you in the `KMeans` class:\n",
        "- `compute_distance()`: used for Step 2 above\n",
        "- `find_closest_cluster()`: used for Step 2 above\n",
        "- `compute_centroid()`: use for Step 3 above\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kJ-_rjVLdncK"
      },
      "outputs": [],
      "source": [
        "class KMeans:\n",
        "    '''Implementing Kmeans clustering'''\n",
        "\n",
        "    def __init__(self, n_clusters, max_iter=1000):\n",
        "        self.n_clusters = n_clusters\n",
        "        self.max_iter = max_iter\n",
        "\n",
        "    def compute_centroids(self, X, clusters):\n",
        "        \"\"\"\n",
        "        Computes new centroids positions given the clusters\n",
        "\n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        clusters -  m dimensional vector, where m is the number of training points\n",
        "                    At an index i, it contains the cluster id that the i-th datapoint\n",
        "                    in X belongs to.\n",
        "\n",
        "        OUTPUT:\n",
        "        centroids - k by n matrix, where k is the number of clusters.\n",
        "        \"\"\"\n",
        "        centroids = np.zeros((self.n_clusters, X.shape[1]))\n",
        "        # STUDENT TODO START:\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return centroids\n",
        "\n",
        "    def compute_distance(self, X, centroids):\n",
        "        \"\"\"\n",
        "        Computes the distance of each datapoint in X from the centroids of all the clusters\n",
        "\n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        centroids - k by n matrix, where k is the number of clusters\n",
        "\n",
        "        OUTPUT:\n",
        "        dist - m by k matrix, for each datapoint in X, the distances from all the k cluster centroids.\n",
        "\n",
        "        \"\"\"\n",
        "        dist = np.zeros((X.shape[0], self.n_clusters))\n",
        "        # STUDENT TODO START:\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return dist\n",
        "\n",
        "    def find_closest_cluster(self, dist):\n",
        "        \"\"\"\n",
        "        Finds the cluster id that each datapoint in X belongs to\n",
        "\n",
        "        INPUT:\n",
        "        dist - m by k matrix, for each datapoint in X, the distances from all the k cluster centroids.\n",
        "\n",
        "        OUTPUT:\n",
        "        clusters - m dimensional vector, where m is the number of training points\n",
        "                    At an index i, it contains the cluster id that the i-th datapoint\n",
        "                    in X belongs to.\n",
        "\n",
        "        \"\"\"\n",
        "        clusters = np.zeros(dist.shape[0])\n",
        "        # STUDENT TODO START:\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return clusters\n",
        "\n",
        "    def fit(self, X, init_centroids=None):\n",
        "        \"\"\"\n",
        "        Fit KMeans clustering to given dataset X.\n",
        "\n",
        "        INPUT:\n",
        "        X - m by n matrix, where m is the number of training points\n",
        "        init_centroids (optional) - k by n matrix, where k is the number of clusters\n",
        "\n",
        "        OUTPUT:\n",
        "        clusters - m dimensional vector, where m is the number of training points\n",
        "                    At an index i, it contains the cluster id that the i-th datapoint\n",
        "                    in X belongs to.\n",
        "        centroids - k by n matrix, where k is the number of clusters.\n",
        "                    These are the k cluster centroids, for cluster ids 0 to k-1\n",
        "        iters_taken - total iterations taken to converge. Should not be more than max_iter.\n",
        "\n",
        "        \"\"\"\n",
        "        # STUDENT TODO START:\n",
        "        # Initialize centroids to random points in the dataset if not provided (i.e. None)\n",
        "\n",
        "        # Iterate until kmeans converges or max_iters is reached. In each iteration:\n",
        "        #  - Update each datapoint's cluster to that whose *centroid* is closest\n",
        "        #  - Calculate the new *centroid* of each cluster\n",
        "        #  - Repeat the previous two steps until no centroid value changes.\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return self.clusters, self.centroids, iters_taken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuvtJjyudpVh"
      },
      "outputs": [],
      "source": [
        "# Test case centroids should be around (1.5,1.5) and (4.5,4.5)\n",
        "points = []\n",
        "result = []\n",
        "for _ in range(500):\n",
        "  x = random.random()*3\n",
        "  y = random.random()*3\n",
        "  points.append((x,y))\n",
        "  result.append(0)\n",
        "for _ in range(500):\n",
        "  x = random.random()*3 + 3\n",
        "  y = random.random()*3 + 3\n",
        "  points.append((x,y))\n",
        "  result.append(1)\n",
        "clf = KMeans(2)\n",
        "points = np.asarray(points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZduFJWmmdp2y"
      },
      "outputs": [],
      "source": [
        "#Test for sanity check\n",
        "def test_compute_centroids():\n",
        "  clf = KMeans(2)\n",
        "  centroid_p = clf.compute_centroids(np.array(points),np.array(result))\n",
        "  centroid_r = [[1.54079082, 1.534581],\n",
        " [4.49834714,4.495329]]\n",
        "  assert(np.linalg.norm(centroid_p - np.array(centroid_r)) <= 1e-2 )\n",
        "test_compute_centroids()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IvP10Sgjdp9i"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_compute_centroids', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wxu-WPKXdqyR"
      },
      "outputs": [],
      "source": [
        "def test_distance():\n",
        "    centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "    clf = KMeans(2)\n",
        "    distance = clf.compute_distance(np.array(points),np.array(centroid_r))\n",
        "    distance_for_0 = [1.44121815, 5.16670124]\n",
        "    assert(np.linalg.norm(distance_for_0-distance[0]) <= 1e-2)\n",
        "test_distance()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQEQdSf7dq0V"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_distance', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I_eYli_idq2G"
      },
      "outputs": [],
      "source": [
        "def test_find_clusters():\n",
        "  centroid_r = [[1.5185255, 1.45970038],\n",
        "      [4.51568108,4.54138552]]\n",
        "  clf = KMeans(2)\n",
        "  distance = clf.compute_distance(np.array(points),np.array(centroid_r))\n",
        "  cluster = clf.find_closest_cluster(distance)\n",
        "  assert(cluster[0] == 0)\n",
        "test_find_clusters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ib9uRPtJdq3q"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_find_clusters', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFmAR2IJdx2j"
      },
      "outputs": [],
      "source": [
        "def test_fit():\n",
        "  clf = KMeans(2)\n",
        "  clusters, centroids, _ = clf.fit(np.array(points),np.array([[1,1],[4,4]]))\n",
        "  centroid_r = [[1.54079082, 1.534581],\n",
        "      [4.49834714,4.495329]]\n",
        "  assert(np.linalg.norm(centroids - np.array(centroid_r)) <= 1e-2 )\n",
        "  assert(sum(np.array(clusters)-np.array(result)) == 0)\n",
        "test_fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Iv6H8xmdx5H"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "grader.grade(test_case_id = 'test_fit', answer = grader_serialize(KMeans))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEjBNHRAd5Sy"
      },
      "source": [
        "## **1.3. [2 pts] Compute distortion**\n",
        "\n",
        "One way to decide on a value for k is to run K-means and plot the distortion (sum of squared error between each point and its assigned centroid). From that we can find the \"elbow of the graph\" that indicates the best tradeoff between number of clusters and corresponding distortion. See distortion equation below, where $C_i$ represents the centroid assigned to point i:\n",
        ">$ \\sum_{i=1}^{n}{(X_i - C_i)^2} $\n",
        "\n",
        "In the function `test_cluster_size`, iterate over possible cluster sizes from 2 to a `max_cluster` (inclusive) value. For each *k* from 2 to `max_k`, run K-means and calculate its distortion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QadOknBbd5Yl"
      },
      "outputs": [],
      "source": [
        "def test_cluster_size(X, max_k):\n",
        "    \"\"\"\n",
        "    Iterates over possible cluster from 2 to max_k, running k-means and calculating distortion.\n",
        "\n",
        "    INPUT:\n",
        "    X - m by n matrix, where m is the number of training points\n",
        "    max_k - the maximum number of clusters to consider\n",
        "\n",
        "    OUTPUT:\n",
        "    scores - a list of scores, that contains the distortion for k = 2 to max_k, in order.\n",
        "    \"\"\"\n",
        "    scores = [0] * (max_k-1)\n",
        "    # STUDENT TODO START:\n",
        "\n",
        "    # STUDENT TODO END\n",
        "    return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8efXTjqHd8iP"
      },
      "outputs": [],
      "source": [
        "def test_test_cluster_size():\n",
        "  scores = test_cluster_size(np.array(points),5)\n",
        "  assert(np.argmax(scores) == 0)\n",
        "test_test_cluster_size()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xBiO6N-jd89g"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "max_k = 20\n",
        "scores = test_cluster_size(X, max_k)\n",
        "grader.grade(test_case_id = 'test_test_cluster_size', answer = scores)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxg6MtUXd_zG"
      },
      "source": [
        "## **1.4. [2 pts, manually graded] Plot distortion vs. k (without feature scaling)**\n",
        "\n",
        "Plot **distortion vs. different k values** by using the function we just wrote on dataset X (no feature scaling) and add it in the written report. Use `max_k` = 20. Determine the best k value from this plot and also mention it in the written report. Make sure your plot has **axes labels, a legend labeling the different k values, and a title**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ou2XslKPd9AK"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START:\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUdyEHPbeDgb"
      },
      "source": [
        "## **1.5. [2 pts, manually graded] Plot distortion vs. k (with feature scaling)**\n",
        "\n",
        "What we just did was running k-means clustering over the dataset X without any feature scaling. This time, we will rescale each feature to the standard range of (0,1) before passing it to k-means and computing the distortion.\n",
        "\n",
        "Use `sklearn.preprocessing.MinMaxScaler` ([docs](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html)) and scale the dataset X before passing it to the `test_cluster_size` function. As before, plot **distortion vs. different k values** and add it in the written report. Use `max_k` = 20. Determine the best k value from this plot and also mention it in the written report. Make sure your plot has **axes labels, a legend labeling the different k values, and a title**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PeqKP3g7eEGN"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START:\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7JCX-tmzeHLV"
      },
      "source": [
        "## **1.6. [5 pts, manually graded 5190 only] Comments about K-Means**\n",
        "\n",
        "Answer these questions in the written report.\n",
        "\n",
        "1. Why do you get different results with and without feature scaling?\n",
        "2. Should you scale the features before fitting k-means? Why or why not?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "gcvbHDg9ggX0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Nw2VyudyPs"
      },
      "source": [
        "# **2. [4190: 2 autograded, 8 manual; 5190: 2 autograded, 13 manual] Principal Component Analysis**\n",
        "\n",
        "## **2.1. [6 pts, manually graded] Exploring Effects of Different Principal Components in Linear Regression**\n",
        "We have introduced you a way of dimension reduction, Principal Component Analysis, in class. Now, we would like to ask you to apply PCA from `sklearn` on the breast cancer dataset to observe its performance and interpret the major components.\n",
        "\n",
        "In order to better compare the effects of PCA, we load the labels from the dataset **without feature scaling**. Then, we will evaluate the performances of raw dataset and various numbers of PCA components on the `LinearRegression` classifier.\n",
        "\n",
        "In the section, you are asked to draw a plot of **test accuracies vs number of different principal components**. The detailed instructions are included in the following cells. Remember to **attach the plot** in your written submission, along with **axes labels and a title**. Also **make comments** about what you observe, explain the reason behind the trend, and what conclusion you could draw from the graph."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhGf2QNbfI4o"
      },
      "outputs": [],
      "source": [
        "# load the label from the dataset, which is a binary label 0/1 representing whether the cancer is benign or malignant\n",
        "\n",
        "# STUDENT TODO START:\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7HJgqEDfcOT"
      },
      "outputs": [],
      "source": [
        "# try raw data vs PCA data\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# STUDENT TODO START:\n",
        "# Step 1: split the data into train and test set by a test_size of 0.33.\n",
        "\n",
        "\n",
        "# Step 2: Train a linear regression model using train set and predict on the test set.\n",
        "# As the labels are binary, we should cast the predictions into binary labels as well. (Set predictions >=0.5 as 1)\n",
        "# You might want to print out accuracy scores here\n",
        "\n",
        "# Step 3: Iterate the number of components from 1 to 10 (exclusive).\n",
        "# For each number of PCs, we are training a linear regression model and save its accuracy on the test set following the same style as above.\n",
        "# Remember to only fit the train set and not the test set.\n",
        "# You might want to store your accuracies in a list\n",
        "\n",
        "# Step 4: Make a plot to compare accuracy vs number of PCs on Linear Regression for the test set.\n",
        "# Add a black, dashed line for the test accuracy of linear regression by feeding the raw input data.\n",
        "# Remeber to add x, y labels and title to your plot, and comment on your observations.\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DwlnQRmqoKoU"
      },
      "source": [
        "## **2.2. [4 pts] Understanding PCA**\n",
        "\n",
        "### **2.2.1 [2 pts, autograded] Explained Ratio of PCA**\n",
        "Given a threshold of explained ratio (0 < ratio < 1), compute the number of required PCs to reach the threshold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWE8VTsguO7Y"
      },
      "outputs": [],
      "source": [
        "def select_n_principal_components(data, variation):\n",
        "  # STUDENT TODO START:\n",
        "\n",
        "  # STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jrkdjex2qMb0"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "student_ans = [select_n_principal_components(cancer_dataset['data'], 0.98), select_n_principal_components(cancer_dataset['data'], 0.99)]\n",
        "grader.grade(test_case_id = 'test_select_n_principal_components', answer = student_ans)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lF0A9640q_cd"
      },
      "source": [
        "### **2.2.2 [2 pts, manually graded] Composition of PCA's Principal Components**\n",
        "In this section, we ask you to understand which features specifically in the dataset contribute to the most important PCs. We ask that you select the **best number of principal components** you got from **Section 2.1** and analyze their composition. Please comment on and analyze the **top 3 features** that make up the PCs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkjwg6H6u23u"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START:\n",
        "\n",
        "# STUDENT TODO END\n",
        "\n",
        "# Display the PCs and related metrics\n",
        "df = pd.DataFrame(abs(pca.components_.T),index=cancer_dataset['feature_names'],columns = ['PC1','PC2', 'PC3','PC4'])\n",
        "df['total'] = df.sum(axis=1)\n",
        "df.sort_values(by='total', ascending=False).head(3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EpNOIfoWnAQq"
      },
      "source": [
        "## **2.3. [5 pts, manually graded 5190 only] PCA and KMeans**\n",
        "It is common practice to run PCA and KMeans together because PCA reduces the number of features and thus variance, enabling KMeans to perform better, especially given how poorly it performs in high dimensions due to the curse of dimensionality.\n",
        "\n",
        "We first run PCA on the dataset for visualization in 2D space. Note that K-means is actually being fit on the entire feature set.\n",
        "\n",
        "Next, call your K-means class on the dataset X and obtain the clusters. **Make sure to populate the `clusters` variable here.** We have provided the plotting code for you.\n",
        "\n",
        "**Add these plots in the written report.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YWI-2KQmd29p"
      },
      "outputs": [],
      "source": [
        "# PCA for visualization in 2D.\n",
        "pca = PCA(n_components=2)\n",
        "pca.fit(X)\n",
        "v = pca.components_\n",
        "\n",
        "for k in [3,5,7,9, 11]:\n",
        "\n",
        "    clusters = np.zeros(X.shape[0])\n",
        "\n",
        "    # STUDENT TODO START:\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    plt.scatter(X[:, 0], X[:, 1], c=clusters, s=18)\n",
        "    plt.title(\"Breast Cancer Clusters (k = \"+str(k) + \")\")\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming X is your data matrix with shape (569, 30)\n",
        "\n",
        "# PCA for visualization in 2D\n",
        "pca = PCA(n_components=2)\n",
        "X_pca = pca.fit_transform(X)  # Shape: (569, 2)\n",
        "\n",
        "# Explained variance\n",
        "print(\"Explained variance by each component:\", pca.explained_variance_ratio_)\n",
        "\n",
        "# Iterate over different values of k for KMeans\n",
        "for k in [3, 5, 7, 9, 11]:\n",
        "    # Initialize KMeans with k clusters and a fixed random state for reproducibility\n",
        "    kmeans = KMeans(n_clusters=k)\n",
        "\n",
        "    # Fit KMeans on the original data and get cluster assignments\n",
        "    clusters, _, _ = kmeans.fit(X)\n",
        "\n",
        "    # Plot the PCA-transformed data colored by cluster assignments\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=clusters, cmap='viridis', s=50)\n",
        "    plt.title(f\"Breast Cancer Clusters (k = {k})\")\n",
        "    plt.xlabel('Principal Component 1')\n",
        "    plt.ylabel('Principal Component 2')\n",
        "    plt.colorbar(scatter, label='Cluster')\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "mqOhRLsyH4vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoidSevnpiqu"
      },
      "source": [
        "# **3. Image Classification using CNN [14 pts, autograded]**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iaLhD7tcacNK"
      },
      "source": [
        "#### **Import libraries**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mgTR67PEQhn6"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms import ToTensor\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydDc504-akhk"
      },
      "source": [
        "#### **Set the random seed**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IgoqGPBPdGBf"
      },
      "outputs": [],
      "source": [
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2bidiHXawdT"
      },
      "source": [
        "#### **Set GPU**: Make sure you are using `cuda`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5lI0V_BYa3LP"
      },
      "outputs": [],
      "source": [
        "# Make sure you're using cuda (GPU) by checking the hardware accelerator under Runtime -> Change runtime type\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"We're using:\", device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dH1H9cSS4nc"
      },
      "source": [
        "#### **Download and extract the data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Ry7qltBzb_1"
      },
      "outputs": [],
      "source": [
        "if not os.path.exists(\"cis5190_hw3_supertuxkart_data.zip\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw3/cis5190_hw3_supertuxkart_data.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EAhGG46XzibJ"
      },
      "outputs": [],
      "source": [
        "!unzip \"cis5190_hw3_supertuxkart_data.zip\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWRZZ6Gd3O5f"
      },
      "source": [
        "## **3.1. Dataset class implementation**\n",
        "\n",
        "In this section, you will be training, validating and testing a CNN model to classify images of objects from a car racing video game called SuperTuxKart. There are 6 classes of objects: kart is 1, pickup is 2, nitro is 3, bomb is 4 and projectile 5. The background class (all other images) is assigned the label 0. First, you need to load data in a way that PyTorch can deal with easily. We will lean on PyTorchâ€™s `Dataset` class to do this.\n",
        "\n",
        "Complete the `STKDataset` class that inherits from `Dataset`.\n",
        "\n",
        "1. `__init__` is a constructor, and would be the natural place to perform operations common to the full dataset, such as parsing the labels and image paths.\n",
        "2. The `__len__` function should return the size of the dataset, i.e., the number of samples.\n",
        "3. The `__getitem__` function should return a python tuple of (image, label). The image should be a torch.Tensor of size (3, 64, 64) and the label should be an int.\n",
        "\n",
        "The labels of the images under a particular folder (`train/` or `val/`) are stored in the same folder as `labels.csv`. Read the `labels.csv` file using `pandas` to understand what it looks like before proceeding. There is also a `labels.csv` in the `test/` folder. That would only contain the file names of the test samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "clNM_8vS4r_0"
      },
      "outputs": [],
      "source": [
        "ENCODING_TO_LABELS = {0: \"background\",\n",
        "                    1: \"kart\",\n",
        "                    2: \"pickup\",\n",
        "                    3: \"nitro\",\n",
        "                    4: \"bomb\",\n",
        "                    5: \"projectile\"}\n",
        "\n",
        "LABELS_TO_ENCODING = {\"background\": 0,\n",
        "                    \"kart\": 1,\n",
        "                    \"pickup\": 2,\n",
        "                    \"nitro\": 3,\n",
        "                    \"bomb\": 4,\n",
        "                    \"projectile\": 5}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMx57cqWySrk"
      },
      "outputs": [],
      "source": [
        "class STKDataset(Dataset):\n",
        "\n",
        "    def __init__(self, image_path, transform=None):\n",
        "        self.image_path = image_path\n",
        "        self.labels = pd.read_csv(image_path + \"/labels.csv\")\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "\n",
        "        # STUDENT TODO START: Return the number of samples in the dataset\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # STUDENT TODO START: Create the path to each image by joining the root path with the name of the file as found in labels.csv\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        # Read the image from the file path\n",
        "        image = Image.open(img_name)\n",
        "        # Transform the image using self.transform\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        if \"label\" in self.labels.columns:\n",
        "            # STUDENT TODO START: Extract label name and encode it using the LABELS_TO_ENCODING dictionary\n",
        "\n",
        "            # STUDENT TODO END\n",
        "            sample = (image, label)\n",
        "        else:\n",
        "            sample = (image)\n",
        "        return sample"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cVLSRk2r4wff"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START: Use transforms.Compose to transform the image such that every pixel takes on a value between -1 and 1\n",
        "# Hint: Refer to transforms.ToTensor() and transforms.Normalize()\n",
        "\n",
        "# STUDENT TODO END\n",
        "\n",
        "train_dataset = STKDataset(image_path=\"train\", transform=transform)\n",
        "val_dataset = STKDataset(image_path=\"val\", transform=transform)\n",
        "test_dataset = STKDataset(image_path=\"test\", transform=transform)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9xU1vMHZTZqA"
      },
      "source": [
        "#### **Visualization**\n",
        "\n",
        "The following cell visualizes the data as a sanity check for your implementation of the `STKDataset` class."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qneu5Tqu5HwP"
      },
      "outputs": [],
      "source": [
        "figure = plt.figure(figsize=(8, 8))\n",
        "cols, rows = 3, 3\n",
        "torch.manual_seed(0)\n",
        "for i in range(1, cols * rows + 1):\n",
        "    sample_idx = torch.randint(len(train_dataset), size=(1,)).item()\n",
        "    img, label = train_dataset[sample_idx]\n",
        "    figure.add_subplot(rows, cols, i)\n",
        "    plt.title(ENCODING_TO_LABELS[label])\n",
        "    plt.axis(\"off\")\n",
        "    plt.imshow(img.permute(1, 2, 0)*0.5 + 0.5)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iO62UxlvT_-B"
      },
      "source": [
        "#### **Data loaders**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vEeDpfBn-FjK"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START: Create data loaders for training, validation, and test sets each having a batch size of 64.\n",
        "# Set shuffle to be True for the training data loader, False for validation and test data loader.\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mc_f7OeMcaeh"
      },
      "source": [
        "## **3.2. CNN architecture**\n",
        "\n",
        "Your goal is to devise a CNN that passes the threshold accuracy (80%) on the test set. You get full score (20 pts) if you get at least 80% test set accuracy and 0 if you get 30% or below. The score varies linearly between 0 and 20 for accuracies between 30% and 80%.\n",
        "\n",
        "There are several decisions that you take in building your CNN including but not limited to:\n",
        "\n",
        "- the number of convolutional layers\n",
        "- the kernel size, stride, padding and number of out channels for each convolutional layer\n",
        "- number of fully connected layers\n",
        "- number of nodes in each fully connected layer\n",
        "\n",
        "You are free to decide the architecture. To make your search easier, we recommend you to use **not more than four convolutional layers and four fully connected layers**. We also suggest that you use the **relu activation function** between the layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CJJEh5rBRmQ"
      },
      "outputs": [],
      "source": [
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # STUDENT TODO START: Create the layers of your CNN here\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def forward(self, x):\n",
        "        # STUDENT TODO START: Perform the forward pass through the layers\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "# STUDENT TODO START: Create an instance of Net and move it to the GPU\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uo7XH85FfrXl"
      },
      "source": [
        "## **3.3. Training, validation, and testing**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-hfsnLHCCxt5"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START:\n",
        "# 1. Set the criterion to be cross entropy loss\n",
        "\n",
        "\n",
        "# 2. Experiment with different optimizers\n",
        "\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-dECXr5bCz-6"
      },
      "outputs": [],
      "source": [
        "train_loss, validation_loss = [], []\n",
        "train_acc, validation_acc = [], []\n",
        "\n",
        "# STUDENT TODO START:\n",
        "# Note that we have set the number of epochs to be 10. You can choose to increase or decrease the number of epochs.\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "\n",
        "        inputs, labels = data\n",
        "        # 1. Store the inputs and labels in the GPU\n",
        "\n",
        "        # 2. Get the model predictions\n",
        "\n",
        "        # 3. Zero the gradients out\n",
        "\n",
        "        # 4. Get the loss\n",
        "\n",
        "        # 5. Calculate the gradients\n",
        "\n",
        "        # 6. Update the weights\n",
        "\n",
        "\n",
        "    for i, data in enumerate(val_dataloader, 0):\n",
        "\n",
        "        # 1. Store the inputs and labels in the GPU\n",
        "\n",
        "        # 2. Get the model predictions\n",
        "\n",
        "        # 3. Get the loss\n",
        "\n",
        "\n",
        "\n",
        "    print(f\"Epoch {epoch+1}:\")\n",
        "\n",
        "    print(f\"Training Loss:\", round(train_loss[epoch], 3))\n",
        "    print(f\"Validation Loss:\", round(validation_loss[epoch], 3))\n",
        "\n",
        "    print(f\"Training Accuracy:\", round(train_acc[epoch], 3))\n",
        "    print(f\"Validation Accuracy:\", round(validation_acc[epoch], 3))\n",
        "\n",
        "    print(\"------------------------------\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Be97yhnt4sAf"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "test_predictions = np.array([])\n",
        "\n",
        "for i, data in enumerate(test_dataloader, 0):\n",
        "\n",
        "    inputs = data\n",
        "    # STUDENT TODO START:\n",
        "    # 1. Store the inputs in the GPU\n",
        "\n",
        "    # 2. Get the model predictions\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    _, predicted = torch.max(predictions, 1)\n",
        "\n",
        "    test_predictions = np.concatenate((test_predictions, predicted.detach().cpu().numpy()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SSzSgCbYH8Zi"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_cnn_predictions', answer = test_predictions)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD6VKdQIwcxg"
      },
      "source": [
        "# **Submit to Gradescope**\n",
        "You've finished the homework. Please submit your final notebook on [Gradescope](gradescope.com)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "377c3e77380f886ab555d62b93e59a1648fc55affccd8d0220be3281f77f4c6d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}