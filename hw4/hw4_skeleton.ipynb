{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qs0yamHN-dY0"
      },
      "source": [
        "# **CIS 4190/5190 Fall 2025 - Homework 4**\n",
        "\n",
        "**Before starting, you must click on the \"Copy To Drive\" option in the top bar. Go to File --> Save a Copy to Drive. This is the master notebook so <u>you will not be able to save your changes without copying it </u>! Once you click on that, make sure you are working on that version of the notebook so that your work is saved**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb-WLp5Z-cdy"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import sys\n",
        "import matplotlib.pyplot as plt\n",
        "from numpy.linalg import *\n",
        "np.random.seed(42)  # don't change this line or one below\n",
        "random.seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O2VtEzsZ-loR"
      },
      "outputs": [],
      "source": [
        "# For autograder only, do not modify this cell.\n",
        "# True for Google Colab, False for autograder\n",
        "NOTEBOOK = (os.getenv('IS_AUTOGRADER') is None)\n",
        "if NOTEBOOK:\n",
        "    print(\"[INFO, OK] Google Colab.\")\n",
        "else:\n",
        "    print(\"[INFO, OK] Autograder.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cjjXBdEb-p8K"
      },
      "source": [
        "# **PennGrader Setup**\n",
        "\n",
        "First, you'll need to set up the PennGrader, an autograder we are going to use throughout the semester. The PennGrader will automatically grade your answer and provide you with an instant feedback. Unless otherwise stated, you can resubmit up to a reasonable number of attempts (e.g. 100 attemptes per day). **We will only record your latest score in our backend database**.\n",
        "\n",
        "After finishing each homework assignment, you must submit your iPython notebook to gradescope before the homework deadline. Gradescope will then retrive and display your scores from our backend database."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GCTLN4G-nK2"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install penngrader-client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9-6ADpans8Da"
      },
      "outputs": [],
      "source": [
        "%%writefile student_config.yaml\n",
        "grader_api_url: 'https://23whrwph9h.execute-api.us-east-1.amazonaws.com/default/Grader23'\n",
        "grader_api_key: 'flfkE736fA6Z8GxMDJe2q8Kfk8UDqjsG3GVqOFOa'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QLnoPRci-sTC"
      },
      "outputs": [],
      "source": [
        "from penngrader.grader import *"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu0XYZHO-t8J"
      },
      "outputs": [],
      "source": [
        "#PLEASE ENSURE YOUR PENN-ID IS ENTERED CORRECTLY. IF NOT, THE AUTOGRADER WON'T KNOW WHO\n",
        "#TO ASSIGN POINTS TO YOU IN OUR BACKEND\n",
        "STUDENT_ID = 12345678          # YOUR PENN-ID GOES HERE AS AN INTEGER#"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIDTGGbo-xkf"
      },
      "source": [
        "Run the following cell to initialize the autograder. This autograder will let you submit your code directly from this notebook and immidiately get a score.\n",
        "\n",
        "**NOTE:** Remember we store your submissions and check against other student's submissions... so, not that you would, but no cheating."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bw_QDnZk-vvI"
      },
      "outputs": [],
      "source": [
        "grader = PennGrader('student_config.yaml', 'cis5190_f25_HW4', STUDENT_ID, STUDENT_ID)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SMtzAPxytu4d"
      },
      "outputs": [],
      "source": [
        "# Serialization code needed by the autograder\n",
        "import inspect, sys\n",
        "from IPython.core.magics.code import extract_symbols\n",
        "\n",
        "def new_getfile(object, _old_getfile=inspect.getfile):\n",
        "    if not inspect.isclass(object):\n",
        "        return _old_getfile(object)\n",
        "\n",
        "    # Lookup by parent module (as in current inspect)\n",
        "    if hasattr(object, '__module__'):\n",
        "        object_ = sys.modules.get(object.__module__)\n",
        "        if hasattr(object_, '__file__'):\n",
        "            return object_.__file__\n",
        "\n",
        "    # If parent module is __main__, lookup by methods (NEW)\n",
        "    for name, member in inspect.getmembers(object):\n",
        "        if inspect.isfunction(member) and object.__qualname__ + '.' + member.__name__ == member.__qualname__:\n",
        "            return inspect.getfile(member)\n",
        "    else:\n",
        "        raise TypeError('Source for {!r} not found'.format(object))\n",
        "inspect.getfile = new_getfile\n",
        "\n",
        "def grader_serialize(obj):\n",
        "    cell_code = \"\".join(inspect.linecache.getlines(new_getfile(obj)))\n",
        "    class_code = extract_symbols(cell_code, obj.__name__)[0][0]\n",
        "    return class_code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcrJaJ73Rk9v"
      },
      "source": [
        "# **1. [4190: 24 autograded; 5190: 24 autograded] Natural Language Processing**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yp9rryptRoLR"
      },
      "source": [
        "#### Stanford Sentiment Treebank (SST)\n",
        "\n",
        "We'll introduce the [Stanford Sentiment Treebank](https://nlp.stanford.edu/sentiment/index.html) (SST) dataset, and use a Naive Bayes model as a simple baseline. The SST was introduced by [(Socher et al. 2013)](http://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf) and it consists of approximately 10,000 sentences from movie reviews. It consists of 11,855 sentences drawn from a corpus of movie reviews (originally from Rotten Tomatoes), each labeled with sentiment on a five-point scale ans is a widely used dataset as a benchmark for text classification.\n",
        "\n",
        "An example of the five-point scale is:\n",
        "```\n",
        "sentence: [A warm , funny , engaging film .]\n",
        "label:    4 (very positive)\n",
        "```\n",
        "\n",
        "**Note:** Unlike most classification datasets, SST is also a _treebank_, which means each sentence is associated with a tree structure that decomposes it into subphrases. So for the example above, we'd also have sentiment labels for `[warm , funny]` and `[engaging film .]` and so on. The tree structure will comes in handy for complex NLP tasks and we will be using it briefly to analyze an example that has negation. The data is distributed as serialized trees in [S-expression](https://en.wikipedia.org/wiki/S-expression) form, like this:\n",
        "```\n",
        "(4 (4 (2 A) (4 (3 (3 warm) (2 ,)) (3 funny))) (3 (2 ,) (3 (4 (4 engaging) (2 film)) (2 .))))\n",
        "```\n",
        "\n",
        "We've downladed the dataset and parse the S-expressions into a dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FByUmOl14eyC"
      },
      "outputs": [],
      "source": [
        "!pip3 install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wothm2Pot6-Z"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "import random, os, sys, re, json, time, datetime, shutil\n",
        "import itertools, collections\n",
        "from collections import defaultdict, Counter\n",
        "from importlib import reload\n",
        "\n",
        "# NLTK, NumPy, and Pandas.\n",
        "import nltk\n",
        "from nltk.tree import Tree\n",
        "import numpy as np\n",
        "from numpy import random as rd\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ay1YROpJAZ_i"
      },
      "outputs": [],
      "source": [
        "# Constants for use by other modules.\n",
        "START_TOKEN = u\"<s>\"\n",
        "END_TOKEN   = u\"</s>\"\n",
        "UNK_TOKEN   = u\"<unk>\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ma4E29Xe8Hs"
      },
      "source": [
        "#### Datasets:\n",
        "Next, we will download the dataset from Github to your local runtime. After successful download, you may verify that all datasets are present in your Colab instance.\n",
        "\n",
        "- [train parquet file](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train.parquet)\n",
        "\n",
        "- [dev parquet file](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/dev.parquet)\n",
        "\n",
        "- [test parquet file](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/test.parquet)\n",
        "\n",
        "- [tokens in training data](https://www.cis.upenn.edu/~myatskar/teaching/cis519/a5/train_tokens.txt)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaG831ctwsao"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "  if not os.path.exists(\"train.parquet\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw4/train.parquet\n",
        "  if not os.path.exists(\"dev.parquet\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw4/dev.parquet\n",
        "  if not os.path.exists(\"test.parquet\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw4/test.parquet\n",
        "  if not os.path.exists(\"train_tokens.txt\"):\n",
        "    !wget https://raw.githubusercontent.com/upenn/cis-4190-5190-fall-25/main/hw4/train_tokens.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v0JC3vd0cvfG"
      },
      "outputs": [],
      "source": [
        "train_file = \"train.parquet\"\n",
        "dev_file = \"dev.parquet\"\n",
        "test_file = \"test.parquet\"\n",
        "vocab_file = \"train_tokens.txt\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olCRGZcIJuEf"
      },
      "source": [
        "Below are some helper code to download and build the dataset, you do not need to modify these."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fDhs_mpQAQIt"
      },
      "outputs": [],
      "source": [
        "class SSTDataset(object):\n",
        "\n",
        "    Example_fields = [\"tokens\", \"ids\", \"label\", \"is_root\", \"root_id\"]\n",
        "    Example = collections.namedtuple(\"Example\", Example_fields)\n",
        "\n",
        "\n",
        "    def canonicalize(self, raw_tokens):\n",
        "        wordset=(self.vocab.wordset if self.vocab else None)\n",
        "        return canonicalize_words(raw_tokens, wordset=wordset)\n",
        "\n",
        "    def __init__(self,train_file,dev_file,test_file,vocab_file,V=20000):\n",
        "        self.vocab = None\n",
        "        self.train = pd.read_parquet(train_file)\n",
        "        self.dev = pd.read_parquet(dev_file)\n",
        "        self.test = pd.read_parquet(test_file)\n",
        "        train_words =[]\n",
        "        with open(vocab_file) as f:\n",
        "            train_words = f.readlines()\n",
        "        train_words = [w.strip() for w in train_words]\n",
        "        # # Build vocabulary over training set\n",
        "        self.vocab = Vocabulary(train_words, size=V)\n",
        "        print(\"Train set has {:,} words\".format(self.vocab.size))\n",
        "        self.target_names = [0,1]\n",
        "\n",
        "    def get_filtered_split(self, split='train',is_root = True):\n",
        "        df = getattr(self, split)\n",
        "        if is_root:\n",
        "            df = df[df.is_root]\n",
        "        return df\n",
        "\n",
        "    def as_padded_array(self, split='train', max_len=40, pad_id=0,is_root = True):\n",
        "        df = self.get_filtered_split(split,is_root)\n",
        "        x, ns = pad_np_array(df.ids, max_len=max_len, pad_id=pad_id)\n",
        "        y = np.empty((1,1))\n",
        "        if split != 'test':\n",
        "            y  = np.array(df.label, dtype=np.int32)\n",
        "        return x, ns, y\n",
        "\n",
        "    def as_sparse_bow(self, split='train',is_root = True):\n",
        "        from scipy import sparse\n",
        "        df = self.get_filtered_split(split,is_root)\n",
        "        x = id_lists_to_sparse_bow(df['ids'], self.vocab.size)\n",
        "        if split != 'test':\n",
        "            return x, np.array(df.label, dtype=np.int32)\n",
        "        return x\n",
        "\n",
        "def require_package(package_name):\n",
        "    import pkgutil\n",
        "    import subprocess\n",
        "    import sys\n",
        "    if not pkgutil.find_loader(package_name):\n",
        "        subprocess.check_call([sys.executable, '-m', 'pip', 'install', package_name])\n",
        "\n",
        "def canonicalize_digits(word):\n",
        "    if any([c.isalpha() for c in word]): return word\n",
        "    word = re.sub(\"\\d\", \"DG\", word)\n",
        "    if word.startswith(\"DG\"):\n",
        "        word = word.replace(\",\", \"\") # remove thousands separator\n",
        "    return word\n",
        "\n",
        "def canonicalize_word(word, wordset=None, digits=True):\n",
        "    word = word.lower()\n",
        "    if digits:\n",
        "        if (wordset != None) and (word in wordset): return word\n",
        "        word = canonicalize_digits(word) # try to canonicalize numbers\n",
        "    if (wordset == None) or (word in wordset):\n",
        "        return word\n",
        "    else:\n",
        "        return UNK_TOKEN\n",
        "\n",
        "def canonicalize_words(words, **kw):\n",
        "    return [canonicalize_word(word, **kw) for word in words]\n",
        "\n",
        "\n",
        "def pad_np_array(example_ids, max_len=250, pad_id=0):\n",
        "    arr = np.full([len(example_ids), max_len], pad_id, dtype=np.int32)\n",
        "    ns = np.zeros([len(example_ids)], dtype=np.int32)\n",
        "    for i, ids in enumerate(example_ids):\n",
        "        cpy_len = min(len(ids), max_len)\n",
        "        arr[i,:cpy_len] = ids[:cpy_len]\n",
        "        ns[i] = cpy_len\n",
        "    return arr, ns\n",
        "\n",
        "def id_lists_to_sparse_bow(id_lists, vocab_size):\n",
        "    from scipy import sparse\n",
        "    ii = []  # row indices (example ids)\n",
        "    jj = []  # column indices (token ids)\n",
        "    for row_id, ids in enumerate(id_lists):\n",
        "        ii.extend([row_id]*len(ids))\n",
        "        jj.extend(ids)\n",
        "    x = sparse.csr_matrix((np.ones_like(ii), (ii, jj)),\n",
        "                          shape=[len(id_lists), vocab_size])\n",
        "    return x\n",
        "\n",
        "class Vocabulary(object):\n",
        "\n",
        "    START_TOKEN = START_TOKEN\n",
        "    END_TOKEN   = END_TOKEN\n",
        "    UNK_TOKEN   = UNK_TOKEN\n",
        "\n",
        "    def __init__(self, tokens, size=None,\n",
        "                 progressbar=lambda l:l):\n",
        "        self.unigram_counts = Counter()\n",
        "        self.bigram_counts = defaultdict(lambda: Counter())\n",
        "        prev_word = None\n",
        "        for word in progressbar(tokens):  # Make a single pass through tokens\n",
        "            self.unigram_counts[word] += 1\n",
        "            self.bigram_counts[prev_word][word] += 1\n",
        "            prev_word = word\n",
        "        self.bigram_counts.default_factory = None  # make into a normal dict\n",
        "\n",
        "        # Leave space for \"<s>\", \"</s>\", and \"<unk>\"\n",
        "        top_counts = self.unigram_counts.most_common(None if size is None else (size - 3))\n",
        "        vocab = ([self.START_TOKEN, self.END_TOKEN, self.UNK_TOKEN] +\n",
        "                 [w for w,c in top_counts])\n",
        "\n",
        "        # Assign an id to each word, by frequency\n",
        "        self.id_to_word = dict(enumerate(vocab))\n",
        "        self.word_to_id = {v:k for k,v in self.id_to_word.items()}\n",
        "        self.size = len(self.id_to_word)\n",
        "        if size is not None:\n",
        "            assert(self.size <= size)\n",
        "\n",
        "        # For convenience\n",
        "        self.wordset = set(self.word_to_id.keys())\n",
        "\n",
        "        # Store special IDs\n",
        "        self.START_ID = self.word_to_id[self.START_TOKEN]\n",
        "        self.END_ID = self.word_to_id[self.END_TOKEN]\n",
        "        self.UNK_ID = self.word_to_id[self.UNK_TOKEN]\n",
        "\n",
        "    def words_to_ids(self, words):\n",
        "        return [self.word_to_id.get(w, self.UNK_ID) for w in words]\n",
        "\n",
        "    def ids_to_words(self, ids):\n",
        "        return [self.id_to_word[i] for i in ids]\n",
        "\n",
        "    def ordered_words(self):\n",
        "        \"\"\"Return a list of words, ordered by id.\"\"\"\n",
        "        return self.ids_to_words(range(self.size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CSRZ9eqxt6-b"
      },
      "outputs": [],
      "source": [
        "ds = SSTDataset(train_file,dev_file, test_file,vocab_file,V=20000)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kY5JE1DQt6-d"
      },
      "source": [
        "A few members of the `SSTDataset()` class that we will be using are:\n",
        "- **`ds.vocab`**: a `vocabulary.Vocabulary` object managing the model vocabulary.\n",
        "- **`ds.{train,dev,test}`**: a Pandas DataFrame containing the _processed_ examples, including all subphrases. `label` is the target label, `is_root` denotes whether this example is a root node (full sentence), and `tokens` are the tokenized words from the original sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16q8ruRjt6-k"
      },
      "source": [
        "Note if you set `root_only=True` the dataframe will return only examples corresponding to whole sentences. If you set `root_only=False` the dataframe will return examples for all phrases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9SXUCz-2YeP0"
      },
      "outputs": [],
      "source": [
        "is_root = False"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K2EMoXKcUQ1G"
      },
      "source": [
        "## **1.1 [4190: 16 autograded; 5190: 16 autograded] [Deep Averaging Networks](https://people.cs.umass.edu/~miyyer/pubs/2015_acl_dan.pdf)**\n",
        "\n",
        "We are going to implement the Deep Averaging Networks\n",
        "\n",
        "![dan](https://miro.medium.com/max/904/1*0LezMYWUk3pXptoMdO5M_Q.png)\n",
        "\n",
        "\n",
        "Vector space models for natural language processing (NLP) represent words using low dimensional vectors called embeddings. To apply vector space\n",
        "models to sentences or documents, one must first select an appropriate composition function, which combines multiple words into a single vector.\n",
        "\n",
        "Composition functions fall into two classes: unordered and syntactic. Unordered functions treat input texts as bags of word embeddings, while syntactic functions take word order and sentence structure\n",
        "into account. Syntactic functions outperform unordered functions on many tasks. However, there is a tradeoff: syntactic functions require more training time and computing resources.\n",
        "\n",
        "The deep averaging network (DAN) is a deep unordered model which that obtains near state-of-the-art accuracies on a variety of sentence and document-level tasks with just minutes of training time on an average laptop computer. It\n",
        "works in three simple steps:\n",
        "1. Take the vector average of the embeddings\n",
        "associated with an input sequence of tokens\n",
        "2. Pass that average through one or more feedforward layers\n",
        "3. Perform (linear) classification on the final\n",
        "layerâ€™s representation\n",
        "\n",
        "Furthermore, DANs, can be effectively trained on data that have high syntactic variance. The model works by magnifying tiny but meaningful differences in the vector average."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6S7vplWWPya4"
      },
      "source": [
        "We are going to use DANs for the same classification problem."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WVXHs8JIQEg8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WU2myqExRR1n"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import time\n",
        "import glob\n",
        "import numpy as np\n",
        "\n",
        "import sys\n",
        "from argparse import ArgumentParser"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q5ulmPzRwr3T"
      },
      "source": [
        "### **1.1.1 [4190: 6 autograded; 5190: 6 autograded] [Glove Embeddings](https://nlp.stanford.edu/projects/glove/)**\n",
        "We are downloading pretrained glove word vectors that has been trained on Common Crawl data, a snapshot of the whole web.\n",
        "These embeddings serve as excellent initilizations for embeddings our model needs.\n",
        "Please download the glove embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nT27aeXBVw01"
      },
      "outputs": [],
      "source": [
        "!wget -nc https://huggingface.co/stanfordnlp/glove/resolve/main/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip\n",
        "!ls -lat"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpUv8X36MEru"
      },
      "outputs": [],
      "source": [
        "glove_file = \"glove.840B.300d.txt\"\n",
        "train_x, train_ns, train_y = ds.as_padded_array(\"train\",is_root = is_root)\n",
        "dev_x, dev_ns, dev_y = ds.as_padded_array(\"dev\",is_root = is_root)\n",
        "test_x, test_ns,_  = ds.as_padded_array(\"test\",is_root = is_root)\n",
        "\n",
        "print(\"Training set: x = {:s} sparse, ns={:s}, y = {:s}\".format(str(train_x.shape), str(train_ns.shape),\n",
        "                                                str(train_y.shape)))\n",
        "print(\"Validation set: x = {:s} sparse,ns={:s}, y = {:s}\".format(str(dev_x.shape), str(dev_ns.shape),\n",
        "                                                str(dev_y.shape)))\n",
        "print(\"Test set:     x = {:s} sparse,ns={:s}\".format(str(test_x.shape), str(test_ns.shape)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qu-xCT7dIfwD"
      },
      "outputs": [],
      "source": [
        "#look at the format of the file\n",
        "!head glove.840B.300d.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFe9SkOcYugK"
      },
      "source": [
        "#### **1.1.1.1 [2 pts autograded] Get Glove embeddings**\n",
        "In this section we want to populate the `glove` dictionary with a mapping of word to the embedding. Remember: the embedding should be an `np.array` of type `np.float` The glove dictionary should only have words that are present in the train vocabulary.\n",
        "\n",
        "\n",
        "**Hint:**\n",
        "For getting the word and corresponding embedding from the glove file, remember to refer to the above structure of the word to embedding mapping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4RNiORwiYTzi"
      },
      "outputs": [],
      "source": [
        "#takes about 1 minute to read through the whole file and find the words we need.\n",
        "def get_glove_mapping(vocab, file):\n",
        "    \"\"\"\n",
        "    Gets the mapping of words from the vocabulary to pretrained embeddings\n",
        "\n",
        "    INPUT:\n",
        "    vocab       - set of vocabulary words\n",
        "    file        - file with pretrained embeddings\n",
        "\n",
        "    OUTPUT:\n",
        "    glove_map   - mapping of words in the vocabulary to the pretrained embedding\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    glove_map = {}\n",
        "    with open(file,'rb') as fi:\n",
        "        for l in fi:\n",
        "            try:\n",
        "                # STUDENT TODO START:\n",
        "\n",
        "                # 1. Decode the bytes into string and split into words\n",
        "\n",
        "                # 2. The first element of the array is the word\n",
        "\n",
        "                # 3. Only process if the word is in the vocabulary set\n",
        "\n",
        "                # 4. Take the rest of the elements as the vector\n",
        "\n",
        "                # 5. Assign the vector to the glove map for the corresponding word\n",
        "\n",
        "                # STUDENT TODO END\n",
        "            except:\n",
        "                # Some lines contains urls, which will raise an exception.\n",
        "                pass\n",
        "    return glove_map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYettRoymWTz"
      },
      "outputs": [],
      "source": [
        "vocab_set = set(ds.vocab.ordered_words())\n",
        "glove_map = get_glove_mapping(vocab_set,glove_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2K5Mm5a2bpda"
      },
      "outputs": [],
      "source": [
        "def test_glove_embedding(glove_map):\n",
        "    assert(len(glove_map.keys()) == 15506)\n",
        "    assert(\"November\" not in glove_map.keys())\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_glove_embedding(glove_map)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nGPjrf8rd__p"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_glove_embedding', answer = list(glove_map.keys()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HoZuTVB9mcZb"
      },
      "source": [
        "#### **1.1.1.2 [2 pts autograded] Dimensions required for the weight matrix**\n",
        "\n",
        "Fill in the dimensions required for weight matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "djhLc9X_Z2E0"
      },
      "outputs": [],
      "source": [
        "# STUDENT TODO START:\n",
        "d_out = #number of outputs\n",
        "n_embed = #size of the dictionary of embeddings\n",
        "d_embed = # the size of each embedding vector\n",
        "dims =(d_out,n_embed,d_embed)\n",
        "# STUDENT TODO END"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qr6jSgYPdJeM"
      },
      "outputs": [],
      "source": [
        "def test_dimensions(dims):\n",
        "    d_out,n_embed,d_embed = dims\n",
        "    assert(n_embed == 16474)\n",
        "    assert(d_out == 2)\n",
        "    assert(d_embed == 300)\n",
        "\n",
        "if NOTEBOOK:\n",
        "    test_dimensions(dims)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K3g0z7Y7e0MA"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_dimensions', answer = dims)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ct2rwpFxlytq"
      },
      "source": [
        "#### **1.1.1.3 [2 pts autograded] Initializing the weight matrix**\n",
        "\n",
        "Create `weights_matrix` for the parameters to be learnt. Initialize the weight matrix for a particular id with the glove embedding for the same id. If you do not find a particular word, initialize the weight matrix with `np.random.normal`\n",
        "\n",
        "Hint: `ds.vocab.ordered_words()` can give you the mapping of id to words. `glove` has the embeddings you need."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MMIZjt4HYuJB"
      },
      "outputs": [],
      "source": [
        "def get_weight_matrix(n_embed, d_embed, glove_map):\n",
        "    \"\"\"\n",
        "    Initialize the weight matrix\n",
        "\n",
        "    INPUT:\n",
        "    n_embed         - size of the dictionary of embeddings\n",
        "    d_embed         - the size of each embedding vector\n",
        "\n",
        "    OUTPUT:\n",
        "    weights_matrix  - matrix of mapping from word id to embedding\n",
        "\n",
        "    \"\"\"\n",
        "    # STUDENT TODO START:\n",
        "\n",
        "    # 1. Initialize zero matrix with the dimensions\n",
        "\n",
        "    # 2. Iterate through the vocabulary words\n",
        "\n",
        "    # -- if the weight is found in the glove map, set the matrix to that embedding\n",
        "\n",
        "    # -- else, assign to a random vector of normal distribution\n",
        "\n",
        "    # STUDENT TODO END\n",
        "    return weights_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rotHEiIqm2sV"
      },
      "outputs": [],
      "source": [
        "weights_matrix = get_weight_matrix(n_embed, d_embed, glove_map)\n",
        "weight_data = (weights_matrix.shape, weights_matrix[:155])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H-Na6gvrdoMN"
      },
      "outputs": [],
      "source": [
        "def test_weight_matrix(weight_data):\n",
        "    mat1 = [-0.18994 ,  0.11016 , -0.46874 ,  0.24375 ,  0.18241 ,  0.2649  ,\n",
        "       -0.025122, -0.58228 , -0.23545 ,  0.20763 ]\n",
        "    shape = (16474, 300)\n",
        "    for i in range(0,10):\n",
        "        if abs(mat1[i] - weight_data[1][150][200+i])>= 0.002:\n",
        "            assert(mat1[i] != weight_data[1][150][200+i])\n",
        "        if shape != weight_data[0]:\n",
        "            assert(shape != weight_data[0])\n",
        "if NOTEBOOK:\n",
        "    test_weight_matrix(weight_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2NHXsYBHgxya"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_weight_matrix', answer = weight_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6rGLI6jm-0g"
      },
      "source": [
        "#### **1.1.1.4 Creating Embedding Layer**\n",
        "Use the weight matrix to create the embedding layer by using `nn.Embedding`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XGB1pAabQAf"
      },
      "outputs": [],
      "source": [
        "def create_emb_layer(weights_matrix, non_trainable=False):\n",
        "    \"\"\"\n",
        "    Create the embedding layer\n",
        "\n",
        "    INPUT:\n",
        "    weights_matrix  - matrix of mapping from word id to embedding\n",
        "    non_trainable   - Flag for whether the weight matrix should be trained.\n",
        "                      If it is set to True, don't update the gradients\n",
        "\n",
        "    OUTPUT:\n",
        "    emb_layer       - embedding layer\n",
        "\n",
        "    \"\"\"\n",
        "    # STUDENT TODO START:\n",
        "\n",
        "    # 1. Extract the dimensions from weights_matrix\n",
        "\n",
        "    # 2. Create an embedding layer using the dimensions\n",
        "\n",
        "    # 3. Convert to tensor and update the embedding layer weight\n",
        "\n",
        "    # 4. If non_trainable is set to True, don't update the gradients\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    return emb_layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FI5HX8dGNQyR"
      },
      "source": [
        "#### **1.1.1.5 Defining the Dataloader**\n",
        "\n",
        "For the ease of batch processing, we are defining the following to use the functionality of the `Dataloader` in Pytorch.\n",
        "\n",
        "Note: The process of creating a mask for the word dropout."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tOubHTzb8CS2"
      },
      "outputs": [],
      "source": [
        "class SSTpytorchDataset(Dataset):\n",
        "    def __init__(self, sst_ds, word_dropout = 0.3, split='train'):\n",
        "        super(SSTpytorchDataset, self).__init__()\n",
        "        assert split in ['train', 'test', 'dev'], \"Error!\"\n",
        "        self.ds = sst_ds\n",
        "        self.split = split\n",
        "        self.word_dropout = word_dropout\n",
        "        self.data_x, self.data_ns, self.data_y = self.ds.as_padded_array(split,is_root =is_root)\n",
        "        self.mask = np.zeros_like(self.data_x)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.data_x.shape[0]\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "        y = 2\n",
        "        if self.split != 'test':\n",
        "            y = self.data_y[idx]\n",
        "\n",
        "        #Returning the mask for the dataloader\n",
        "\n",
        "        mask = np.zeros(len(self.data_x[idx]))\n",
        "        sentl = self.data_ns[idx]\n",
        "        total_dropped = 0\n",
        "        for j in range(0,sentl):\n",
        "            mask[j] = 1\n",
        "            if self.split == 'train':\n",
        "                rv = random.random()\n",
        "                if rv  < self.word_dropout:\n",
        "                    mask[j] = 0\n",
        "                    total_dropped+=1\n",
        "        if total_dropped >= sentl:\n",
        "            mask[0] = 1\n",
        "        for i in range(sentl,len(self.data_x[idx])):\n",
        "            mask[i] = 0\n",
        "        self.mask[idx] = mask\n",
        "        return self.data_x[idx], self.data_ns[idx], self.mask[idx], y\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uKlnbGShNCDV"
      },
      "source": [
        "### **1.1.2 [10 pts autograded] Training**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnSMLJcrsYC0"
      },
      "source": [
        "####  Masked Averaging\n",
        "\n",
        "In this section, you will need to compute the average word embedding of tokens in the input. One complication is that sentences come in different lengths, and we will need to keep track of this to correctly average.\n",
        "\n",
        "When a sentence is input into our network, it is mapped to list of token ids, up to some maximum length. We construct a matrix, M, where each row corresponds to a sentence, and entries correspond to integers representing tokens. Some sentences are, of course, shorter than this maximum length. For these sentences, we fill in the remaining elements of M with a pad index, up to the max length. This is a special pad index indicating we are beyond the end of a sentence. The dataloader takes care of this for you. When averaging, we need to ignore these elements.\n",
        "\n",
        "Irrespective of if a token is pad or a real token, the first step is to look up an embedding for the index in our embedding table (the first line of the forward method). At this point we will have retrieved some vectors that correspond to the pad tokens as well. We need to ignore these, and only average vectors that correspond to non-pad symbols.\n",
        "\n",
        "To help do so, often NLP applications will introduce a mask as part of the input. The mask is a binary vector for every sentence, where each position encodes whether the token is really from the sentence, or instead should be ignored. The shape of the mask is batch_size by maximum_length. Again, the dataloader has taken care of this for you. Your job will be to use this mask to ignore the embeddings components we don't want to average over.\n",
        "\n",
        "You have to perform the following steps:\n",
        "\n",
        "1. Change the view of the mask so it extends to the embeddings size. It started `batch` by `max_sent`, but we need it to be `batch` by `max_sent` by `d_embed`. The [expand](https://pytorch.org/docs/stable/generated/torch.Tensor.expand.html) Pytorch function will help.\n",
        "2. Pointwise multiply the expanded mask with the embeddings, to eliminate the tokens that aren't in the mask, and sum the rest (this is the `numerator` of our average). Remember: the mask is a binary vector, so the zeros correspond to elements we don't want in our average. The output of this sum should be `batch` by `d_embed`.\n",
        "4. Calculate the number of words in each sentence (this is the `denominator` of our average)\n",
        "3. return `x = numerator/denoninator`, the average"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M0aq_l_f4xiv"
      },
      "source": [
        "#### Defining the architecture for Deep Averaging Networks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1q7oqyrCP8fK"
      },
      "outputs": [],
      "source": [
        "import random as random\n",
        "\n",
        "class DAN(nn.Module):\n",
        "\n",
        "    def __init__(self,\n",
        "                 n_embed=20000,\n",
        "                 d_embed=300,\n",
        "                 d_hidden=100,\n",
        "                 d_out=2,\n",
        "                 layer_dropout = 0.2,\n",
        "                 word_dropout = 0.3,\n",
        "                 embeddings=None,\n",
        "                 depth = 0):\n",
        "        super(DAN, self).__init__()\n",
        "\n",
        "        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        self.embed = create_emb_layer(weights_matrix,False)\n",
        "\n",
        "        self.fc_out = nn.Linear(d_hidden, d_out)\n",
        "        self.word_dropout = word_dropout\n",
        "\n",
        "    def masked_mean(self,v, mask):\n",
        "        \"\"\"\n",
        "        Create the masked mean\n",
        "\n",
        "        INPUT:\n",
        "        v       - input\n",
        "        mask    - mask that has 0 and 1 for all the tokens in the input\n",
        "                  0 corresponds to a token we should not include in the average and 1 otherwise\n",
        "\n",
        "        OUTPUT:\n",
        "        x       - average\n",
        "\n",
        "        \"\"\"\n",
        "        (batch, max_sent, d_embed ) = v.size() #these values we will be useful for expanding the mask\n",
        "        # STUDENT TODO START:\n",
        "        # 1. Reshape the mask and expand to the embeddings\n",
        "\n",
        "        # 2. Sum the number of non-masked tokens\n",
        "\n",
        "        # 3. Eliminate the masked tokens and sum the number of remaining tokens\n",
        "\n",
        "        # 4. Take the average embedding as x\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return x\n",
        "\n",
        "    def forward(self, text_ids, mask):\n",
        "        embeddings = self.embed(text_ids) #this is a matrix of embeddings, one for each id, of size batch_size X max_sent_size X embedding dimension\n",
        "        avg = self.masked_mean(embeddings,mask) #should return the average of the embeddings, ignoring the embeddings corresponding to the pad token\n",
        "        output = self.fc_out(avg) #final classification layer\n",
        "        return output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmdzEvd2Nc2M"
      },
      "source": [
        "#### Training Loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4MLxfgD7M-aI"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "batch_size = 64\n",
        "epochs = 3\n",
        "dev_every = 100\n",
        "lr = 0.001\n",
        "save_path = \"best_model\"\n",
        "drop_out = 0\n",
        "word_dropout = 0.01\n",
        "weight_decay = 1e-5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HK3X1nSxCB8g"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train(lr = .005, drop_out = 0, word_dropout = .3, batch_size = 16, weight_decay = 1e-5,args = None):\n",
        "    if args is not None:\n",
        "      drop_out = args[\"drop_out\"]\n",
        "      drop_out = args[\"drop_out\"]\n",
        "\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    trainset = SSTpytorchDataset(ds, word_dropout, 'train')\n",
        "    testset = SSTpytorchDataset(ds, word_dropout, 'test')\n",
        "    devset = SSTpytorchDataset(ds, word_dropout, 'dev')\n",
        "\n",
        "    train_iter = DataLoader(trainset, batch_size, shuffle=True, num_workers=0)\n",
        "    test_iter = DataLoader(testset, batch_size, shuffle=False, num_workers=0)\n",
        "    dev_iter = DataLoader(devset, batch_size, shuffle=False, num_workers=0)\n",
        "\n",
        "    model = DAN(n_embed=n_embed, d_embed=d_embed, d_hidden=300, d_out=d_out, layer_dropout=drop_out, word_dropout = word_dropout )\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay = weight_decay)\n",
        "\n",
        "\n",
        "    acc, val_loss = evaluate(dev_iter, model, device)\n",
        "    best_acc = acc\n",
        "\n",
        "    print(\n",
        "        'epoch |   %        |  loss  |  avg   |val loss|   acc   |  best  | time | save |')\n",
        "    print(\n",
        "        'val   |            |        |        | {:.4f} | {:.4f} | {:.4f} |      |      |'.format(\n",
        "            val_loss, acc, best_acc))\n",
        "\n",
        "    iterations = 0\n",
        "    last_val_iter = 0\n",
        "    train_loss = 0\n",
        "    start = time.time()\n",
        "    _save_ckp = ''\n",
        "    for epoch in range(epochs):\n",
        "        n_correct, n_total, train_loss = 0, 0, 0\n",
        "        last_val_iter = 0\n",
        "        for batch_idx, batch in enumerate(train_iter):\n",
        "            # switch model to training mode, clear gradient accumulators\n",
        "            model.train();\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            iterations += 1\n",
        "\n",
        "            data, ns, mask, label = batch\n",
        "\n",
        "            data = data.to(device)\n",
        "            label = label.to(device).long()\n",
        "            mask = mask.to(device).long()\n",
        "            mask.requires_grad = False\n",
        "\n",
        "            answer = model(data,mask)\n",
        "            loss = criterion(answer, label)\n",
        "\n",
        "            loss.backward();\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss += loss.item()\n",
        "            print('\\r {:4d} | {:4d}/{} | {:.4f} | {:.4f} |'.format(\n",
        "                epoch, batch_size * (batch_idx + 1), len(trainset), loss.item(),\n",
        "                       train_loss / (iterations - last_val_iter)), end='')\n",
        "\n",
        "            if iterations > 0 and iterations % dev_every == 0:\n",
        "                acc, val_loss= evaluate(dev_iter, model, device)\n",
        "\n",
        "                if acc > best_acc:\n",
        "                    best_acc = acc\n",
        "                    torch.save(model.state_dict(), save_path)\n",
        "                    _save_ckp = '*'\n",
        "\n",
        "                print(\n",
        "                    ' {:.4f} | {:.4f} | {:.4f} | {:.2f} | {:4s} |'.format(\n",
        "                        val_loss, acc, best_acc, (time.time() - start) / 60,\n",
        "                        _save_ckp))\n",
        "\n",
        "                train_loss = 0\n",
        "                last_val_iter = iterations\n",
        "    model.load_state_dict(torch.load(save_path)) #this will be the best model\n",
        "    test_y_pred = evaluate(test_iter,model, device,\"test\")\n",
        "    print(\"\\nValidation Accuracy : \", evaluate(dev_iter,model, device))\n",
        "    return best_acc, test_y_pred\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaMw5RMINjZd"
      },
      "outputs": [],
      "source": [
        "def evaluate(loader, model, device, split = \"dev\"):\n",
        "    model.eval()\n",
        "    n_correct, n = 0, 0\n",
        "    losses = []\n",
        "    y_pred = []\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch in enumerate(loader):\n",
        "            data, ns, mask, label = batch\n",
        "            data = data.to(device)\n",
        "            label = label.to(device).long()\n",
        "            mask = mask.to(device).long()\n",
        "            answer = model(data,mask)\n",
        "            if split != \"test\":\n",
        "                n_correct += (torch.max(answer, 1)[1].view(label.size()) == label).sum().item()\n",
        "                n += answer.shape[0]\n",
        "                loss = criterion(answer, label)\n",
        "                losses.append(loss.data.cpu().numpy())\n",
        "            else:\n",
        "                y_pred.extend(torch.max(answer, 1)[1].view(label.size()).tolist())\n",
        "    if split != \"test\":\n",
        "        acc = 100. * n_correct / n\n",
        "        loss = np.mean(losses)\n",
        "        return acc, loss\n",
        "    else:\n",
        "        return y_pred\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tFF_2GQXhqGz"
      },
      "source": [
        "Run this to get the validation accuracy on the dev dataset and the predictions of the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x7FkX-MDokS7"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(1234)\n",
        "\n",
        "epochs = 3\n",
        "dev_value, test_y_pred = train(lr, batch_size, word_dropout, batch_size, weight_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z1hq5605hbjx"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_dan_predictions', answer = test_y_pred)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Wdn9vahb1sb"
      },
      "source": [
        "####"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4tAw9tgbGY1D"
      },
      "source": [
        "## **1.2 [4190: 8 autograded; 5190: 10 autograded] Transformers**\n",
        "\n",
        "In Lecture we have discussed the heated model architecture Transformers. The original paper that proposed Transformer is [Attention Is All You Need (Vaswani et al. 2017)](https://arxiv.org/abs/1706.03762), and you can read it if interested.\n",
        "\n",
        "Recall that it is a composition of self-attention layers, here is a graph representation of the architecture:\n",
        "![transformer architecture](https://d2l.ai/_images/transformer.svg)\n",
        "\n",
        "So the idea of self-attention is essential for Transformers, and in this homework question your task is to implement the multi-head attention block in a Transformer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bP0L25asIZvl"
      },
      "source": [
        "### **1.2.1 Helper functions**\n",
        "\n",
        "There is no code that you need to write here, but you do need to run this section!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zkxH5BsjoEJz"
      },
      "outputs": [],
      "source": [
        "!pip install torchtext"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mfesBhaeH7vy"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import math,copy,re\n",
        "import warnings\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "warnings.simplefilter(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyqPvjUtGdn0"
      },
      "outputs": [],
      "source": [
        "cur_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "class PositionalEncoder(nn.Module):\n",
        "    def __init__(self, embed_dim, max_len=300, device=cur_device):\n",
        "        super().__init__()\n",
        "        self.position_embedding = torch.zeros((1, max_len, embed_dim)).to(device)\n",
        "        i = torch.arange(max_len, dtype=torch.float32).reshape(-1, 1)\n",
        "        j2 = torch.arange(0, embed_dim, step=2, dtype=torch.float32)\n",
        "        x = i / torch.pow(10000, j2 / embed_dim)\n",
        "        self.position_embedding[..., 0::2] = torch.sin(x)\n",
        "        self.position_embedding[..., 1::2] = torch.cos(x)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_plus_p = x + self.position_embedding[:, : x.shape[1]]\n",
        "        return x_plus_p"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tMbEDKEkJQ2J"
      },
      "outputs": [],
      "source": [
        "class ResidualNorm(nn.Module):\n",
        "    def __init__(self, embed_dim):\n",
        "        super().__init__()\n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "\n",
        "    def forward(self, x, residual):\n",
        "        return self.norm(x + residual)\n",
        "\n",
        "\n",
        "class Feedforward(nn.Module):\n",
        "    def __init__(self, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(embed_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, embed_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(F.relu(self.fc1(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1goKvQT7JgQK"
      },
      "source": [
        "### **1.2.2 [8 pts autograded] Multihead Attention**\n",
        "\n",
        "Recall that the attention mechanism requires three main components:\n",
        "\n",
        " - the values vectors V\n",
        " - the query vectors Q\n",
        " - the key vectors K\n",
        "\n",
        " And for self-attention, these are all calulated from the original input using three different learnable weight matrices. Essentially we are trying to see that how similar are my queries and keys and use this attention score to construct a weight sum of my values.\n",
        "\n",
        " As for Multihead attention, each head will attend to a set of (V, Q, K) values, so we need to replicate (V, Q, K) n times if we have `n_heads` number of attention heads. This is done by our helper function `mha_transform_input`. You will also need to transform the output back to the correct size at the end to make sure that it can be used as input to future layers using `mha_transform_output`.\n",
        "\n",
        " **Note:** EVERY computation with the 3 vectors must be done **in this order: Q,K,V**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-z2RSkbcJbPi"
      },
      "outputs": [],
      "source": [
        "def masked_softmax(x, mask):\n",
        "    \"\"\"\n",
        "    x:   (B*H, T, T)\n",
        "    mask:(B*H, T)  # 1 = keep, 0 = mask\n",
        "    \"\"\"\n",
        "    mask = mask.to(dtype=torch.bool)\n",
        "    # expand to (B*H, 1, T) so it masks along the key dimension\n",
        "    x = x.masked_fill(~mask.unsqueeze(1), torch.finfo(x.dtype).min)\n",
        "    attn = F.softmax(x, dim=-1)\n",
        "    return attn\n",
        "\n",
        "\n",
        "def mha_transform_input(x, n_heads, head_dim):\n",
        "    \"\"\"Restructure the input tensors to compute the heads in parallel\n",
        "    Requires that head_dim = embed_dim / n_heads\n",
        "    Args:\n",
        "      x (n_batch, n_tokens, embed_dim): input tensor, one of queries, keys, or values\n",
        "      n_heads (int): the number of attention heads\n",
        "      head_dim (int): the dimensionality of each head\n",
        "    Returns:\n",
        "      (n_batch*n_heads, n_tokens, head_dim): 3D Tensor containing all the input heads\n",
        "    \"\"\"\n",
        "    n_batch, n_tokens, _ = x.shape\n",
        "    x = x.reshape((n_batch, n_tokens, n_heads, head_dim))\n",
        "    x = x.permute(0, 2, 1, 3)\n",
        "    return x.reshape((n_batch * n_heads, n_tokens, head_dim))\n",
        "\n",
        "\n",
        "def mha_transform_output(x, n_heads, head_dim):\n",
        "    \"\"\"Restructures the output back to the original format\n",
        "    Args:\n",
        "      x (n_bacth*n_heads, n_tokens, head_dim): multi-head representation tensor\n",
        "      n_heads (int): the number of attention heads\n",
        "      head_dim (int): the dimensionality of each head\n",
        "    Returns:\n",
        "      (n_batch, n_tokens, embed_dim): 3D Tensor containing all the input heads\n",
        "    \"\"\"\n",
        "    n_concat, n_tokens, _ = x.shape\n",
        "    n_batch = n_concat // n_heads\n",
        "    x = x.reshape((n_batch, n_heads, n_tokens, head_dim))\n",
        "    x = x.permute(0, 2, 1, 3)\n",
        "    return x.reshape((n_batch, n_tokens, n_heads * head_dim))\n",
        "\n",
        "\n",
        "class ScaledDotProductAttention(nn.Module):\n",
        "    def __init__(self, head_dim):\n",
        "        super().__init__()\n",
        "        self.head_dim = head_dim\n",
        "\n",
        "    def forward(self, queries, keys, values, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
        "          keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
        "          values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
        "          mask (n_batch, n_tokens): binary mask tensor\n",
        "        Returns:\n",
        "          (n_batch, n_tokens, embed_dim): scaled dot product attention tensor\n",
        "        \"\"\"\n",
        "        # STUDENT TODO START:\n",
        "        # 1. Calculate the batched dot product of queries and keys\n",
        "\n",
        "        # 2. Scale it by the square root of embedding dimensions\n",
        "\n",
        "        # 3. Pass the scaled dot product through masked_softmax to get attention weights\n",
        "\n",
        "        # 4. Compute final attention using the attention weights and values\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return attention\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, n_heads, embed_dim):\n",
        "        super().__init__()\n",
        "        self.n_heads = n_heads\n",
        "        self.head_dim = embed_dim // n_heads\n",
        "\n",
        "        self.attention = ScaledDotProductAttention(self.head_dim)\n",
        "\n",
        "        # STUDENT TODO START:\n",
        "        # Define the weight matrices for each of V, Q, and K\n",
        "        # You can do this with fully connected linear layers\n",
        "        # Remember to set bias=False to make sure that it is pure weight matrices\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        self.out_fc = nn.Linear(embed_dim, embed_dim, bias=False)\n",
        "\n",
        "    def forward(self, queries, keys, values, mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          queries (n_batch, n_tokens, embed_dim): queries (Q) tensor\n",
        "          keys (n_batch, n_tokens, embed_dim): keys (K) tensor\n",
        "          values (n_batch, n_tokens, embed_dim): values (V) tensor\n",
        "          mask (n_batch, n_tokens): binary mask tensor\n",
        "        Returns:\n",
        "          (n_batch, n_tokens, embed_dim): multi-head attention tensor\n",
        "        \"\"\"\n",
        "        # STUDENT TODO START:\n",
        "\n",
        "        # For each of V, Q, and K\n",
        "        # 1. Multiply its corresponding weight matrix (passing through the Linear layer)\n",
        "\n",
        "        # 2. Use mha_transform_input to transform it into multihead\n",
        "\n",
        "        # 3. Calculate the attention results: pass in Q, K, V, mask\n",
        "\n",
        "        # 4. Use mha_transform_output to transform it back into the correct size\n",
        "\n",
        "        # 5. Pass the results through the output fully connect layer\n",
        "\n",
        "        # STUDENT TODO END:\n",
        "        return attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CufZYbn9Qk8y"
      },
      "source": [
        "#### Test your Multihead Attention implementation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhjesZP1Q_H8"
      },
      "outputs": [],
      "source": [
        "embed_dim = 4\n",
        "head_dim = 4\n",
        "my_scaled = ScaledDotProductAttention(head_dim)\n",
        "\n",
        "torch.manual_seed(522)\n",
        "src_tokens = torch.Tensor([[[7,1,6,5],[8,2,2,3],[5,3,4,2],[1,4,2,1],[10,1,9,7]]]).to(cur_device)\n",
        "src_mask = torch.IntTensor([[1,1,1,1,0]]).to(cur_device)\n",
        "\n",
        "# HINT: scaled_answer should have shape (1, 5, 2)\n",
        "scaled_answer = my_scaled(src_tokens, src_tokens, src_tokens, src_mask).cpu().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gR7T3nQeVWIg"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_scaled_dot_product', answer = scaled_answer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuKeYAMpQcg1"
      },
      "outputs": [],
      "source": [
        "n_heads = 2\n",
        "embed_dim = 2\n",
        "my_att = MultiHeadAttention(n_heads, embed_dim)\n",
        "\n",
        "torch.manual_seed(522)\n",
        "src_tokens = torch.Tensor([[[2, 7],[3, 8],[4, 5],[9, 1],[2, 10]]])\n",
        "src_mask = torch.IntTensor([[1,1,1,1,0]])\n",
        "\n",
        "# HINT: att_answer should have shape (1, 5, 2)\n",
        "att_answer = my_att(src_tokens, src_tokens, src_tokens, src_mask).detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QR4fxUXBXN7e"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_multihead_attention', answer = att_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FK8IoX2EOpWv"
      },
      "source": [
        "#### Full Transformers model code\n",
        "\n",
        "These code blocks implement a full transformer model using PyTorch, including all essential encoder and decoder components: embedding, positional encoding, multi-head attention, feedforward layers, and normalization. You can use the provided test case to verify that your own implementation matches the required architecture and produces the expected output shape for sequence-to-sequence tasks. This setup ensures your model is ready for tasks like translation, summarization, or next-token prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l96uiPyUOgg9"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, n_heads, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "        self.norm1 = ResidualNorm(embed_dim)\n",
        "        self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
        "        self.norm2 = ResidualNorm(embed_dim)\n",
        "\n",
        "    def forward(self, src_tokens, src_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          src_tokens (n_batch, n_tokens, embed_dim): the source sequence\n",
        "          src_mask (n_batch, n_tokens): binary mask over the source\n",
        "        Returns:\n",
        "          (n_batch, n_tokens, embed_dim): the encoder state\n",
        "        \"\"\"\n",
        "        # First compute self-attention on the source tokens by passing them in\n",
        "        # as the queries, keys, and values to the attention module.\n",
        "        self_attention = self.attention(src_tokens, src_tokens, src_tokens, src_mask)\n",
        "        # Next compute the norm of the self-attention result with a residual\n",
        "        # connection from the source tokens\n",
        "        normed_attention = self.norm1(self_attention, src_tokens)\n",
        "        # Pass the normed attention result through the feedforward component\n",
        "        ff_out = self.feedforward(normed_attention)\n",
        "        # Finally compute the norm of the feedforward output with a residual\n",
        "        # connection from the normed attention output\n",
        "        out = self.norm2(ff_out, normed_attention)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Tq2PTyUOvAl"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_heads, n_blocks):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim).to(cur_device)\n",
        "        self.positional_encoding = PositionalEncoder(embed_dim).to(cur_device)\n",
        "        self.encoder_blocks = nn.ModuleList(\n",
        "            [EncoderBlock(n_heads, embed_dim, hidden_dim) for _ in range(n_blocks)]\n",
        "        ).to(cur_device)\n",
        "\n",
        "    def forward(self, src_tokens, src_mask):\n",
        "        x = self.embedding(src_tokens)\n",
        "        x = self.positional_encoding(x)\n",
        "        for block in self.encoder_blocks:\n",
        "            x = block(x, src_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "svwo2qeVO7Ke"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, n_heads, embed_dim, hidden_dim):\n",
        "        super().__init__()\n",
        "        self.self_attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "        self.norm1 = ResidualNorm(embed_dim)\n",
        "        self.encoder_attention = MultiHeadAttention(n_heads, embed_dim)\n",
        "        self.norm2 = ResidualNorm(embed_dim)\n",
        "        self.feedforward = Feedforward(embed_dim, hidden_dim)\n",
        "        self.norm3 = ResidualNorm(embed_dim)\n",
        "\n",
        "    def forward(self, tgt_tokens, tgt_mask, encoder_state, src_mask):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          tgt_tokens (n_batch, n_tokens, embed_dim): the target sequence\n",
        "          tgt_mask (n_batch, n_tokens): binary mask over the target tokens\n",
        "          encoder_state (n_batch, n_tokens, embed_dim): the output of the encoder pass\n",
        "          src_mask (n_batch, n_tokens): binary mask over the source tokens\n",
        "        Returns:\n",
        "          (n_batch, n_tokens, embed_dim): the decoder state\n",
        "        \"\"\"\n",
        "        # First compute self-attention on the target tokens by passing them in\n",
        "        # as the queries, keys, and values to the attention module along with the\n",
        "        # target mask.\n",
        "        self_attention = self.self_attention(tgt_tokens, tgt_tokens, tgt_tokens, tgt_mask)\n",
        "        # Next compute the norm of the self-attention result with a residual\n",
        "        # connection from the target tokens\n",
        "        normed_self_attention = self.norm1(self_attention, tgt_tokens)\n",
        "        # Compute the encoder attention by using the normed self-attention output as\n",
        "        # the queries and the encoder state as the keys and values along with the\n",
        "        # source mask.\n",
        "        encoder_attention = self.encoder_attention(normed_self_attention, encoder_state, encoder_state, src_mask)\n",
        "        # Next compute the norm of the encoder attention result with a residual\n",
        "        # connection from the normed self-attention\n",
        "        normed_encoder_attention = self.norm2(encoder_attention, normed_self_attention)\n",
        "        # Pass the normed encoder attention result through the feedforward component\n",
        "        ff_out = self.feedforward(normed_encoder_attention)\n",
        "        # Finally compute the norm of the feedforward output with a residual\n",
        "        # connection from the normed attention output\n",
        "        out = self.norm3(ff_out, normed_encoder_attention)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgLIgcCPPBSX"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, n_heads, n_blocks):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim).to(cur_device)\n",
        "        self.positional_encoding = PositionalEncoder(embed_dim).to(cur_device)\n",
        "        self.decoder_blocks = nn.ModuleList(\n",
        "            [DecoderBlock(n_heads, embed_dim, hidden_dim) for _ in range(n_blocks)]\n",
        "        ).to(cur_device)\n",
        "\n",
        "    def forward(self, tgt_tokens, tgt_mask, encoder_state, src_mask):\n",
        "        x = self.embedding(tgt_tokens)\n",
        "        x = self.positional_encoding(x)\n",
        "        for block in self.decoder_blocks:\n",
        "            x = block(x, tgt_mask, encoder_state, src_mask)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6nBwmZ0qPO8y"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(\n",
        "        self, src_vocab_size, tgt_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.encoder = Encoder(src_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
        "        self.decoder = Decoder(tgt_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
        "        self.out = nn.Linear(embed_dim, tgt_vocab_size).to(cur_device)\n",
        "\n",
        "    def forward(self, src_tokens, src_mask, tgt_tokens, tgt_mask):\n",
        "        # Compute the encoder output state from the source tokens and mask\n",
        "        encoder_state = self.encoder(src_tokens, src_mask)\n",
        "        # Compute the decoder output state from the target tokens and mask as well\n",
        "        # as the encoder state and source mask\n",
        "        decoder_state = self.decoder(tgt_tokens, tgt_mask, encoder_state, src_mask)\n",
        "        # Compute the vocab scores by passing the decoder state through the output\n",
        "        # linear layer\n",
        "        out = self.out(decoder_state)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdRKWXXkPgTR"
      },
      "source": [
        "#### **1.2.2.3 [2 pts autograded] [5190 Extra] Test your implementation with the entire Transformer implementation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WQx8lWdhPZV6"
      },
      "outputs": [],
      "source": [
        "# Test for Transformer\n",
        "torch.manual_seed(522)\n",
        "src_vocab_size = tgt_vocab_size = 5\n",
        "n_blocks, n_heads, batch_size, embed_dim, hidden_dim = 10, 2, 1, 4, 8\n",
        "src_tokens = tgt_tokens = torch.IntTensor([[0,1,2,3,4]]).to(cur_device)\n",
        "src_mask = tgt_mask = torch.IntTensor([[1,1,1,1,1]]).to(cur_device)\n",
        "\n",
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, embed_dim, hidden_dim, n_heads, n_blocks)\n",
        "\n",
        "# HINT: trans_answer should have shape (1, 5, 5)\n",
        "trans_answer = transformer(src_tokens, src_mask, tgt_tokens, tgt_mask).cpu().detach().numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FowQZP0YYYE6"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    grader.grade(test_case_id = 'test_transformer', answer = trans_answer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iR-DGV_94HJx"
      },
      "source": [
        "# **2. [4190: 2 autograded + 14 manual; 5190: 2 autograded + 22 manual] Reinforcement Learning Section**\n",
        "\n",
        "Install Dependencies and Imports:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mloN11-laGA6"
      },
      "outputs": [],
      "source": [
        "if NOTEBOOK:\n",
        "  !apt-get update\n",
        "  !apt-get -qq -y install libnvtoolsext1 > /dev/null\n",
        "  !ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n",
        "  !apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n",
        "  !pip -q install gymnasium[classic_control]\n",
        "  !pip -q install pyglet\n",
        "  !pip -q install pyopengl\n",
        "  !pip -q install pyvirtualdisplay\n",
        "  !apt-get install xvfb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M_8V6fYxaH6x"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import gymnasium as gym\n",
        "import itertools\n",
        "import matplotlib\n",
        "import numpy as np\n",
        "import sys\n",
        "import collections\n",
        "import pandas as pd\n",
        "from collections import namedtuple\n",
        "from matplotlib import pyplot as plt\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "import sklearn.pipeline\n",
        "import sklearn.preprocessing\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical, Normal\n",
        "\n",
        "from sklearn.kernel_approximation import RBFSampler\n",
        "\n",
        "matplotlib.style.use('ggplot')\n",
        "\n",
        "import matplotlib.animation\n",
        "import imageio\n",
        "from IPython.display import HTML, display, Image\n",
        "from pyvirtualdisplay import Display\n",
        "\n",
        "virtual_display = Display(visible=0, size=(1024, 768))\n",
        "virtual_display.start()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WR-sHW-Yae3w"
      },
      "outputs": [],
      "source": [
        "seed = 5190 # for the autograded test_featurize_state, use seed 5190\n",
        "env = gym.make(\"MountainCarContinuous-v0\", render_mode=\"rgb_array\")\n",
        "env.reset()\n",
        "env.action_space.seed(seed)\n",
        "env.observation_space.seed(seed)\n",
        "env.action_space.sample()\n",
        "env.observation_space.sample()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10aD0AWF5LvF"
      },
      "source": [
        "### **2.1 [2 pts autograded] Implement State Featurization for RL Function Approximation**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPREgT6I5MU1"
      },
      "source": [
        "The Mountain Car environment provides states as raw, continuous vectors (position and velocity). To make these more suitable for RL algorithms using function approximation, we first fit a normalization transform on many observed states using `StandardScaler`, so each feature has mean 0 and unit variance. Next, we employ a feature map using multiple Radial Basis Function (RBF) kernels: this step expands the state into a high-dimensional feature vector, where each new dimension reflects similarity to a set of reference points in the normalized space at different scales. This data-driven featurization pipeline ensures that our RL agentâ€™s neural networks or linear models receive well-scaled, expressive inputs, improving both learning stability and generalization.\n",
        "\n",
        "The final result is that, for any input state, we obtain a 400-dimensional feature vector that encodes both normalized input values and a set of similarities to different parts of the state space. This featurized vector is then used as input for value or policy function approximators (neural networks or linear models), enabling stable, efficient function approximation in continuous reinforcement learning environments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s8WtSP845IZQ"
      },
      "outputs": [],
      "source": [
        "# Feature Preprocessing: Normalize to zero mean and unit variance\n",
        "# We use a few samples from the observation space to do this\n",
        "np.random.seed(seed)\n",
        "observation_examples = np.array([env.observation_space.sample() for x in range(10000)])\n",
        "scaler = sklearn.preprocessing.StandardScaler()\n",
        "scaler.fit(observation_examples)\n",
        "\n",
        "# Used to converte a state to a featurizes represenation.\n",
        "# We use RBF kernels with different variances to cover different parts of the space\n",
        "featurizer = sklearn.pipeline.FeatureUnion([\n",
        "        (\"rbf1\", RBFSampler(gamma=5.0, n_components=100, random_state=seed)),\n",
        "        (\"rbf2\", RBFSampler(gamma=2.0, n_components=100, random_state=seed)),\n",
        "        (\"rbf3\", RBFSampler(gamma=1.0, n_components=100, random_state=seed)),\n",
        "        (\"rbf4\", RBFSampler(gamma=0.5, n_components=100, random_state=seed))\n",
        "        ])\n",
        "featurizer.fit(scaler.transform(observation_examples))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9gkXQAkFgoA"
      },
      "source": [
        "Complete the `featurize_state` function by applying the fitted scaler and RBF-based featurizer to a new state. Your goal is to convert a raw continuous state into a normalized and high-dimensional feature vector suitable for policy and value function approximation in reinforcement learning. Fill in the TODO block with code to (1) scale the state and (2) transform it using the featurizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-lTigjq65E3P"
      },
      "outputs": [],
      "source": [
        "def featurize_state(state):\n",
        "    \"\"\"\n",
        "    Returns the featurized representation for a state.\n",
        "    \"\"\"\n",
        "    # STUDENT TODO START:\n",
        "    # 1. Apply scaler to the state\n",
        "\n",
        "    # 2. Transform the state using the featurizer\n",
        "\n",
        "    # STUDENT TODO END\n",
        "\n",
        "    return featurized[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TiIhvQKp5F6a"
      },
      "outputs": [],
      "source": [
        "# PennGrader Grading Cell\n",
        "if NOTEBOOK:\n",
        "    state = np.array([-0.8248636 ,  0.02986798])\n",
        "    feat_state = featurize_state(state)\n",
        "    grader.grade(test_case_id = 'test_featurize_state', answer = feat_state)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6UqMyZxa4G4"
      },
      "source": [
        "### **2.2 [14 pts manually graded] Policy Gradient with Neural Network Policies (REINFORCE)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GWvIaFHHa9TM"
      },
      "source": [
        "In policy gradient reinforcement learning, neural networks can be used to directly parameterize the agentâ€™s policyâ€”specifically, how actions are chosen from states. The `PolicyEstimator` constructs a probabilistic policy for continuous actions by outputting the mean (Î¼) and standard deviation (Ïƒ) parameters of a Normal (Gaussian) distribution, typically via neural network layers.\n",
        "\n",
        "During an episode, the agent samples actions from this distribution for both exploration and learning. Policy parameters are updated by maximizing the expected **return**, where **â€œreturnâ€** is the discounted sum of rewards collected throughout each trajectory,  using the REINFORCE algorithm, which applies the gradient of the log-probabilities of the taken actions weighted by the observed returns (discounted sums of rewards). To encourage further exploration, an entropy bonus term can be included in the policy loss.\n",
        "\n",
        "Use the code below to implement the missing logic in your `PolicyEstimator` class.\n",
        "\n",
        "- Complete the network heads for Î¼ and Ïƒ, their initialization, and the parameterization of the action distribution.\n",
        "- Implement both the forward and update steps for policy gradients, including handling the entropy regularization term.\n",
        "- *Note: In this context, \"reward\" refers to the instantaneous feedback from the environment; \"return\" refers to the (discounted) sum of these rewards over an episode, which is what the REINFORCE update maximizes. \"Undiscounted return\" is the simple sum of rewards, used for monitoring learning progress and reporting results.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "15YRXq2IbBWj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Normal\n",
        "import math\n",
        "\n",
        "\n",
        "class PolicyEstimator(nn.Module):\n",
        "    \"\"\"\n",
        "    Policy: linear Î¼, linear Ïƒ (softplus), Normal policy,\n",
        "    entropy bonus, and REINFORCE-style loss using provided action & target.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim=400,\n",
        "        learning_rate=5e-2,\n",
        "        entropy_rate=1e-1,\n",
        "        device='cuda',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # STUDENT TODO START:\n",
        "        # 1. Define the neural network heads for mu and sigma\n",
        "\n",
        "        # 2. Initialize the weights and biases to zero (for controlled learning start).\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "        # Optimizer setup\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.featurize_state = featurize_state\n",
        "        self.entropy_rate = entropy_rate\n",
        "\n",
        "        # STUDENT TODO START:\n",
        "        # Initialize and store device for computations (CPU/GPU).\n",
        "\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    def forward(self, state_tensor: torch.Tensor):\n",
        "        \"\"\"\n",
        "        state_tensor: (400,) or (N, 400)\n",
        "        returns: dist (Normal), mu (N,1), sigma (N,1)\n",
        "        \"\"\"\n",
        "        x = state_tensor\n",
        "\n",
        "        # STUDENT TODO START:\n",
        "        # 1. Compute the mean (mu) of the action distribution by passing x through the mu head.\n",
        "\n",
        "        # 2. Compute the standard deviation (sigma) by passing x through the sigma head and applying softplus activation.\n",
        "        # (Add a small epsilon like 1e-5 to avoid zero std.)\n",
        "\n",
        "        # 3. Construct a Normal (Gaussian) distribution parameterized by mu and sigma.\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return dist, mu, sigma\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, state):\n",
        "        \"\"\"\n",
        "        Samples and returns an action\n",
        "        \"\"\"\n",
        "        s = torch.as_tensor(self.featurize_state(state), dtype=torch.float32, device=self.device)\n",
        "        dist, _, _ = self.forward(s)\n",
        "        action = dist.sample()\n",
        "        return action.squeeze()\n",
        "\n",
        "    def update(self, state, target, action):\n",
        "        \"\"\"\n",
        "        state: raw state; will be featurized to shape (400,)\n",
        "        target: scalar advantage/return (float)\n",
        "        action: scalar action actually taken (float)\n",
        "        returns: loss value (float)\n",
        "        \"\"\"\n",
        "        # STUDENT TODO START:\n",
        "        # 1. Zero out gradients for the optimizer (standard PyTorch preparation).\n",
        "\n",
        "        # 2. Featurize the state and format as torch tensor; batchify it for the forward pass.\n",
        "\n",
        "        # 3. Pass state through the forward method to get the action distribution.\n",
        "\n",
        "        # 4. Prepare action and target tensors for loss calculation.\n",
        "\n",
        "        # 5. Compute the log-probability of the action and policy distribution entropy.\n",
        "\n",
        "        # 6. Construct the policy gradient loss, including an entropy bonus for exploration.\n",
        "\n",
        "        # 7. Backpropagate loss and perform optimizer step to update policy parameters.\n",
        "\n",
        "        # STUDENT TODO END\n",
        "        return float(loss.item())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYzG5zuZGBhr"
      },
      "source": [
        "**REINFORCE** is a simple, foundational policy gradient algorithm in reinforcement learning that directly updates an agent's policy using gradients computed from the returns of complete episodes. The agent collects trajectories by sampling actions from its policy, then uses the total discounted return of each action to increase the likelihood of better actions in future episodes via gradient ascent on the expected reward. This algorithm operates without requiring a separate value function or environment model, which makes it widely applicable but also prone to high variance and instability during training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G8wciLgSDwiq"
      },
      "outputs": [],
      "source": [
        "def reinforce(env, estimator_policy, num_episodes, discount_factor=1.0):\n",
        "    \"\"\"\n",
        "    REINFORCE with function approximation.\n",
        "    estimator_policy: has .predict(state) -> action and .update(state, target, action)\n",
        "                      .update(state, return, action) updates the policy\n",
        "    \"\"\"\n",
        "    # stats[\"episode_rewards\"] stores the undiscounted returns for each state\n",
        "    stats = {\"episode_rewards\": np.zeros(num_episodes, dtype=np.float32),\n",
        "             \"episode_lengths\": np.zeros(num_episodes, dtype=np.float32)}\n",
        "\n",
        "    Transition = collections.namedtuple(\n",
        "        \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\"]\n",
        "    )\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        # Reset environment and initialize state\n",
        "        out = env.reset(seed=seed)\n",
        "        if hasattr(env, \"action_space\"):\n",
        "          env.action_space.seed(seed)\n",
        "        if hasattr(env, \"observation_space\"):\n",
        "          env.observation_space.seed(seed)\n",
        "        state = out[0] if hasattr(out, \"__getitem__\") else out\n",
        "\n",
        "        episode = [] # type list[Transition]\n",
        "\n",
        "        # STUDENT TODO START:\n",
        "\n",
        "        # 1. Play through the episode, collecting transitions\n",
        "        # Make sure to write to the stats buffer for every episode\n",
        "\n",
        "        # 2. Calculate (discounted) returns for each step\n",
        "\n",
        "        # 3. Update policy parameters for each transition\n",
        "\n",
        "        # 4. Print progress\n",
        "        print(\n",
        "            \"\\rEpisode {}/{}  Cumulative Reward:{}\".format(\n",
        "                i_episode + 1, num_episodes, stats[\"episode_rewards\"][i_episode]\n",
        "            ),\n",
        "            end=\"\"\n",
        "        )\n",
        "        # STUDENT TODO END\n",
        "\n",
        "    return stats\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the cell below, weâ€™ll train a policy in 50 episodes using your REINFORCE implementation with function approximation. The process relies on stochastic exploration, so outcomes can vary substantially from run to run.â€‹\n",
        "\n",
        "To ensure results are reproducible and consistent with the course staff's testing, we set random seed to `5190` (`torch.manual_seed`, `np.random.seed`). This helps with learning deterministically and debugging if your agent does not converge as expected.\n",
        "\n",
        "With a correct REINFORCE implementation, you should see your agent reach good performance (undiscounted returns above 80) in less than 50 episodes or under 3 minutes of training. If it takes longer than 5 minutes or you aren't seeing learning after several episodes, carefully re-check your policy update logic and reward calculations. You can also try out different configurations `num_episodes` and `discount_factor`, and even different seeds for experimentation."
      ],
      "metadata": {
        "id": "JRTCC9k0-oKG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g_ZE_JoaYENU"
      },
      "outputs": [],
      "source": [
        "seed = 5190 # try changing seed if you find your agent not learning\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "policy_estimator = PolicyEstimator(\n",
        "    learning_rate=1e-4,\n",
        ")\n",
        "\n",
        "# Run training\n",
        "num_episodes = 50\n",
        "discount_factor = 0.90\n",
        "\n",
        "stats = reinforce(env, policy_estimator,\n",
        "                     num_episodes=num_episodes,\n",
        "                     discount_factor=discount_factor)\n",
        "\n",
        "print(stats)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_NR-6auSKSIa"
      },
      "source": [
        "### Plots and Visualizations"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this cell, we visualize the learning process of your REINFORCE agent using `matplotlib` and `pandas`. Three plots are generated:\n",
        "\n",
        "* **Undiscounted return per episode**: Shows how the agentâ€™s cumulative (undiscounted) reward evolves as it learns.\n",
        "\n",
        "* **Smoothed undiscounted returns**: Highlights the learning trend and helps reveal any gradual improvements or plateaus in performance.\n",
        "\n",
        "* **Episode length per episode**: Indicates how efficiently the agent completes each episode; in this task, shorter episodes typically mean better policies.\n",
        "\n",
        "Your plots will be manually checked if training trajectory is reasonable and interpretable. Specifically, the agent should achieve an average undiscounted return of more than 80 within the 50 episodes. If your agentâ€™s performance plateaus much lower or your curves do not show learning, please revisit your implementation, debugging code logic, and ensure proper reward calculations."
      ],
      "metadata": {
        "id": "yQYty5Bw_uVl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I3SK38HuRkQd"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "# assuming 'stats' contains episode-level undiscounted returns:\n",
        "episode_undiscounted_returns = stats[\"episode_rewards\"]  # cumulative rewards (undiscounted return)\n",
        "episode_lengths = stats[\"episode_lengths\"]\n",
        "\n",
        "# Plot: Undiscounted return per episode\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(episode_undiscounted_returns, label=\"Undiscounted Return (Cumulative Reward per Episode)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Undiscounted Return\")\n",
        "plt.title(\"Undiscounted Return per Episode - REINFORCE (MountainCar)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot: Rolling average for smoother trend (window=3)\n",
        "rolling_avg = pd.Series(episode_undiscounted_returns).rolling(window=3, min_periods=1).mean()\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(rolling_avg, label=\"Rolling Avg Undiscounted Return (window=3)\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Undiscounted Return\")\n",
        "plt.title(\"Smoothed Undiscounted Return - REINFORCE (MountainCar)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot: Episode lengths (how quickly solved)\n",
        "plt.figure(figsize=(12,5))\n",
        "plt.plot(episode_lengths, label=\"Episode Length\")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Length\")\n",
        "plt.title(\"Episode Length per Episode - REINFORCE (MountainCar)\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqwp_E7mFuKK"
      },
      "outputs": [],
      "source": [
        "def visualize_policy(policy, max_steps=400, fps=40, seed=seed):\n",
        "    obs, info = env.reset(seed=seed)\n",
        "\n",
        "    frames = []\n",
        "    policy.eval()\n",
        "    for t in range(max_steps):\n",
        "        frame = env.render()  # (H, W, 3) uint8\n",
        "        frames.append(frame)\n",
        "        action = policy.predict(obs).unsqueeze(0)\n",
        "        obs, reward, terminated, truncated, info = env.step([action.item()])\n",
        "        if terminated or truncated:\n",
        "            break\n",
        "    # env.close()\n",
        "\n",
        "    path = \"/content/mountaincar_policy.gif\"\n",
        "    imageio.mimsave(path, frames, duration=1.0 / fps)\n",
        "    display(Image(filename=path))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hi8zngoyF71a"
      },
      "outputs": [],
      "source": [
        "visualize_policy(policy_estimator)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.3 [0 pts] Behavioral Cloning**\n",
        "\n",
        "In this section, youâ€™ll run helper functions to collect expert data and evaluate continuous policies in the Mountain Car environment.\n",
        "\n",
        "- collect_expert_data(...) rolls out the provided expert policy for several episodes, collecting states, actions, and rewards as the car attempts to reach the flag. Success is counted whenever the car achieves the goal (defined as reaching the top of the hill/flag). The output dataset will later be used for behavioral cloning and imitation learning experiments.â€‹\n",
        "\n",
        "- test_agent(...) runs your policy and records its final position, success rate, and episode rewards. This enables systematic comparison between different learned or expert policiesâ€”giving you a way to measure performance and understand how reliably your agent reaches the goal.\n",
        "\n",
        "These functions generate the ground-truth and measured data that fuel later learning algorithms and benchmarking. In modern RL workflows, expert rollouts are crucial for behavioral cloning, and robust evaluation is needed to confirm if your agent consistently achieves high reward and reaches the flag.\n",
        "\n",
        "No code are necessary from this point on, but run these cells and use the resulting data to assess your agent in the next part."
      ],
      "metadata": {
        "id": "-Oth002UEiig"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collect_expert_data(env, policy, max_episodes=50):\n",
        "    \"\"\"\n",
        "    Roll out a continuous policy on Gymnasium's MountainCarContinuous-v0 and\n",
        "    return (states, actions, success_count).\n",
        "    \"\"\"\n",
        "\n",
        "    states, actions, rewards = [], [], []\n",
        "    success = 0\n",
        "\n",
        "    for _ in range(max_episodes):\n",
        "        obs, _ = env.reset(seed=seed)\n",
        "        ep_reward = 0.0\n",
        "        for t in range(1, 1000):\n",
        "            pred = policy.predict(obs)\n",
        "            if isinstance(pred, (np.ndarray, list)) and np.size(pred) == 1:\n",
        "                a = float(np.ravel(pred)[0])\n",
        "            elif hasattr(pred, \"item\"):\n",
        "                a = float(pred.item())\n",
        "            else:\n",
        "                a = float(pred)\n",
        "\n",
        "            next_obs, reward, terminated, truncated, _ = env.step([a])\n",
        "\n",
        "            states.append(np.array(obs, dtype=np.float32))\n",
        "            actions.append(np.array([a], dtype=np.float32))\n",
        "            ep_reward += float(reward)\n",
        "\n",
        "            if terminated:\n",
        "                success += 1\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "            obs = next_obs\n",
        "        rewards.append(ep_reward)\n",
        "\n",
        "    print(f\"Avg Reward ({max_episodes} episodes): {np.mean(rewards):.3f}\")\n",
        "\n",
        "    return states, actions, rewards, success"
      ],
      "metadata": {
        "id": "5cMekqj0New-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_agent(env, policy, max_episodes=50):\n",
        "    \"\"\"\n",
        "    Evaluate a continuous policy on Gymnasium's MountainCarContinuous-v0.\n",
        "\n",
        "    Assumes: policy.predict(obs) -> scalar tensor/float action in [-1, 1].\n",
        "    Returns: (position_list, success_list, frames)  # frames kept empty for API parity\n",
        "    \"\"\"\n",
        "\n",
        "    position_list, success_list, reward_list = [], [], []\n",
        "    frames = []  # kept for signature compatibility\n",
        "    success = 0\n",
        "\n",
        "    for i in range(max_episodes):\n",
        "        obs, _ = env.reset(seed=seed)\n",
        "        ep_reward = 0.0\n",
        "        last_obs = obs\n",
        "\n",
        "        for t in range(1, 1000):\n",
        "            # Reshape obs to be a 2D array (1 sample, N features) for policy.predict\n",
        "            pred = policy.predict(obs.reshape(1, -1))\n",
        "            if isinstance(pred, (np.ndarray, list)) and np.size(pred) == 1:\n",
        "                a = float(np.ravel(pred)[0])\n",
        "            elif hasattr(pred, \"item\"):\n",
        "                a = float(pred.item())\n",
        "            else:\n",
        "                a = float(pred)\n",
        "\n",
        "            obs, reward, terminated, truncated, _ = env.step([a])\n",
        "\n",
        "            ep_reward += float(reward)\n",
        "            last_obs = obs\n",
        "\n",
        "            if terminated:\n",
        "                success += 1\n",
        "            if terminated or truncated:\n",
        "                break\n",
        "\n",
        "        position_list.append(float(last_obs[0]))          # final x-position\n",
        "        reward_list.append(ep_reward)\n",
        "        success_list.append(success / (i + 1))            # running success rate\n",
        "\n",
        "    print(f\"Avg Reward ({max_episodes} episodes): {np.mean(reward_list):.3f}\")\n",
        "    # print(f\"Successes: {success}/{max_episodes}\")\n",
        "\n",
        "    return position_list, success_list, frames"
      ],
      "metadata": {
        "id": "LouNA-k1Olrg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.3.1 BC from REINFORCE (<5 min)**\n",
        "\n",
        "Behavioral Cloning (BC) is a supervised learning technique where the agent learns to imitate expert actions by fitting a regression model, such as a decision tree, to state-action pairs collected from expert demonstrations. In the next code blocks, BC serves as a baseline for policy performance, allowing direct comparison between imitation-based policies and reinforcement learning approaches like actor-critic. By using BC, you can assess how well a policy trained from demonstration alone performs in the Mountain Car environment and contrast it with policies that learn from the REINFORCE feedback.\n",
        "\n",
        "As before, if you find your agent not learning under 5 minutes, you might want to use other seed values."
      ],
      "metadata": {
        "id": "ftayqktdPCqK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.tree as tree"
      ],
      "metadata": {
        "id": "33guIR5JRHO1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Collect REINFORCE evaluations\n",
        "print('Expert')\n",
        "states, actions, _, _ = collect_expert_data(env, policy_estimator)\n",
        "\n",
        "# Train BC policy\n",
        "print('BC')\n",
        "bc_clf = tree.DecisionTreeRegressor()\n",
        "bc_clf = bc_clf.fit(states, actions)\n",
        "position, successes, frames = test_agent(env, bc_clf)"
      ],
      "metadata": {
        "id": "FoV2L3HfO8aV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.3.2 BC from Actor-Critic**\n",
        "\n",
        "Now we'll train another expert policy using an Actor-Critic algorithm (learning should reach a reward of above 80 in under 50 episodes/5 minutes), and collect demonstration data to train another BC policy. Again, please try different seed values if your agent is not learning correctly."
      ],
      "metadata": {
        "id": "OSK2Q4bZE5yr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ValueEstimator(nn.Module):\n",
        "    \"\"\"\n",
        "    PyTorch value function: linear head, MSE loss, Adam.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        state_dim=400,\n",
        "        learning_rate=1e-1,\n",
        "        device='cuda',\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Define the value head as a linear layer mapping featurized state to scalar value.\n",
        "        self.value_head = nn.Linear(state_dim, 1)\n",
        "\n",
        "        # 2. Initialize value head weights and biases to zero for reproducibility.\n",
        "        nn.init.zeros_(self.value_head.weight)\n",
        "        nn.init.zeros_(self.value_head.bias)\n",
        "\n",
        "        # Optimizer setup\n",
        "        self.optimizer = torch.optim.Adam(self.parameters(), lr=learning_rate)\n",
        "        self.featurize_state = featurize_state\n",
        "\n",
        "        # Initialize and store device for computations (CPU/GPU).\n",
        "        self.device = torch.device(device)\n",
        "        self.to(self.device)\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def predict(self, state):\n",
        "        # 1. Featurize the input state and convert it to a torch tensor appropriate for model input.\n",
        "        s = torch.as_tensor(self.featurize_state(state),dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        # 2. Pass this tensor through the value_head network to get the predicted value.\n",
        "        v = self.value_head(s)\n",
        "\n",
        "        # Return the value as a squeezed scalar for easy downstream use.\n",
        "        return v.squeeze()\n",
        "\n",
        "    def update(self, state, target):\n",
        "        # 1. Zero out gradients for the optimizer to prepare for the update step.\n",
        "        self.optimizer.zero_grad()\n",
        "        # 2. Featurize the input state, convert to tensor, and batchify for the network input.\n",
        "        s = torch.as_tensor(self.featurize_state(state), dtype=torch.float32, device=self.device).unsqueeze(0)\n",
        "        # 3. Convert the target to a tensor and shape appropriately for MSE calculation.\n",
        "        tgt = torch.as_tensor(target, dtype=torch.float32, device=self.device).view(1, 1)\n",
        "        # 4. Forward pass: predict the value for the featurized state.\n",
        "        v = self.value_head(s)\n",
        "        # 5. Calculate the mean squared error (MSE) loss between prediction and target value.\n",
        "        loss = F.mse_loss(v, tgt)\n",
        "        # 6. Backpropagate the loss and update the network parameters using the optimizer.\n",
        "        loss.backward()\n",
        "        self.optimizer.step()\n",
        "\n",
        "        return loss"
      ],
      "metadata": {
        "id": "fHv3Sn1JL7dV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def actor_critic(env, estimator_policy, estimator_value, num_episodes, discount_factor=1.0):\n",
        "    \"\"\"\n",
        "    Actor-Critic with function approximation (PyTorch estimators).\n",
        "    estimator_policy: has .predict(state) -> action and .update(state, target, action)\n",
        "    estimator_value:  has .predict(state) -> value  and .update(state, target)\n",
        "    \"\"\"\n",
        "    stats = {\"episode_rewards\":np.zeros(num_episodes, dtype=np.float32),\n",
        "             \"episode_lengths\":np.zeros(num_episodes, dtype=np.float32)}\n",
        "\n",
        "    Transition = collections.namedtuple(\n",
        "        \"Transition\", [\"state\", \"action\", \"reward\", \"next_state\", \"done\"]\n",
        "    )\n",
        "\n",
        "    for i_episode in range(num_episodes):\n",
        "        # 1. Reset the environment and obtain the initial state.\n",
        "        out = env.reset(seed=seed)\n",
        "        state = out[0]\n",
        "        episode = []\n",
        "\n",
        "        for t in itertools.count():\n",
        "            # 2. Use the policy network to select an action given the current state.\n",
        "            action = estimator_policy.predict(state).unsqueeze(0)\n",
        "\n",
        "            # 3. Step the environment using the selected action and observe results.\n",
        "            step_out = env.step([action.item()])\n",
        "            next_state, reward, terminated, truncated, _info = step_out\n",
        "            done = bool(terminated or truncated)\n",
        "\n",
        "            episode.append(Transition(state, action, reward, next_state, done))\n",
        "\n",
        "            # 4. Update episode reward and episode length statistics.\n",
        "            stats[\"episode_rewards\"][i_episode] += float(reward)\n",
        "            if done:\n",
        "                stats[\"episode_lengths\"][i_episode] = t + 1\n",
        "\n",
        "            # 5. Compute the value of the next state and calculate TD target and TD error for learning.\n",
        "            value_next = estimator_value.predict(next_state) if not done else 0.0\n",
        "            td_target = reward + discount_factor * value_next\n",
        "            td_error = td_target - estimator_value.predict(state)\n",
        "\n",
        "            # 6. Update the value function (\"critic\") using the TD target.\n",
        "            estimator_value.update(state, td_target)\n",
        "\n",
        "            # 7. Update the policy (\"actor\") using the TD error (advantage) and actual action taken.\n",
        "            estimator_policy.update(state, td_error, action)\n",
        "\n",
        "            print(\n",
        "                \"\\rStep {} @ Episode {}/{} Reward:{}\".format(\n",
        "                    t, i_episode + 1, num_episodes,\n",
        "                    stats[\"episode_rewards\"][i_episode - 1] if i_episode > 0 else 0.0\n",
        "                ),\n",
        "                end=\"\"\n",
        "            )\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            # 8. Move to the next state for the next time step/decision.\n",
        "            state = next_state\n",
        "\n",
        "    return stats\n"
      ],
      "metadata": {
        "id": "DNPzAR-fMBvm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "policy_estimator = PolicyEstimator(\n",
        "    learning_rate=1e-4,\n",
        ")\n",
        "value_estimator = ValueEstimator(\n",
        "    learning_rate=1e-1,\n",
        ")\n",
        "\n",
        "# Run training (same episode count / discount)\n",
        "num_episodes = 50\n",
        "discount_factor = 0.90\n",
        "\n",
        "stats = actor_critic(env, policy_estimator, value_estimator,\n",
        "                     num_episodes=num_episodes,\n",
        "                     discount_factor=discount_factor)\n",
        "\n",
        "print(stats)"
      ],
      "metadata": {
        "id": "39JsatgoMD0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Collect actor-critic evaluations\n",
        "print('Expert')\n",
        "states, actions, _, _ = collect_expert_data(env, policy_estimator)\n",
        "\n",
        "# Train BC policy\n",
        "print('BC')\n",
        "bc_clf = tree.DecisionTreeRegressor()\n",
        "bc_clf = bc_clf.fit(states, actions)\n",
        "position, successes, frames = test_agent(env, bc_clf)"
      ],
      "metadata": {
        "id": "UEXsz53gOAiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "visualize_policy(policy_estimator)"
      ],
      "metadata": {
        "id": "3IVr7ZTnFkPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "env.close()"
      ],
      "metadata": {
        "id": "UzVQIetpqzcY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2.4 [8 pts manual 5190 only] Analysis of Reinforcement and Imitation Learning Methods**\n"
      ],
      "metadata": {
        "id": "OFZdbkcYifo0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.4.1 [5190 2 pts manual] Actor-Critic Methods**\n",
        "Why does our actor-critic algorithm, which learns the state-value function $V(s)$ in addition to the policy $\\pi$, require fewer iterations (experiences in the environment) than the policy-gradient algorithm (which learns $\\pi$ directly) to achieve similar returns?"
      ],
      "metadata": {
        "id": "ZnSRuXUnGfsI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** [Your answer here]"
      ],
      "metadata": {
        "id": "xYGrvDbYhh3W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.4.2 [5190 2 pts manual] Reward Alignment**\n",
        "Review the [documentation](https://gymnasium.farama.org/environments/classic_control/mountain_car_continuous/#reward) of the reward function. Does the instantaneous reward encourage efficient solutions? Justify your answer."
      ],
      "metadata": {
        "id": "BvGMLc0IiAUS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** [Your answer here]"
      ],
      "metadata": {
        "id": "FZRPQPldA5ZI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.4.3 [5190 2 pts manual] Hyperparameters**\n",
        "\n",
        "Suppose we would like to modify the `reinforce()` call to encourage more efficient solutions. Which of the hyperparameter(s) could we change and how?"
      ],
      "metadata": {
        "id": "8docw9Pd_a6Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** [Your answer here]"
      ],
      "metadata": {
        "id": "cUs6t2jLBADK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **2.4.4 [5190 2 pts manual] Behavioral Cloning**\n",
        "\n",
        "For each of the 4 approaches in the previous section (REINFORCE, Actor-Critic, and BC using each of these as the expert), report the average undiscounted returns over 50 episodes for the trained policy. Why might it be possible for a BC policy to out-perform its expert?"
      ],
      "metadata": {
        "id": "4Dg4ZEad_eVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Answer:** [Your answer here]"
      ],
      "metadata": {
        "id": "F3lvXCbGAObW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KD6VKdQIwcxg"
      },
      "source": [
        "# **Submit to Gradescope**\n",
        "Congratulations! You've finished the homework. Don't forget to submit your final notebook on [Gradescope](gradescope.com)."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "QnSMLJcrsYC0"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}