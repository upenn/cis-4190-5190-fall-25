{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K937HdwxdVu"
   },
   "source": [
    "# **CIS 419/519**\n",
    "## NLP Worksheet\n",
    "In this notework, we rank plays and words of Shakespeare using Term Context Matrix and Term Document Matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fj1nLLdlzUKW",
    "outputId": "e2b3176a-7e03-40d7-9402-c084468a4a37"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=19To7EPkUd2bHTF0tqJqUkMSq1JRdK33k\n",
      "To: /content/play_names.txt\n",
      "100% 587/587 [00:00<00:00, 1.09MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1doAEuyZ5cBfzEBZUYqrW0RMKcciBZbk5\n",
      "To: /content/vocab.txt\n",
      "100% 206k/206k [00:00<00:00, 73.2MB/s]\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1wGL3C7xAoNdlfUi8fcotj31syc2rIyM_\n",
      "To: /content/will_play_text.csv\n",
      "100% 10.3M/10.3M [00:00<00:00, 162MB/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import csv\n",
    "import subprocess\n",
    "import re\n",
    "import random\n",
    "import numpy as np\n",
    "import gdown\n",
    "\n",
    "if not os.path.exists(\"play_names.txt\"):\n",
    "    !gdown --id 19To7EPkUd2bHTF0tqJqUkMSq1JRdK33k\n",
    "if not os.path.exists(\"vocab.txt\"):\n",
    "    !gdown --id 1doAEuyZ5cBfzEBZUYqrW0RMKcciBZbk5\n",
    "if not os.path.exists(\"will_play_txt.csv\"):\n",
    "    !gdown --id 1wGL3C7xAoNdlfUi8fcotj31syc2rIyM_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aEXkW5b9tvj9"
   },
   "outputs": [],
   "source": [
    "def read_in_shakespeare():\n",
    "    '''Reads in the Shakespeare dataset processesit into a list of tuples.\n",
    "       Also reads in the vocab and play name lists from files.\n",
    "    Each tuple consists of\n",
    "    tuple[0]: The name of the play\n",
    "    tuple[1] A line from the play as a list of tokenized words.\n",
    "    Returns:\n",
    "      tuples: A list of tuples in the above format.\n",
    "      document_names: A list of the plays present in the corpus.\n",
    "      vocab: A list of all tokens in the vocabulary.\n",
    "    '''\n",
    "\n",
    "    tuples = []\n",
    "\n",
    "    with open('will_play_text.csv') as f:\n",
    "        csv_reader = csv.reader(f, delimiter=';')\n",
    "        for row in csv_reader:\n",
    "            play_name = row[1]\n",
    "            line = row[5]\n",
    "            line_tokens = re.sub(r'[^a-zA-Z0-9\\s]', ' ', line).split()\n",
    "            line_tokens = [token.lower() for token in line_tokens]\n",
    "\n",
    "            tuples.append((play_name, line_tokens))\n",
    "\n",
    "    with open('vocab.txt') as f:\n",
    "        vocab = [line.strip() for line in f]\n",
    "\n",
    "    with open('play_names.txt') as f:\n",
    "        document_names = [line.strip() for line in f]\n",
    "\n",
    "    return tuples, document_names, vocab\n",
    "\n",
    "\n",
    "def get_row_vector(matrix, row_id):\n",
    "    return matrix[row_id, :]\n",
    "\n",
    "\n",
    "def get_column_vector(matrix, col_id):\n",
    "    return matrix[:, col_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qo5-vG1_ybep"
   },
   "source": [
    "## Term Document Matrix\n",
    "Recall that a term document matrix describes the frequency of terms that occur in a collection of documents. In the matrix, rows correspond to documents in the collection and columns correspond to terms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-NdB-d-0ya3w"
   },
   "outputs": [],
   "source": [
    "def create_term_document_matrix(line_tuples, document_names, vocab):\n",
    "    '''Returns a numpy array containing the term document matrix for the input lines.\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and \n",
    "      a tokenized line from that document.\n",
    "      document_names: A list of the document names\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "    Let m = len(vocab) and n = len(document_names).\n",
    "    Returns:\n",
    "      td_matrix: A mxn numpy array where the number of rows is the number of words\n",
    "          and each column corresponds to a document. A_ij contains the\n",
    "          frequency with which word i occurs in document j.\n",
    "    '''\n",
    "\n",
    "    vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "    docname_to_id = dict(zip(document_names, range(0, len(document_names))))\n",
    "    matrix = np.zeros((len(vocab), len(document_names)))\n",
    "    for (doc, lines) in line_tuples:\n",
    "        for w in lines:\n",
    "            matrix[vocab_to_id[w]][docname_to_id[doc]] += 1\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PK9Yg4Vgyuh6"
   },
   "source": [
    "## Term Context Matrix (word-word co-occurrence matrix)\n",
    "Instead of using entire documents, we can use smaller contexts to describe words. For example, we can use a paragraph or a window size of 10. A word is now defined by a vector over counts of context words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mtRivVdGyt_T"
   },
   "outputs": [],
   "source": [
    "def create_term_context_matrix(line_tuples, vocab, context_window_size=1):\n",
    "    '''Returns a numpy array containing the term context matrix for the input lines.\n",
    "    Inputs:\n",
    "      line_tuples: A list of tuples, containing the name of the document and \n",
    "      a tokenized line from that document.\n",
    "      vocab: A list of the tokens in the vocabulary\n",
    "    Let n = len(vocab).\n",
    "    Returns:\n",
    "      tc_matrix: A nxn numpy array where A_ij contains the frequency with which\n",
    "          word j was found within context_window_size to the left or right of\n",
    "          word i in any sentence in the tuples.\n",
    "    '''\n",
    "\n",
    "    vocab_to_id = dict(zip(vocab, range(0, len(vocab))))\n",
    "    matrix = np.zeros((len(vocab), len(vocab)))\n",
    "\n",
    "    for _, line in line_tuples:\n",
    "        for i in range(len(line)):\n",
    "          w = line[i]\n",
    "          for j in range(max(0, i-context_window_size), min(len(line), i+context_window_size)):\n",
    "            c = line[j]\n",
    "            matrix[vocab_to_id[w]][vocab_to_id[c]] += 1\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bt-cKP6rz8yv"
   },
   "source": [
    "## Ranking Similarity of Plays\n",
    "From the term document matrix, we can represent a document by a vector. Each element of the vector is the number of occurances of a word in the document. Therefore, by comparing two vectors, we can compare the similarity of two documents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J7QNbKmPvqxC"
   },
   "outputs": [],
   "source": [
    "def rank_plays(target_play_index, term_document_matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the plays to the target play.\n",
    "    Inputs:\n",
    "      target_play_index: The integer index of the play we want to compare all others against.\n",
    "      term_document_matrix: The term-document matrix as a mxn numpy array.\n",
    "      similarity_fn: Function that should be used to compared vectors for two\n",
    "        documents. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "        compute_cosine_similarity.\n",
    "    Returns:\n",
    "      A length-n list of integer indices corresponding to play names,\n",
    "      ordered by decreasing similarity to the play indexed by target_play_index\n",
    "    '''\n",
    "\n",
    "    _, N = term_document_matrix.shape\n",
    "    target = term_document_matrix[:, target_play_index]\n",
    "    similarity_score = np.zeros(N)\n",
    "    for i in range(N):\n",
    "        similarity_score[i] = similarity_fn(term_document_matrix[:, i], target)\n",
    "\n",
    "    return np.argsort(similarity_score*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SXC9rc-E3fF4"
   },
   "source": [
    "## Ranking Similarity of Words\n",
    "From the term context matrix, we can represent a word by a vector. Each element of the vector is the number of occurances of the word in a context. Therefore, by comparing two vectors, we can compare the similarity of two words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CefeTnuDvwQV"
   },
   "outputs": [],
   "source": [
    "def rank_words(target_word_index, matrix, similarity_fn):\n",
    "    ''' Ranks the similarity of all of the words to the target word.\n",
    "    Inputs:\n",
    "      target_word_index: The index of the word we want to compare all others against.\n",
    "      matrix: Numpy matrix where the ith row represents a vector embedding of the ith word.\n",
    "      similarity_fn: Function that should be used to compared vectors for two word\n",
    "        ebeddings. Either compute_dice_similarity, compute_jaccard_similarity, or\n",
    "        compute_cosine_similarity.\n",
    "    Returns:\n",
    "      A length-n list of integer word indices, ordered by decreasing similarity to the \n",
    "      target word indexed by word_index\n",
    "    '''\n",
    "\n",
    "    m, n = matrix.shape\n",
    "    target = matrix[target_word_index, :]\n",
    "    similarity_score = np.zeros(m)\n",
    "    for i in range(m):\n",
    "        similarity_score[i] = similarity_fn(matrix[i,:], target)\n",
    "\n",
    "    return np.argsort(similarity_score*-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2uiU2VIo0ima"
   },
   "source": [
    "## Similarity Metrics\n",
    "How can we compare the similarity between two vectors? We introduce three measures: cosine similarity, Jaccard similarity and Dice similarity. \n",
    "\n",
    "###Cosine Similarity\n",
    "\n",
    "$\\text{Similarity}({\\bf t},{\\bf e})= {{\\bf t} {\\bf e} \\over \\|{\\bf t}\\| \\|{\\bf e}\\|} = \\frac{ \\sum_{i=1}^{n}{{\\bf t}_i{\\bf e}_i} }{ \\sqrt{\\sum_{i=1}^{n}{({\\bf t}_i)^2}} \\sqrt{\\sum_{i=1}^{n}{({\\bf e}_i)^2}} }$ \\\\\n",
    "This equals the cosine of the angle between two vectors.\n",
    "\n",
    "###Jaccard Similarity\n",
    "$\\text{Similarity}({\\bf t}, {\\bf e}) = \\frac{|{\\bf t}\\cap {\\bf e}|}{|{\\bf t}\\cup {\\bf e}|}$\n",
    "\n",
    "###Dice Similarity \n",
    "$\\text{Similarity}({\\bf t}, {\\bf e}) = \\frac{2J}{J+1}$ where $J$ is the Jaccard Similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b6arU4_UwSpL"
   },
   "outputs": [],
   "source": [
    "def compute_cosine_similarity(vector1, vector2):\n",
    "    '''Computes the cosine similarity of the two input vectors.\n",
    "    Inputs:\n",
    "      vector1: A nx1 numpy array\n",
    "      vector2: A nx1 numpy array\n",
    "    Returns:\n",
    "      A scalar similarity value.\n",
    "    '''\n",
    "\n",
    "    return np.inner(vector1, vector2)/(np.linalg.norm(vector1)*np.linalg.norm(vector2))\n",
    "\n",
    "\n",
    "def compute_jaccard_similarity(vector1, vector2):\n",
    "    '''Computes the jaccard similarity of the two input vectors.\n",
    "    Inputs:\n",
    "      vector1: A nx1 numpy array\n",
    "      vector2: A nx1 numpy array\n",
    "    Returns:\n",
    "      A scalar similarity value.\n",
    "    '''\n",
    "\n",
    "    return np.sum(np.minimum(vector1, vector2))/np.sum(np.maximum(vector1, vector2))\n",
    "\n",
    "\n",
    "def compute_dice_similarity(vector1, vector2):\n",
    "    '''Computes the dice similarity of the two input vectors.\n",
    "    Inputs:\n",
    "      vector1: A nx1 numpy array\n",
    "      vector2: A nx1 numpy array\n",
    "    Returns:\n",
    "      A scalar similarity value.\n",
    "    '''\n",
    "\n",
    "    j = compute_jaccard_similarity(vector1, vector2)\n",
    "    return 2*j/(j+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eN-37DzL3ydZ"
   },
   "source": [
    "## Running the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m02X0TIQv1Js",
    "outputId": "544fd57c-2a0d-49dc-8e41-77a8ab867ae9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing term document matrix...\n",
      "Computing term context matrix...\n",
      "\n",
      "The 10 most similar plays to \"Hamlet\" using compute_cosine_similarity are:\n",
      "1: Henry VIII\n",
      "2: A Winters Tale\n",
      "3: Troilus and Cressida\n",
      "4: Cymbeline\n",
      "5: King Lear\n",
      "6: Pericles\n",
      "7: Richard III\n",
      "8: macbeth\n",
      "9: King John\n",
      "10: Loves Labours Lost\n",
      "\n",
      "The 10 most similar plays to \"Hamlet\" using compute_jaccard_similarity are:\n",
      "1: Othello\n",
      "2: Cymbeline\n",
      "3: A Winters Tale\n",
      "4: King Lear\n",
      "5: Richard III\n",
      "6: Coriolanus\n",
      "7: Troilus and Cressida\n",
      "8: Henry VIII\n",
      "9: Alls well that ends well\n",
      "10: Antony and Cleopatra\n",
      "\n",
      "The 10 most similar plays to \"Hamlet\" using compute_dice_similarity are:\n",
      "1: Othello\n",
      "2: Cymbeline\n",
      "3: A Winters Tale\n",
      "4: King Lear\n",
      "5: Richard III\n",
      "6: Coriolanus\n",
      "7: Troilus and Cressida\n",
      "8: Henry VIII\n",
      "9: Alls well that ends well\n",
      "10: Antony and Cleopatra\n",
      "\n",
      "The 10 most similar words to \"abhor\" using compute_cosine_similarity on term-context frequency matrix are:\n",
      "0: abhor\n",
      "1: i\n",
      "2: gaps\n",
      "3: promulgate\n",
      "4: wis\n",
      "5: recommended\n",
      "6: propend\n",
      "7: debted\n",
      "8: weringly\n",
      "9: erween\n",
      "10: avenue\n",
      "\n",
      "The 10 most similar words to \"abhor\" using compute_jaccard_similarity on term-context frequency matrix are:\n",
      "0: abhor\n",
      "1: conceive\n",
      "2: apply\n",
      "3: commit\n",
      "4: choke\n",
      "5: invite\n",
      "6: approve\n",
      "7: torment\n",
      "8: rob\n",
      "9: procure\n",
      "10: adore\n",
      "\n",
      "The 10 most similar words to \"abhor\" using compute_dice_similarity on term-context frequency matrix are:\n",
      "0: abhor\n",
      "1: conceive\n",
      "2: apply\n",
      "3: commit\n",
      "4: choke\n",
      "5: invite\n",
      "6: approve\n",
      "7: torment\n",
      "8: rob\n",
      "9: procure\n",
      "10: adore\n"
     ]
    }
   ],
   "source": [
    "tuples, document_names, vocab = read_in_shakespeare()\n",
    "\n",
    "print('Computing term document matrix...')\n",
    "td_matrix = create_term_document_matrix(tuples, document_names, vocab)\n",
    "\n",
    "print('Computing term context matrix...')\n",
    "tc_matrix = create_term_context_matrix(tuples, vocab, context_window_size=2)\n",
    "\n",
    "\n",
    "random_idx = 6 #for Hamlet\n",
    "similarity_fns = [compute_cosine_similarity, compute_jaccard_similarity, compute_dice_similarity]\n",
    "for sim_fn in similarity_fns:\n",
    "  print('\\nThe 10 most similar plays to \"%s\" using %s are:' % (document_names[random_idx], sim_fn.__qualname__))\n",
    "  ranks = rank_plays(random_idx, td_matrix, sim_fn)\n",
    "  for idx in range(1, 11):\n",
    "    doc_id = ranks[idx]\n",
    "    print('%d: %s' % (idx, document_names[doc_id]))\n",
    "\n",
    "word = 'abhor'\n",
    "vocab_to_index = dict(zip(vocab, range(0, len(vocab))))\n",
    "for sim_fn in similarity_fns:\n",
    "  print('\\nThe 10 most similar words to \"%s\" using %s on term-context frequency matrix are:' % (word, sim_fn.__qualname__))\n",
    "  ranks = rank_words(vocab_to_index[word], tc_matrix, sim_fn)\n",
    "  for idx in range(0, 11):\n",
    "    word_id = ranks[idx]\n",
    "    print('%d: %s' % (idx, vocab[word_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lP9lRTAi31DB"
   },
   "source": [
    "## Result Analysis\n",
    "The ranking between Jaccard Similarity and Dice Similarity are identical for both cases, which is probably due to the similar nature of Jaccard and Dice scoring.  \\\n",
    "Many other conclusions could be drawn and this is left as an exercise for the reader."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "NLP Comparing Documents Worksheet.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/CIS-519/worksheet-dev/blob/master/module_10_nlp_comparing_documents.ipynb",
     "timestamp": 1635949407104
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
