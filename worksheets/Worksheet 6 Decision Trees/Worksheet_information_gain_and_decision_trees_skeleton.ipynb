{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ppukpJ6iazKX"
   },
   "source": [
    "#CIS 519\n",
    "\n",
    "##Decision Trees and Information Gain\n",
    "\n",
    "# **Information Gain**\n",
    "**Goal** : To understand how to compute Information gain for a given dataset programtically and Manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guXs5Na3bzw4"
   },
   "source": [
    "# 1. Recap\n",
    "\n",
    "Unlike linear regression, decision trees can pick up nonlinear interactions between variables in the data. In a decision tree, what we essentially do is keep making decisions(splits) until certain criteria are met. Once met we can use it to classify or make a prediction. But, if you have a dataset with thousands of variables/columns how do you decide which variables/columns are the most efficient to split on? A popular way to solve this problem, especially if using an ID3 algorithm, is to use entropy and information gain.\n",
    "## Entropy\n",
    "In information theory, \"entropy\" measures the level of uncertainty or surprise with the outcome of a random variable. Let $X$ a discrete random variable with possible outcomes $x_i$ which occur with probability $P(x_{i})$ , the entropy of $X$ is formally defined as:\n",
    "$$H(X) = -\\sum_{i} P(x_{i}) \\log(P(x_{i})$$\n",
    "\n",
    "In simpler terms, entropy is used as a way to measure how “mixed” a column is. Lets start by finding the entropy of our target column \"diabetic?\" There are five people who are diabetic and five people who aren't. If someone was going to ask you how mixed the column is, you could say it was equally mixed, with half of the people being diabetic and the other half not being diabetic. Entropy gives us a way to quantify this. The more mixed the columns are, the higher the entropy.\n",
    "<div align=\"center\">\n",
    "<img src=\"https://drive.google.com/uc?id=1y6dtvgznnS4nXWxNLFid055aYnwrNse8\" alt=\"drawing\" width=\"70\"/>\n",
    "</div>\n",
    "\n",
    "Our goal is to find the best variable(s)/column(s) to split on when building a decision tree. Eventually, we want to keep splitting the variables/columns until our mixed target column is no longer mixed i.e we want splits that lower the entropy of our target column.\n",
    "\n",
    "## Information Gain (aka Mutual Information)\n",
    "One heuristic to decide on which feature to partition the training instances into smaller subsets is the \"INFORMATION GAIN\". Information gain measures the difference between the entropy $H(X)$ and the conditional entropy $H(X|A)$. Intuitively, one can think of this as the difference between the current node's entropy and the weighted sum of the entropy of its children nodes.  \n",
    "<div align=\"center\">\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1e14Nj9jej7_2vUuuVU4wtn6FOwpX2g2X\" alt=\"drawing\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "---\n",
    "> After computing $InfoGain(X,A), \\forall A$, we pick the attribute with the maximum information gain. Why? (think about $H(X|A)$. We want this value to be high, or low?)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "## How to Compute Information Gain\n",
    "Computing the information gain for selecting an attribute $A$ on a dataset $X$\n",
    "<div align=\"center\">\n",
    "<img src=\"http://drive.google.com/uc?export=view&id=1EBaULnksLceSdIGOCZKlHNRLa_g5CEU-\" alt=\"drawing\" width=\"500\"/>\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gOVhr87DnSe8"
   },
   "source": [
    "# 2. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKzLY6CPbaH-"
   },
   "outputs": [],
   "source": [
    "## 1. creating data\n",
    "\"\"\"\n",
    "  In this example, we use pandas library for data handling. One can also use numpy.ndarray\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "data = pd.DataFrame({\"Ageover50\":[\"True\",\"False\",\"False\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"False\"],\n",
    "                     \"smoking\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"True\",\"False\",\"False\",\"True\",\"False\"],\n",
    "                     \"asthma\":[\"True\",\"True\",\"True\",\"True\",\"True\",\"True\",\"False\",\"True\",\"True\",\"True\"],\n",
    "                     \"obese\":[\"True\",\"True\",\"False\",\"True\",\"True\",\"False\",\"False\",\"False\",\"True\",\"True\"],\n",
    "                     \"diabetic\":[\"yes\",\"yes\",\"no\",\"yes\",\"no\",\"yes\",\"no\",\"no\",\"yes\",\"no\"]},\n",
    "                    columns=[\"Ageover50\",\"smoking\",\"asthma\",\"obese\",\"diabetic\"])\n",
    "\n",
    "features = data[[\"Ageover50\",\"smoking\",\"asthma\",\"obese\"]]\n",
    "target = data[\"diabetic\"]\n",
    "# Display the data\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SmQQzrICoKbc"
   },
   "source": [
    "---\n",
    "## Computing Information Gain Manually\n",
    "Let computing the information gain for the attribute \"Age over 50\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vT3jFBFlpQ_L"
   },
   "source": [
    "### Entropy $H(X)$ (w.r.t the classes)\n",
    "There are $10$ instances in total\n",
    "\n",
    "Number of instances in the class $c_1 = diabetic$: $5$ \\\\\n",
    "Number of instances in the class $c_2 = non-diabetic$: $5$ \\\\\n",
    "Then, we can compute:\n",
    "$$P(X = c_1) = 5/10 = 0.5$$\n",
    "$$P(X = c_2) = 5/10 = 0.5$$\n",
    "And eventually,  \n",
    "$$H(X) = -\\sum_i P(c_i) \\log(P(c_i)) = - (0.5 * \\log_2(0.5) + 0.5 * \\log_2(0.5) = 1.0$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zGf_eyfrDJD"
   },
   "source": [
    "### Conditional Entropy $H(X|A)$\n",
    "Note that $A$ has two possible outcomes: over 50, and not over 50.\n",
    "$$H(X|A) = P(A = \\mbox{\"over 50\"}) * H(X|A = \\mbox{\"over 50\"}) + P(A = \\mbox{\"not over 50\"}) * H(X|A = \\mbox{\"not over 50\"})$$\n",
    "\n",
    "The total number of instances is still $10$ \\\\\n",
    "Based on column 1,\n",
    "$$P(A = \\mbox{\"over 50\"}) = 6/10$$\n",
    "$$P(A = \\mbox{\"NOT over 50\"}) = 4/10$$\n",
    "\n",
    "Number of instances in the class $c_1 = diabetic$ that has \"age over 50\": $4$ (rows 0, 3, 5, 8). Conditional probability: \\\\\n",
    "\n",
    "$$P_{11} = P(X = c_1| A = \\mbox{\"over 50\"}) = P(X = c_1, A = \\mbox{\"over 50\"})/ P(A = \\mbox{\"over 50\"})  = (4/10)/(6/10)= 4/6$$\n",
    "\n",
    "Number of instances in the class $c_2 = non-diabetic$ that has \"age over 50\": $2$ (rows 4, 6) \\\\\n",
    "$$P_{21} = P(X = c_2| A = \\mbox{\"over 50\"}) = P(X = c_2, A = \\mbox{\"over 50\"})/P(A = \\mbox{\"over 50\"}) =  (2/10)/(6/10)= 2/6$$\n",
    "\n",
    "Number of instances in the class $c_1 = diabetic$ that has \"age NOT over 50\": $1$ (rows 1) \\\\\n",
    "$$P_{12} =  P(X = c_1| A = \\mbox{\"NOT over 50\"}) = P(X = c_1, A = \\mbox{\"NOT over 50\"})/ P(A = \\mbox{\"NOT over 50\"}) =  (1/10)/(4/10)= 1/4$$\n",
    "\n",
    "Number of instances in the class $c_2 = non-diabetic$ that has \"age NOT over 50\": $1$ (rows 2, 7, 9) \\\\\n",
    "$$P_{22} = P(X = c_2| A = \\mbox{\"NOT over 50\"}) =  P(X = c_2, A = \\mbox{\"NOT over 50\"})/ P(A = \\mbox{\"NOT over 50\"}) =  (3/10)/(4/10)= 3/4$$\n",
    "\n",
    "So\n",
    "$$H(X|A = \\mbox{\"over 50\"}) = - (P_{11} * \\log_2  P_{11} + P_{21} * \\log_2 P_{21}) =  4/6 * \\log_2 (4/6) + 2/6 * \\log_2 (2/6) = 0.918$$\n",
    "\n",
    "$$H(X|A = \\mbox{\"Not over 50\"}) = - (P_{12} * \\log_2  P_{12} + P_{22} * \\log_2 P_{22}) =  1/4 * \\log_2 (1/4) + 3/4 * \\log_2 (3/4) = 0.811$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "$$H(X|A) = P(A = \\mbox{\"over 50\"}) * H(X|A = \\mbox{\"over 50\"}) + P(A = \\mbox{\"not over 50\"}) * H(X|A = \\mbox{\"not over 50\"}) $$\n",
    "$$H(X|A) = 6/10 * 0.918 + 4/10 * 0.811 = 0.875$$\n",
    "\n",
    "### Information Gain\n",
    "Eventually,\n",
    "$$H(X) - H(X|A) = 1 - 0.875 = 0.125$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LCdncnzaoPud"
   },
   "source": [
    "---\n",
    "## Computing Information Gain in Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ltNkvcd4xCNp"
   },
   "outputs": [],
   "source": [
    "def entropy(target):\n",
    "    \"\"\"\n",
    "    Calculate the entropy of a dataset.\n",
    "    The only parameter of this function is the target_col parameter which specifies the target column\n",
    "    \"\"\"\n",
    "    # TODO: Get the unique values of the column and their counts and store in an array. Maybe use numpy.unique()\n",
    "    elem,count =\n",
    "\n",
    "    # TODO: Calculate the entropy for the target with the counts calculated above\n",
    "    entropy =\n",
    "    return entropy\n",
    "\n",
    "def InfoGain(data,attribute_name,target_name=\"class\"):\n",
    "    \"\"\"\n",
    "    Calculate the information gain of a dataset. This function takes three parameters:\n",
    "    1. data = The dataset for whose feature the IG should be calculated (pd Dataframe)\n",
    "    2. attribute_name = the name of the feature for which the information gain should be calculated(string)\n",
    "    3. target_name = the name of the target feature.(string)\n",
    "    \"\"\"\n",
    "    # TODO: Calculate the entropy of the total dataset using the function we completed before\n",
    "    total_entropy = entropy(...)\n",
    "\n",
    "    # TODO: Calculate the values and the corresponding counts for the attribute by which tree is split\n",
    "    vals, counts= n\n",
    "\n",
    "    # TODO: Calculate the weighted entropy\n",
    "    # note: when selecting values in a dataframe, using df.where() is very useful, and might need to be used as df.where().drop_na()\n",
    "    Weighted_Entropy =\n",
    "\n",
    "    # TODO: Calculate the information gain\n",
    "    Information_Gain =\n",
    "\n",
    "    return Information_Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kxi_YReYxTl6"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VFaWIyUHcBm8"
   },
   "source": [
    "Compute the attribute with the highest information gain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-DGsajDcBm8"
   },
   "outputs": [],
   "source": [
    "def highest_info_gain(columns):\n",
    "  \"\"\"\n",
    "  Calculates the variable/column name with the highest information gain.\n",
    "  \"\"\"\n",
    "  #Intialize an empty dictionary for information gains\n",
    "  information_gains = {}\n",
    "\n",
    "  #TODO: Iterate through each column name in our list and find the column with the highest info gain\n",
    "  for col in columns:\n",
    "    #Find the information gain for the column\n",
    "    information_gain =\n",
    "    #Add the information gain to our dictionary using the column name as the ekey\n",
    "    information_gains[col] = information_gain\n",
    "  print(\"Information gains of each attribute : \", information_gains)\n",
    "\n",
    "  # TODO: Return the key with the highest value\n",
    "  return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "09N7GcFOcBm8"
   },
   "outputs": [],
   "source": [
    "columns=[\"Ageover50\",\"smoking\",\"asthma\",\"obese\"]\n",
    "print(highest_info_gain(columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7-9_Jd0lRwaY"
   },
   "source": [
    "# **Decision Trees**\n",
    "Now we will create some decision trees using these principles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sbtFpqEqRwaY"
   },
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LZ0cM6i3RwaY"
   },
   "outputs": [],
   "source": [
    "# Load data\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Use IRIS dataset that can be loaded using load_iris\n",
    "iris = load_iris()\n",
    "\n",
    "# Investigate data\n",
    "print(dir(iris))\n",
    "\n",
    "iris_data = iris['data']\n",
    "iris_target  = iris['target']\n",
    "iris_feature_names = iris['feature_names']\n",
    "iris_target_names  = iris['target_names']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jBCPBsavRwaY"
   },
   "source": [
    "## Split Test and Train Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NR6dZNrsRwaZ"
   },
   "outputs": [],
   "source": [
    "# Split data\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We will use only two features (petal length and width) for this illustration as it is easy to visualize decision boundaries for 2D data\n",
    "X = iris_data[:, -2:]\n",
    "y = iris_target\n",
    "\n",
    "# TODO: Split 20% of the data to act as the test set\n",
    "X_train, X_test, y_train, y_test ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HWDs7uZ8Rwaa"
   },
   "source": [
    "## Create a Classifier and Fit the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nhxWuUr2Rwaa"
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# TODO: Initialize a decision tree. Take a look at the documentation for the function imported above.\n",
    "tree_clf =\n",
    "\n",
    "# TODO: fit the initialized tree to the dataset. clf.fit() will be useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L8qEGHJyRwaa"
   },
   "source": [
    "## Now let's see the accuracy on the train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uEmvGuodRwab"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# TODO: predict the labels for the train set and generate the confusion matrix for the train set.\n",
    "# The classifier.predict() function should be useful.\n",
    "y_pred =\n",
    "confusion_matrix_train =\n",
    "print(confusion_matrix_train)\n",
    "\n",
    "# TODO: predict the labels for the test set and generate the confusion matrix for the test set.\n",
    "y_pred =\n",
    "confusion_matrix_test =\n",
    "print(confusion_matrix_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2v5WYonTRwab"
   },
   "source": [
    "## Visualization\n",
    "We can actually visualze the decision boundary of the classifier with the code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6M7GWj-ORwab"
   },
   "outputs": [],
   "source": [
    "# Visualize the tree\n",
    "from graphviz import Source\n",
    "from sklearn.tree import export_graphviz\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "out_file = \"iris_tree.dot\"\n",
    "\n",
    "# Draw the tree and save the file as a .dot file\n",
    "export_graphviz(tree_clf,\n",
    "                out_file=out_file,\n",
    "                feature_names=iris.feature_names[-2:],\n",
    "                class_names=iris.target_names,\n",
    "                rounded=True,\n",
    "                filled=True)\n",
    "\n",
    "Source.from_file(out_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g2qXwfJJRwab"
   },
   "outputs": [],
   "source": [
    "# Function to plot the decision boundary of a decision tree classifier\n",
    "def plot_decision_boundary(clf, X, y, axes=[0, 7.5, 0, 3]):\n",
    "    x1s = np.linspace(axes[0], axes[1], 100)\n",
    "    x2s = np.linspace(axes[2], axes[3], 100)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s) # Create a mesh of points across the space\n",
    "    X_new = np.c_[x1.ravel(), x2.ravel()]\n",
    "    y_pred = clf.predict(X_new).reshape(x1.shape) # generate predictions for the space\n",
    "    custom_cmap = ListedColormap(['#fafab0','#9898ff','#a0faa0'])\n",
    "    plt.contourf(x1, x2, y_pred, alpha=0.5, cmap=custom_cmap) # draw a contour of the different classifications\n",
    "\n",
    "    plt.plot(X[:, 0][y==0], X[:, 1][y==0], \"yo\", label=\"Iris-Setosa\")\n",
    "    plt.plot(X[:, 0][y==1], X[:, 1][y==1], \"bs\", label=\"Iris-Versicolor\")\n",
    "    plt.plot(X[:, 0][y==2], X[:, 1][y==2], \"g^\", label=\"Iris-Virginica\")\n",
    "    plt.axis(axes)\n",
    "\n",
    "    plt.xlabel(\"Petal length\", fontsize=14)\n",
    "    plt.ylabel(\"Petal width\", fontsize=14)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plot_decision_boundary(tree_clf, X, y)\n",
    "\n",
    "# here we show the splits chosen\n",
    "plt.plot([0, 7.5], [0.8, 0.8], \"k-\", linewidth=2, label=\"Split-1\")\n",
    "plt.plot([0, 7.5], [1.75, 1.75], \"k--\", linewidth=2, label=\"Split-2\")\n",
    "\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "https://github.com/CIS-519/worksheet-dev/blob/master/module_02_Information_Gain_and_Decision_Trees_skeleton.ipynb",
     "timestamp": 1631559835624
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "nteract": {
   "version": "0.26.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
