{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"module_11_Gym_and_RL_solutions.ipynb","provenance":[{"file_id":"https://github.com/CIS-519/worksheet-dev/blob/master/module_11_Gym_and_RL.ipynb","timestamp":1636541705123},{"file_id":"https://github.com/CIS-519/worksheet-dev/blob/master/week12_gym_envs_for_RL.ipynb","timestamp":1636504093312}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"u-U2JS_unzXC"},"source":["# Installing Dependencies"]},{"cell_type":"code","metadata":{"id":"gi5AKiDQlwkk"},"source":["!apt-get -qq -y install libnvtoolsext1 > /dev/null\n","!ln -snf /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so.8.0 /usr/lib/x86_64-linux-gnu/libnvrtc-builtins.so\n","!apt-get -qq -y install xvfb freeglut3-dev ffmpeg> /dev/null\n","!pip -q install gym\n","!pip -q install pyglet \n","!pip -q install pyopengl\n","!pip -q install pyvirtualdisplay\n","!pip -q install stable-baselines3\n","!pip -q install box2d-py\n","!pip -q install gym[Box_2D]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tE7_hSrIlpFu"},"source":["import gym\n","import numpy as np\n","import math\n","import matplotlib.pyplot as plt\n","from collections import deque\n","import random\n","from gym import wrappers\n","\n","from pyvirtualdisplay import Display\n","display = Display(visible=0, size=(1024, 768))\n","display.start()\n","import os\n","# os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n","\n","import matplotlib.animation\n","import numpy as np\n","from IPython.display import HTML\n","\n","# Note, you may get a few warnings regarding Tensorflow and xdpyinfo, these are to be expected\n","import itertools"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"571E-Bzun9SR"},"source":["#OpenAI Gym\n","\n","Gym is a toolkit for developing and comparing reinforcement learning algorithms. It makes no assumptions about the structure of your agent, and is compatible with any numerical computation library, such as TensorFlow or PyTorch.\n","\n","In this notebook, we will have a quick introduction to OpenAI Gym and code-up one example environment named \"MountainCar\".\n","\n","**We strongly recommend looking over the [\"Getting Started\" documentation](https://gym.openai.com/docs/) before working on this worksheet.**"]},{"cell_type":"markdown","metadata":{"id":"99YSECXsokoK"},"source":["# Lunar Lander Environment\n","We are trying to land a robot on the moon! \n","\n","From OpenAI Gym docs:\n","Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Four discrete actions available: do nothing, fire left orientation engine, fire main engine, fire right orientation engine.\n","\n","\n",">The observation space is 6 dimensional`:\n","- X position\n","- Y position\n","- X velocity\n","- Y velocity\n","- θ angle\n","- ω angular velocity\n","- bool: left leg on gound\n","- bool: right leg on ground\n","\n",">There are four possible actions of type `Discrete(4)`:\n","- 0: Do nothing\n","- 1: Fire left engine\n","- 2: Fire main engine\n","- 3: Fire right engine\n"]},{"cell_type":"markdown","metadata":{"id":"yD3aeXfOV4ux"},"source":["Let's now create the environment from gym. "]},{"cell_type":"code","metadata":{"id":"_FiuKfHvjJMh"},"source":["# Initialize the environment\n","np.random.seed(0)\n","env = gym.make('LunarLander-v2')\n","print(env.observation_space) # This is the state representation\n","print(env.action_space) # These are the available actions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"A_IMHKLFWJfV"},"source":["In order to interact with the environment, we're going to use the following line of code in a loop to represent one environment step\n","\n","```observation, reward, done, info = env.step(action)``` \n","\n","Here, we pass in an action to the environment and it will update some internal variables and produce the new state (observation), the reward collected at that timestep, whether the simulation has been completed, and then some extra environment specific info. \n","\n","We can use this call in a for loop to simulate a complete episode or rollout. "]},{"cell_type":"code","metadata":{"id":"Yu-CURyLo2bw"},"source":["# Helper functions\n","def play_once(env, render=False, verbose=False):\n","    '''\n","    Run the simulation for 1 episode\n","    '''\n","    frames = []\n","    observation = env.reset()\n","    episode_reward = 0.\n","    for step in itertools.count():\n","        if render:\n","            frames.append(env.render(mode = 'rgb_array'))\n","        action = get_action(observation)\n","        observation, reward, done, _ = env.step(action)\n","        episode_reward += reward\n","        if done:\n","            break\n","    if verbose:\n","        print('got {} rewards in {} steps'.format(\n","                episode_reward, step + 1))\n","    return episode_reward, frames\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"02IcSnOstWNt"},"source":["## The Simulation\n","Our task is to develop a policy -- a mapping from the robot's current state to an action (`do nothing`, `thrust left`, `thrust up`, `thrust right`) that is evaluated at each timestep. The `play_once` function above will call a function `get_action` to get the action that we want to apply to the car given its current state.\n","\n","Let's start with a dumb policy, simply picking a random action amongst the available options. "]},{"cell_type":"code","metadata":{"id":"BbpfjYVYuGsX"},"source":["def get_action(observation):\n","    action = env.action_space.sample()\n","    return action"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SGAx32RQuMzE"},"source":["# run the simulation\n","episode_rewards, frames = play_once(env, render=True, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WjpGc3zAuRyk"},"source":["In order to see what happened, we can use the following code to generate a video from the frames of the rollout. "]},{"cell_type":"code","metadata":{"id":"0fVwXzjsuP-4"},"source":["# render the frames\n","plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n","patch = plt.imshow(frames[0])\n","plt.axis('off')\n","animate = lambda i: patch.set_data(frames[i])\n","ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n","HTML(ani.to_jshtml())\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ywX2pmfMX_6f"},"source":["# Reinforcement Learning Policy\n","\n","Ultimately, we want to create a much better policy than a random action from the observation. Clearly, we can see that given the 8 observation variables, picking an action is hard. In order to be able to solve this task, we will use a powerful algorithm, `A2C (Asynchronous Actor-Critic)`, from a large collection of actively maintained RL algorithms, `stable baselines`."]},{"cell_type":"markdown","metadata":{"id":"RiYpPi4_Y0mX"},"source":["Let's start by creating the model and then training it for a number of epochs. \n"]},{"cell_type":"code","metadata":{"id":"3mZNpIJnYzfe"},"source":["from stable_baselines3 import A2C\n","\n","model = A2C(\"MlpPolicy\", env, verbose=1)\n","model.learn(total_timesteps=30000)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jdXftCkDZX3D"},"source":["That was quite simple! We simply need to invoke the model and then train the model using 2 lines of code. Naturally, there's a number of hyperparameters as well as other algorithms available, so if you want to learn more or experiment the documentation is available [here](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html)\n"]},{"cell_type":"markdown","metadata":{"id":"KZvzDYPJZuKq"},"source":["Now let's see how our model does:"]},{"cell_type":"code","metadata":{"id":"Mpdywm_jZ11P"},"source":["def play_once(env, render=False, verbose=False):\n","    '''\n","    Run the simulation for 1 episode\n","    '''\n","    frames = []\n","    observation = env.reset()\n","    episode_reward = 0.\n","    for step in itertools.count():\n","        if render:\n","            frames.append(env.render(mode = 'rgb_array'))\n","        action, _states = model.predict(observation)\n","        observation, reward, done, _ = env.step(action)\n","        episode_reward += reward\n","        if done:\n","            break\n","    if verbose:\n","        print('got {} rewards in {} steps'.format(\n","                episode_reward, step + 1))\n","    return episode_reward, frames\n","\n","# run the simulation\n","episode_rewards, frames = play_once(env, render=True, verbose=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"j6QM56HdaBuc"},"source":["# render the frames\n","plt.figure(figsize=(frames[0].shape[1] / 72.0, frames[0].shape[0] / 72.0), dpi = 72)\n","patch = plt.imshow(frames[0])\n","plt.axis('off')\n","animate = lambda i: patch.set_data(frames[i])\n","ani = matplotlib.animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval = 50)\n","HTML(ani.to_jshtml())"],"execution_count":null,"outputs":[]}]}