{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjppR2sWvIPE"
   },
   "source": [
    "#**CIS 419/519**\n",
    "\n",
    "## Logistic Regression worksheet\n",
    "\n",
    "In this notebook, two examples for logistic regression are provided. In the first example, you will use the logistic regression module in `sklearn` to do a multi-class classification with the IRIS dataset. In the second example, you will explore using L1 regularization with logistic regression with the popular MNIST dataset. More information on the Logistic Regression function in `sklearn` can be found [here](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html?highlight=logistic%20regression#sklearn.linear_model.LogisticRegression)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iPyeQEh1vTaz"
   },
   "source": [
    "---\n",
    "#Example 1 - Multi-class classification with logistic regression in `sklearn`\n",
    "\n",
    "In this example, you will use the IRIS plant dataset to perform multi-class classification with the logistic regression function in the `sklearn` package. This example is adapted from [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_iris_logistic.html#sphx-glr-auto-examples-linear-model-plot-iris-logistic-py). For more information on the dataset can be found [here](https://scikit-learn.org/stable/datasets/toy_dataset.html#iris-dataset).\n",
    "\n",
    "---\n",
    "\n",
    "As usual, the first step is to load the relevant libraries for computation and plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zmlB-nPSvgm8"
   },
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import libraries from sklearn\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNA2KC5dvkgR"
   },
   "source": [
    "Next, we load the IRIS dataset and create the logistic regression classifier. Then, we create the decision boundaries for visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 218,
     "status": "error",
     "timestamp": 1675976045652,
     "user": {
      "displayName": "Anusha Srikanthan",
      "userId": "13441616374621952773"
     },
     "user_tz": 300
    },
    "id": "XCram8UVvCtI",
    "outputId": "27b4aa63-125e-4bd1-9f93-9dbb2d5d9a34"
   },
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "iris    = datasets.load_iris()\n",
    "X       = iris.data[:, :2]  # we only take the first two features.\n",
    "y       = iris.target\n",
    "\n",
    "#Use 20% of the data as test data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# Create an instance of Logistic Regression Classifier and fit the data.\n",
    "\n",
    "logreg  = LogisticRegression(penalty = 'none')\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "\n",
    "# Create the decision boundaries. We need to create a mesh and test the class of each point in the space.\n",
    "# For that, we will assign a color to each point in the mesh [x_min, x_max]x[y_min, y_max].\n",
    "x_min, x_max  = X_train[:, 0].min() - .5, X_train[:, 0].max() + .5\n",
    "y_min, y_max  = X_train[:, 1].min() - .5, X_train[:, 1].max() + .5\n",
    "h             = .02  # step size in the mesh\n",
    "xx, yy        = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))\n",
    "Z             = logreg.predict(np.c_[xx.ravel(), yy.ravel()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tj_sr0Fsv4zK"
   },
   "source": [
    "Next, we plot the predictions results and training points in a color plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 208
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1632757887799,
     "user": {
      "displayName": "Kelly Feng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiE_zqw1wGgtnqrMNNYVIn3LXnFSdlpXcozSv-W=s64",
      "userId": "05766461114990380038"
     },
     "user_tz": 240
    },
    "id": "4m4cokYtv46X",
    "outputId": "ce732f72-aaf6-4441-8bd3-19f176d45292"
   },
   "outputs": [],
   "source": [
    "# Put the result into a color plot\n",
    "Z = Z.reshape(xx.shape)\n",
    "plt.figure(1, figsize=(4, 3))\n",
    "plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)\n",
    "\n",
    "# Plot also the training points\n",
    "plt.scatter(X_train[:, 0], X_train[:, 1], c=y_train, edgecolors='k', cmap=plt.cm.Paired)\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "\n",
    "plt.xlim(xx.min(), xx.max())\n",
    "plt.ylim(yy.min(), yy.max())\n",
    "plt.xticks(())\n",
    "plt.yticks(())\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MayaMzXb0EtP"
   },
   "source": [
    "---\n",
    "#Example 2 - Exploring logistic regression with regularization\n",
    "\n",
    "In this next example, we explore the implementation of L1 regularization with the logistic regression module in `sklearn`. This example is adapted from [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_sparse_logistic_regression_mnist.html#sphx-glr-auto-examples-linear-model-plot-sparse-logistic-regression-mnist-py).\n",
    "\n",
    "---\n",
    "First, we load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "803HhxC00YvS"
   },
   "outputs": [],
   "source": [
    "# Import numpy\n",
    "import numpy as np\n",
    "\n",
    "# Import libraries from sklearn\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils import check_random_state\n",
    "\n",
    "# Import matplotlib for plotting\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9tNnrJIF1DV1"
   },
   "source": [
    "Next, we load the MNIST dataset, perform pre-processing of the data using `StandardScaler`. `train_test_split` was used to partition training and test data.\n",
    "\n",
    "Then, the logistic regression function `LogisticRegression` was called with the L1 regularization option, `penalty='l1'`.\n",
    "\n",
    "Observe that with L1 regularization, we achieve sparsity in the number of coefficients required in the logistic regression model. In particular, 82.81% of the coefficients (`clf.coef_`) are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RHGuJ5cx87ls"
   },
   "outputs": [],
   "source": [
    "# Load data from https://www.openml.org/d/554\n",
    "X, y          = fetch_openml('mnist_784', version=1, return_X_y=True, as_frame=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1783,
     "status": "ok",
     "timestamp": 1632759913359,
     "user": {
      "displayName": "Kelly Feng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiE_zqw1wGgtnqrMNNYVIn3LXnFSdlpXcozSv-W=s64",
      "userId": "05766461114990380038"
     },
     "user_tz": 240
    },
    "id": "pyWMoHvl0E1j",
    "outputId": "ef32eb24-7b85-4c64-dd9c-771b4da4838d"
   },
   "outputs": [],
   "source": [
    "# Split the data into train and test\n",
    "train_samples = 5000 # User-defined number of samples\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=train_samples, test_size=10000, random_state=0)\n",
    "\n",
    "# Scale our data\n",
    "scaler        = StandardScaler()\n",
    "X_train       = scaler.fit_transform(X_train)\n",
    "X_test        = scaler.transform(X_test)\n",
    "\n",
    "# Turn up tolerance for faster convergence\n",
    "# C is the inverse of our regularization parameter- the larger it is, the less regularization we have\n",
    "clf           = LogisticRegression(C=100. / train_samples, penalty='l1', solver='saga', tol=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "sparsity      = np.mean(clf.coef_ == 0) * 100\n",
    "score         = clf.score(X_test, y_test)\n",
    "print(\"Sparsity with L1 penalty: %.2f%%\" % sparsity)\n",
    "print(\"Test score with L1 penalty: %.4f\" % score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5MtkxpWt07Pl"
   },
   "source": [
    "Finally, we visualize our results with plots. Coefficients (`clf.coef_`) of each feature for each class (total 10 classes, digits 0-9) are plotted as images below (see `l1_plot.imshow`). In this MNIST example, the features corresponds directly to pixels in the images. Hence, by plotting these coefficients, we can identify (qualitatively) which parts of the images are the most critical in classifying the digits. As a side note, based on the color map used (`RdBu`), a darker blue shade represents a higher-than-average coefficient, while a darker red shade represents a lower-than-average coefficient.\n",
    "\n",
    "**Question:** What do these plots tell you about a potential advantage of regression models over something like decision trees or knns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 339
    },
    "executionInfo": {
     "elapsed": 568,
     "status": "ok",
     "timestamp": 1632759924278,
     "user": {
      "displayName": "Kelly Feng",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiE_zqw1wGgtnqrMNNYVIn3LXnFSdlpXcozSv-W=s64",
      "userId": "05766461114990380038"
     },
     "user_tz": 240
    },
    "id": "TYZfHr8o07ZP",
    "outputId": "d4ef1935-5723-4f85-d334-bad6fc5911fd"
   },
   "outputs": [],
   "source": [
    "coef          = clf.coef_.copy()\n",
    "plt.figure(figsize=(10, 5))\n",
    "scale = np.abs(coef).max()\n",
    "for i in range(10):\n",
    "    l1_plot = plt.subplot(2, 5, i + 1)\n",
    "    l1_plot.imshow(coef[i].reshape(28, 28), interpolation='nearest',\n",
    "                   cmap=plt.cm.RdBu, vmin=-scale, vmax=scale)\n",
    "    l1_plot.set_xticks(())\n",
    "    l1_plot.set_yticks(())\n",
    "    l1_plot.set_xlabel('Digit %i' % i)\n",
    "plt.suptitle('Classification vectors for 10 classes:')\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
